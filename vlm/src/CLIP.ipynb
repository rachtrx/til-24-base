{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b2c1cf-b838-4f5a-8c68-3a326ac81272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import clip\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf324eb-a7b9-4d04-944d-fe846d8fbfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "\n",
    "##training data to be added to tune the models\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "rcnn_img_dir = os.path.join(data_dir, \"rcnn\") # for normal res images\n",
    "clip_img_dir = os.path.join(data_dir, \"clip\") # for 224x224 images\n",
    "\n",
    "for dir in [train_dir, test_dir, val_dir]:\n",
    "    os.makedirs(dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08afedd0-6c6d-462e-975e-3f8d1f703bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue missile', 'green and brown camouflage fighter jet', 'red helicopter', 'black camouflage fighter jet', 'white, red, and blue commercial aircraft', 'grey and white light aircraft', 'blue and red commercial aircraft', 'red and grey missile', 'white and grey helicopter', 'grey and green cargo aircraft', 'white drone', 'blue and grey fighter jet', 'red and white missile', 'yellow and green helicopter', 'black and white missile', 'white, blue, and red commercial aircraft', 'black helicopter', 'white and black cargo aircraft', 'orange light aircraft', 'white commercial aircraft', 'grey and red commercial aircraft', 'white and red helicopter', 'white, black, and grey missile', 'blue and green fighter plane', 'grey and black helicopter', 'green and black missile', 'green fighter plane', 'yellow, black, and red helicopter', 'white light aircraft', 'white and blue light aircraft', 'blue, yellow, and white cargo aircraft', 'blue commercial aircraft', 'yellow, red, and blue fighter plane', 'blue and yellow helicopter', 'white, black, and red drone', 'blue camouflage fighter jet', 'red fighter jet', 'green helicopter', 'red fighter plane', 'black fighter plane', 'white and black fighter plane', 'blue and white commercial aircraft', 'green and white fighter plane', 'black and orange drone', 'yellow and red light aircraft', 'black cargo aircraft', 'grey and red missile', 'yellow and black fighter plane', 'blue and yellow fighter jet', 'white and red commercial aircraft', 'black and white cargo aircraft', 'grey cargo aircraft', 'white and black drone', 'red, white, and blue fighter jet', 'grey and yellow fighter plane', 'white and blue helicopter', 'white and blue cargo aircraft', 'white and red fighter plane', 'red and white fighter jet', 'blue and white light aircraft', 'grey, red, and blue commercial aircraft', 'red light aircraft', 'white and black fighter jet', 'black fighter jet', 'white and orange light aircraft', 'grey helicopter', 'yellow commercial aircraft', 'red and white helicopter', 'black and yellow missile', 'green camouflage helicopter', 'grey commercial aircraft', 'red, white, and blue light aircraft', 'white and orange commercial aircraft', 'grey fighter jet', 'white helicopter', 'green and brown camouflage fighter plane', 'green and black camouflage helicopter', 'yellow missile', 'black and yellow drone', 'black and white commercial aircraft', 'blue and red light aircraft', 'blue and white helicopter', 'grey and red fighter jet', 'white cargo aircraft', 'red and white light aircraft', 'white and blue commercial aircraft', 'red and black drone', 'black drone', 'yellow, red, and grey helicopter', 'blue, yellow, and green fighter plane', 'white and yellow commercial aircraft', 'yellow helicopter', 'grey and black fighter plane', 'blue helicopter', 'green missile', 'white and black helicopter', 'orange and black fighter jet', 'grey missile', 'blue, yellow, and black helicopter', 'grey drone', 'silver and blue fighter plane', 'grey light aircraft', 'green light aircraft', 'green and grey helicopter', 'white and blue fighter jet', 'white and red missile', 'grey fighter plane', 'white missile', 'white fighter jet', 'white and red fighter jet', 'red and white fighter plane', 'white, red, and green fighter plane', 'yellow fighter plane', 'green and brown camouflage helicopter', 'blue and white missile', 'white and blue fighter plane', 'green and yellow fighter plane', 'yellow light aircraft', 'black and brown camouflage helicopter', 'grey camouflage fighter jet', 'white and black light aircraft', 'yellow fighter jet', 'white fighter plane', 'white and red light aircraft', 'grey and white fighter plane', 'silver fighter plane'}\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    total_examples = len(data['image'])\n",
    "    indices = list(range(total_examples))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(train_ratio * total_examples)\n",
    "    val_end = train_end + int(val_ratio * total_examples)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "    \n",
    "    train_data = {'image': [data['image'][i] for i in train_indices], 'annotations': [data['annotations'][i] for i in train_indices]}\n",
    "    val_data = {'image': [data['image'][i] for i in val_indices], 'annotations': [data['annotations'][i] for i in val_indices]}\n",
    "    test_data = {'image': [data['image'][i] for i in test_indices], 'annotations': [data['annotations'][i] for i in test_indices]}\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'image': [], 'annotations': []}\n",
    "data_path = os.path.join(test_dir, \"vlm.jsonl\")\n",
    "with jsonlines.open(metadata_path) as reader:\n",
    "    for obj in reader:\n",
    "        if MAX_FILE_COUNT and len(data['image']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        for annotation in obj['annotations']:\n",
    "            data['image'].append(os.path.join(img_dir, obj['image']))\n",
    "            data['annotations'].append(annotation)\n",
    "            \n",
    "captions_set = set()\n",
    "\n",
    "# Loop through the annotations and extract the text_features\n",
    "for annotation in data['annotations']:\n",
    "    captions_set.add(annotation['caption'])\n",
    "\n",
    "# Print the resulting set of text_features\n",
    "print(captions_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a18b735-81f0-495e-8b26-3aa6688e961e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f77d808-2e6a-4244-85e7-7b5763a4f0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_to_jsonl(data, file_name):\n",
    "    with jsonlines.open(file_name, mode='w') as writer:\n",
    "        for img, ann in zip(data['image'], data['annotations']):\n",
    "            writer.write({'image': img, 'annotations': ann})\n",
    "\n",
    "# Write each dataset to a separate JSONL file\n",
    "write_to_jsonl(train_data, os.path.join(train_dir, \"train.jsonl\"))\n",
    "write_to_jsonl(val_data, os.path.join(val_dir, \"val.jsonl\"))\n",
    "write_to_jsonl(test_data, os.path.join(test_dir, \"test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac7b92d-de8c-41d9-971d-a9d071f33aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"blue missile\": {\n",
      "    \"colors\": [\n",
      "      \"blue\"\n",
      "    ],\n",
      "    \"objects\": [\n",
      "      \"missile\"\n",
      "    ]\n",
      "  },\n",
      "  \"green and brown camouflage fighter jet\": {\n",
      "    \"colors\": [\n",
      "      \"camouflage\",\n",
      "      \"brown\",\n",
      "      \"green\"\n",
      "    ],\n",
      "    \"objects\": [\n",
      "      \"fighter jet\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the custom matcher for multi-word expressions\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns for multi-word expressions (objects)\n",
    "multi_word_patterns = [\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"fighter\"}, {\"LOWER\": \"jet\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"light\"}, {\"LOWER\": \"aircraft\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"commercial\"}, {\"LOWER\": \"aircraft\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"fighter\"}, {\"LOWER\": \"plane\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"cargo\"}, {\"LOWER\": \"aircraft\"}]}\n",
    "]\n",
    "\n",
    "for pattern in multi_word_patterns:\n",
    "    matcher.add(pattern[\"label\"], [pattern[\"pattern\"]])\n",
    "\n",
    "# Add patterns for single-word objects\n",
    "single_word_objects = [\"missile\", \"helicopter\", \"drone\"]\n",
    "for obj in single_word_objects:\n",
    "    matcher.add(\"OBJECT\", [[{\"LOWER\": obj}]])\n",
    "\n",
    "# Define custom colors\n",
    "valid_colors = {\"white\", \"blue\", \"green\", \"black\", \"red\", \"yellow\", \"grey\", \"orange\", \"silver\", \"camouflage\"}\n",
    "\n",
    "excluded_adjectives = {\"light\", \"commercial\"}\n",
    "\n",
    "# Function to identify custom colors\n",
    "def is_valid_color(token):\n",
    "    return token.text.lower() in valid_colors\n",
    "\n",
    "def filter_colors(token):\n",
    "    return (token.pos_ == \"ADJ\" or is_valid_color(token)) and token.text.lower() not in excluded_adjectives\n",
    "\n",
    "# Function to filter out overlapping spans\n",
    "def filter_overlapping_spans(spans):\n",
    "    sorted_spans = sorted(spans, key=lambda span: (span.start, span.end))\n",
    "    filtered_spans = []\n",
    "    last_end = -1\n",
    "    for span in sorted_spans:\n",
    "        if span.start >= last_end:\n",
    "            filtered_spans.append(span)\n",
    "            last_end = span.end\n",
    "    return filtered_spans\n",
    "\n",
    "# Process each string\n",
    "results = {}\n",
    "colors_set = set()\n",
    "objects_set = set()\n",
    "\n",
    "for string in captions_set:\n",
    "    doc = nlp(string)\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=label) for match_id, start, end in matches for label in [nlp.vocab.strings[match_id]]]\n",
    "    \n",
    "    # Filter out overlapping spans\n",
    "    spans = filter_overlapping_spans(spans)\n",
    "    \n",
    "    doc.ents = spans  # Set the identified multi-word expressions as named entities\n",
    "    \n",
    "    colors = set([token.text for token in doc if filter_colors(token)])\n",
    "    objects = set([ent.text for ent in doc.ents if ent.label_ == \"OBJECT\"])\n",
    "    \n",
    "    colors_set.update(colors)\n",
    "    objects_set.update(objects)\n",
    "    \n",
    "    results[string] = {\"colors\": list(colors), \"objects\": list(objects)}\n",
    "\n",
    "first_two_pairs = {k: results[k] for k in list(results.keys())[:2]}\n",
    "print(json.dumps(first_two_pairs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005f3127-fe98-4a04-9e42-cd9b3a5c11cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cargo aircraft',\n",
       " 'commercial aircraft',\n",
       " 'drone',\n",
       " 'fighter jet',\n",
       " 'fighter plane',\n",
       " 'helicopter',\n",
       " 'light aircraft',\n",
       " 'missile'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {\n",
    "#     \"img_paths\": []\n",
    "#     \"labels\": []\n",
    "#     \"bbox\": []\n",
    "#     \"text_features\": []\n",
    "# }\n",
    "\n",
    "objects_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abced297-53b6-4c52-a24c-b2f6e9226a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e83b592-be04-423d-a043-7aa59231ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id_mapping = {\n",
    "    'padding': 0,\n",
    "    'cargo aircraft': 1,\n",
    "    'commercial aircraft': 2,\n",
    "    'drone': 3,\n",
    "    'fighter jet': 4,\n",
    "    'fighter plane': 5,\n",
    "    'helicopter': 6,\n",
    "    'light aircraft': 7,\n",
    "    'missile': 8\n",
    "}\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self, dataset, output_dir, batch_size=2, max_caption_length=20):\n",
    "        self.dataset = dataset\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.clip_preprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.rcnn_preprocessor = weights.transforms()\n",
    "        self.rcnn_file_path, self.clip_file_path = self.create_dummy_images()\n",
    "        self.max_caption_length = max_caption_length\n",
    "\n",
    "    def preprocess_and_save_batches(self):\n",
    "        images = self.dataset['image']\n",
    "        annotations = self.dataset['annotations']\n",
    "        num_batches = (len(images) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "            batch_images = images[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_annotations = annotations[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_data = list(zip(batch_images, batch_annotations))\n",
    "            rcnn_img_paths, clip_img_paths, text_data, labels, bboxes = self.process_batch(batch_data)\n",
    "            rcnn_img_paths, clip_img_paths, text_data, labels, bboxes = self.pad_batch(rcnn_img_paths, clip_img_paths, text_data, labels, bboxes)\n",
    "            self.save_batch(batch_idx, rcnn_img_paths, clip_img_paths, text_data, labels, bboxes)\n",
    "        return num_batches\n",
    "\n",
    "    def process_batch(self, batch_data):\n",
    "        rcnn_img_paths = []\n",
    "        clip_img_paths = []\n",
    "        text_data = []\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        \n",
    "        prev_image_path = None\n",
    "        prev_image = None\n",
    "        for image_path, annotation in batch_data:\n",
    "            try:\n",
    "                if image_path != prev_image_path:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    \n",
    "                    preprocessed_rcnn = self.rcnn_preprocessor(image).to('cuda')  # Move to GPU\n",
    "                    image_array_rcnn = preprocessed_rcnn.permute(1, 2, 0).cpu().numpy()  # Move back to CPU for saving\n",
    "                    rcnn_output_path = image_path.replace('/novice/images/', '/til-24-base/vlm/src/data/rcnn/').replace('.jpg', '.npy')\n",
    "                    np.save(rcnn_output_path, image_array_rcnn)\n",
    "                    \n",
    "                    # Preprocess image for CLIP\n",
    "                    preprocessed_clip = self.clip_preprocessor(images=image, return_tensors=\"pt\")\n",
    "                    preprocessed_clip = {k: v.to('cuda') for k, v in preprocessed_clip.items()}  # Move to GPU\n",
    "                    image_array_clip = preprocessed_clip['pixel_values'].squeeze().cpu().numpy()  # Move back to CPU for saving\n",
    "                    clip_output_path = image_path.replace('/novice/images/', '/til-24-base/vlm/src/data/clip/').replace('.jpg', '_clip.npy')\n",
    "                    np.save(clip_output_path, image_array_clip)  # Save the numpy array for CLIP\n",
    "                    \n",
    "                caption = annotation['caption']\n",
    "                text_input = self.clip_preprocessor(text=[caption], return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_caption_length)\n",
    "                text_input = {k: v.to('cuda') for k, v in text_input.items()}  # Move to GPU\n",
    "                text_input_serializable = {key: value.cpu().tolist() for key, value in text_input.items()}  # Move back to CPU for saving\n",
    "                \n",
    "                bbox = annotation['bbox']\n",
    "                x, y, w, h = bbox\n",
    "                bbox = self.correct_bbox_format(bbox)\n",
    "                \n",
    "                rcnn_img_paths.append(rcnn_output_path)\n",
    "                clip_img_paths.append(clip_output_path)\n",
    "                \n",
    "                true_label = results.get(caption)\n",
    "                labels.append(label_to_id_mapping[true_label['objects'][0]])\n",
    "                bboxes.append(bbox)\n",
    "                text_data.append(text_input_serializable)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid image: {image_path}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        return rcnn_img_paths, clip_img_paths, text_data, labels, bboxes\n",
    "\n",
    "    def pad_batch(self, rcnn_img_paths, clip_img_paths, text_data, labels, bboxes):\n",
    "        while len(rcnn_img_paths) < self.batch_size:\n",
    "            rcnn_img_paths.append(self.rcnn_file_path)\n",
    "            clip_img_paths.append(self.clip_file_path)\n",
    "            \n",
    "            caption = \"\"\n",
    "            text_input = self.clip_preprocessor(text=[caption], return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_caption_length)\n",
    "            text_input = {k: v.to('cuda') for k, v in text_input.items()}  # Move to GPU\n",
    "            text_input_serializable = {key: value.cpu().tolist() for key, value in text_input.items()}  # Move back to CPU for saving\n",
    "            text_data.append(text_input_serializable)\n",
    "            \n",
    "            labels.append(label_to_id_mapping['padding'])\n",
    "            bboxes.append([0,0,0,0])\n",
    "            \n",
    "        return rcnn_img_paths, clip_img_paths, text_data, labels, bboxes\n",
    "    \n",
    "    def create_dummy_images(self):\n",
    "        # Create a dummy image with the target size and filled with zeros\n",
    "        rcnn_file_path = os.path.join(data_dir, 'rcnn', 'dummy.npy') # rcnn\n",
    "        if not os.path.exists(rcnn_file_path):\n",
    "            image = Image.new('RGB', (1520, 870), (0, 0, 0))\n",
    "            preprocessed_rcnn = self.rcnn_preprocessor(image).to('cuda')  # Move to GPU\n",
    "            image_array_rcnn = preprocessed_rcnn.permute(1, 2, 0).cpu().numpy()  # Move back to CPU for saving\n",
    "            np.save(rcnn_file_path, image_array_rcnn)\n",
    "            print(f\"RCNN Image saved to {rcnn_file_path}\")\n",
    "        \n",
    "        clip_file_path = os.path.join(data_dir, 'clip', 'dummy.npy') # clip\n",
    "        if not os.path.exists(clip_file_path):\n",
    "            image = Image.new('RGB', (1520, 870), (0, 0, 0))\n",
    "            preprocessed_clip = self.clip_preprocessor(images=image, return_tensors=\"pt\")\n",
    "            preprocessed_clip = {k: v.to('cuda') for k, v in preprocessed_clip.items()}  # Move to GPU\n",
    "            image_array_clip = preprocessed_clip['pixel_values'].squeeze().cpu().numpy()  # Move back to CPU for saving\n",
    "            np.save(clip_file_path, image_array_clip)  # Save the numpy array for CLIP\n",
    "            print(f\"CLIP Image saved to {clip_file_path}\")\n",
    "        \n",
    "        return rcnn_file_path, clip_file_path\n",
    "\n",
    "    def save_batch(self, batch_idx, rcnn_img_paths, clip_img_paths, text_data, labels, bboxes):\n",
    "        batch_output_dir = os.path.join(self.output_dir, f\"batch_{batch_idx}\")\n",
    "        os.makedirs(batch_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save image paths as JSON\n",
    "        with open(os.path.join(batch_output_dir, \"rcnn_img_paths.json\"), 'w') as f:\n",
    "            json.dump(rcnn_img_paths, f)\n",
    "            \n",
    "        with open(os.path.join(batch_output_dir, \"clip_img_paths.json\"), 'w') as f:\n",
    "            json.dump(clip_img_paths, f)\n",
    "\n",
    "        # Save inputs as JSON\n",
    "        with open(os.path.join(batch_output_dir, \"text_data.json\"), 'w') as f:\n",
    "            json.dump(text_data, f)\n",
    "\n",
    "        # Save bounding boxes as numpy arrays\n",
    "        np.save(os.path.join(batch_output_dir, \"bboxes.npy\"), np.array(bboxes))\n",
    "\n",
    "        # Save labels as numpy arrays\n",
    "        np.save(os.path.join(batch_output_dir, \"labels.npy\"), np.array(labels))\n",
    "        \n",
    "    @staticmethod\n",
    "    def correct_bbox_format(bbox):\n",
    "        x, y, width, height = bbox\n",
    "        xmin = x\n",
    "        ymin = y\n",
    "        xmax = x + width\n",
    "        ymax = y + height\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a4558c-ed74-4914-a6c5-35646777c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41cf840f-cadb-44bc-9725-a6b8f20a05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processor = ImagePreprocessor(train_data, output_dir=os.path.join(train_dir))\n",
    "val_processor = ImagePreprocessor(val_data, output_dir=os.path.join(val_dir))\n",
    "test_processor = ImagePreprocessor(test_data,output_dir=os.path.join(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d982024f-b81b-4cb7-80cb-690bcd3987ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 5982/5982 [1:14:06<00:00,  1.35it/s]\n",
      "Processing Batches: 100%|██████████| 748/748 [09:58<00:00,  1.25it/s]\n",
      "Processing Batches: 100%|██████████| 748/748 [09:41<00:00,  1.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processor.preprocess_and_save_batches()\n",
    "val_processor.preprocess_and_save_batches()\n",
    "test_processor.preprocess_and_save_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c029a1b9-194a-404f-bd51-623da457fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following code is probably wrong\n",
    "# train_num_batches = (len(train_processor.dataset['image']) + train_processor.batch_size - 1)\n",
    "# val_num_batches = (len(val_processor.dataset['image']) + val_processor.batch_size - 1)\n",
    "# test_num_batches = (len(test_processor.dataset['image']) + test_processor.batch_size - 1)\n",
    "\n",
    "# print(train_num_batches)\n",
    "# print(val_num_batches)\n",
    "# print(test_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f1b67a-6939-40a2-8f14-65a4185c96da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1766079-7888-4944-bc71-68e24f4ca94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c6f72-8c41-4067-864b-1cb26c791798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     @staticmethod\n",
    "#     def adjust_bbox_for_resize_and_pad(bbox, scale, padding): # RESIZE\n",
    "#         \"\"\"\n",
    "#         Adjust the bounding box coordinates based on the image scaling and padding.\n",
    "\n",
    "#         Args:\n",
    "#         bbox (list): The bounding box in format [x, y, width, height].\n",
    "#         scale (tuple): The scaling factors (scale_x, scale_y).\n",
    "#         padding (tuple): Padding added (pad_left, pad_top).\n",
    "\n",
    "#         Returns:\n",
    "#         list: The adjusted bounding box.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         x, y, width, height = bbox\n",
    "#         x1 = x\n",
    "#         y1 = y\n",
    "#         x2 = x + width\n",
    "#         y2 = y + height\n",
    "        \n",
    "#         scale_x, scale_y = scale\n",
    "#         pad_x, pad_y = padding\n",
    "\n",
    "#         # Scale and adjust for padding\n",
    "#         new_x1 = x1 * scale_x + pad_x\n",
    "#         new_y1 = y1 * scale_y + pad_y\n",
    "#         new_x2 = x2 * scale_x + pad_x\n",
    "#         new_y2 = y2 * scale_y + pad_y\n",
    "\n",
    "#         return [new_x1, new_y1, new_x2, new_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f66ed-52f9-4c49-983a-ff4f2d69de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_and_bbox(image, bbox, target_size=224):\n",
    "    \"\"\"\n",
    "    Resize an image and adjust the bounding boxes. Maintain aspect ratio and pad if necessary.\n",
    "\n",
    "    Args:\n",
    "    image (PIL.Image): The original image.\n",
    "    bbox (list): Bounding box with format [x_min, y_min, x_max, y_max].\n",
    "    target_size (int): The target size to which the longer side of the image will be resized.\n",
    "\n",
    "    Returns:\n",
    "    PIL.Image: Resized and padded image.\n",
    "    list: Adjusted bounding box coordinates.\n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    ratio = min(target_size / original_width, target_size / original_height)\n",
    "    new_width = int(original_width * ratio)\n",
    "    new_height = int(original_height * ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "\n",
    "    # Calculate padding to make the image square\n",
    "    pad_width = (target_size - new_width) // 2\n",
    "    pad_height = (target_size - new_height) // 2\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = Image.new('RGB', (target_size, target_size), (128, 128, 128))\n",
    "    padded_image.paste(resized_image, (pad_width, pad_height))\n",
    "\n",
    "    # Adjust bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_min = x_min * ratio + pad_width\n",
    "    y_min = y_min * ratio + pad_height\n",
    "    x_max = x_max * ratio + pad_width\n",
    "    y_max = y_max * ratio + pad_height\n",
    "    adjusted_bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return padded_image, adjusted_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b782796-2b1f-49fb-acb4-ef5d7140301c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_and_pad_image(image, target_size=224):\n",
    "        \"\"\"\n",
    "        Resize the image to maintain aspect ratio and pad to make it square (target_size x target_size).\n",
    "\n",
    "        Args:\n",
    "        image (PIL.Image): The original image.\n",
    "        target_size (int): The desired size of the square image (both width and height).\n",
    "\n",
    "        Returns:\n",
    "        PIL.Image: The resized and padded image.\n",
    "        tuple: The scaling factors (scale_x, scale_y).\n",
    "        tuple: Padding added (pad_left, pad_top).\n",
    "        \"\"\"\n",
    "        # Calculate scaling factors to maintain aspect ratio\n",
    "        original_width, original_height = image.size\n",
    "        ratio = min(target_size / original_width, target_size / original_height)\n",
    "        new_width = int(original_width * ratio)\n",
    "        new_height = int(original_height * ratio)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "\n",
    "        # Calculate padding to make the image square\n",
    "        pad_width = (target_size - new_width) // 2\n",
    "        pad_height = (target_size - new_height) // 2\n",
    "\n",
    "        # Pad the resized image\n",
    "        padded_image = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
    "        padded_image.paste(resized_image, (pad_width, pad_height))\n",
    "\n",
    "        return padded_image, (ratio, ratio), (pad_width, pad_height)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
