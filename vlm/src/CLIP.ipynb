{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b2c1cf-b838-4f5a-8c68-3a326ac81272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/tmp/tmpecm2hxfl\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/tmp/tmpecm2hxfl/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import clip\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf324eb-a7b9-4d04-944d-fe846d8fbfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "\n",
    "##training data to be added to tune the models\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "for dir in [train_dir, test_dir, val_dir]:\n",
    "    os.makedirs(dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08afedd0-6c6d-462e-975e-3f8d1f703bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'white and blue fighter plane', 'blue, yellow, and black helicopter', 'black fighter plane', 'green and brown camouflage helicopter', 'blue and red commercial aircraft', 'white and blue light aircraft', 'blue missile', 'grey and black helicopter', 'blue helicopter', 'white and black fighter plane', 'red fighter plane', 'white and blue fighter jet', 'black and yellow drone', 'blue and red light aircraft', 'red light aircraft', 'white fighter jet', 'grey and white fighter plane', 'green and yellow fighter plane', 'blue commercial aircraft', 'blue and white commercial aircraft', 'grey commercial aircraft', 'green fighter plane', 'white and red commercial aircraft', 'white commercial aircraft', 'black fighter jet', 'black cargo aircraft', 'yellow helicopter', 'grey drone', 'grey and black fighter plane', 'black and brown camouflage helicopter', 'white and blue commercial aircraft', 'white and red light aircraft', 'black and white cargo aircraft', 'green helicopter', 'white and orange light aircraft', 'white and black drone', 'green and black camouflage helicopter', 'yellow light aircraft', 'yellow and green helicopter', 'white and yellow commercial aircraft', 'yellow fighter jet', 'white, blue, and red commercial aircraft', 'blue camouflage fighter jet', 'green and brown camouflage fighter jet', 'red and white missile', 'white and red helicopter', 'white and black cargo aircraft', 'black helicopter', 'red helicopter', 'yellow, black, and red helicopter', 'grey cargo aircraft', 'black and yellow missile', 'red, white, and blue fighter jet', 'red and white fighter plane', 'yellow, red, and grey helicopter', 'yellow and red light aircraft', 'blue and white missile', 'grey fighter jet', 'white missile', 'grey missile', 'red fighter jet', 'orange and black fighter jet', 'white and black light aircraft', 'white and black fighter jet', 'white and blue cargo aircraft', 'blue and white helicopter', 'red, white, and blue light aircraft', 'white, black, and red drone', 'grey fighter plane', 'white, red, and green fighter plane', 'blue and grey fighter jet', 'red and white fighter jet', 'white, black, and grey missile', 'white, red, and blue commercial aircraft', 'blue, yellow, and white cargo aircraft', 'orange light aircraft', 'white and blue helicopter', 'white drone', 'white and grey helicopter', 'silver and blue fighter plane', 'green and brown camouflage fighter plane', 'black and white missile', 'yellow fighter plane', 'red and white helicopter', 'white and orange commercial aircraft', 'blue, yellow, and green fighter plane', 'yellow commercial aircraft', 'white and red missile', 'grey, red, and blue commercial aircraft', 'green camouflage helicopter', 'red and grey missile', 'white fighter plane', 'yellow, red, and blue fighter plane', 'white cargo aircraft', 'yellow missile', 'blue and white light aircraft', 'red and white light aircraft', 'grey and yellow fighter plane', 'white helicopter', 'red and black drone', 'grey and green cargo aircraft', 'white and black helicopter', 'black drone', 'yellow and black fighter plane', 'black and white commercial aircraft', 'grey camouflage fighter jet', 'green missile', 'blue and green fighter plane', 'grey and white light aircraft', 'green and grey helicopter', 'grey and red fighter jet', 'grey and red missile', 'black and orange drone', 'white and red fighter plane', 'blue and yellow fighter jet', 'black camouflage fighter jet', 'grey and red commercial aircraft', 'white and red fighter jet', 'blue and yellow helicopter', 'green light aircraft', 'green and white fighter plane', 'grey light aircraft', 'silver fighter plane', 'grey helicopter', 'white light aircraft', 'green and black missile'}\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    total_examples = len(data['image'])\n",
    "    indices = list(range(total_examples))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(train_ratio * total_examples)\n",
    "    val_end = train_end + int(val_ratio * total_examples)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "    \n",
    "    train_data = {'image': [data['image'][i] for i in train_indices], 'annotations': [data['annotations'][i] for i in train_indices]}\n",
    "    val_data = {'image': [data['image'][i] for i in val_indices], 'annotations': [data['annotations'][i] for i in val_indices]}\n",
    "    test_data = {'image': [data['image'][i] for i in test_indices], 'annotations': [data['annotations'][i] for i in test_indices]}\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'image': [], 'annotations': []}\n",
    "data_path = os.path.join(test_dir, \"vlm.jsonl\")\n",
    "with jsonlines.open(metadata_path) as reader:\n",
    "    for obj in reader:\n",
    "        if MAX_FILE_COUNT and len(data['image']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        for annotation in obj['annotations']:\n",
    "            data['image'].append(os.path.join(img_dir, obj['image']))\n",
    "            data['annotations'].append(annotation)\n",
    "            \n",
    "captions_set = set()\n",
    "\n",
    "# Loop through the annotations and extract the text_features\n",
    "for annotation in data['annotations']:\n",
    "    captions_set.add(annotation['caption'])\n",
    "\n",
    "# Print the resulting set of text_features\n",
    "print(captions_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a18b735-81f0-495e-8b26-3aa6688e961e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f77d808-2e6a-4244-85e7-7b5763a4f0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_to_jsonl(data, file_name):\n",
    "    with jsonlines.open(file_name, mode='w') as writer:\n",
    "        for img, ann in zip(data['image'], data['annotations']):\n",
    "            writer.write({'image': img, 'annotations': ann})\n",
    "\n",
    "# Write each dataset to a separate JSONL file\n",
    "write_to_jsonl(train_data, os.path.join(train_dir, \"train.jsonl\"))\n",
    "write_to_jsonl(val_data, os.path.join(val_dir, \"val.jsonl\"))\n",
    "write_to_jsonl(test_data, os.path.join(test_dir, \"test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac7b92d-de8c-41d9-971d-a9d071f33aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"white and blue fighter plane\": {\n",
      "    \"colors\": [\n",
      "      \"white\",\n",
      "      \"blue\"\n",
      "    ],\n",
      "    \"objects\": [\n",
      "      \"fighter plane\"\n",
      "    ]\n",
      "  },\n",
      "  \"blue, yellow, and black helicopter\": {\n",
      "    \"colors\": [\n",
      "      \"black\",\n",
      "      \"blue\",\n",
      "      \"yellow\"\n",
      "    ],\n",
      "    \"objects\": [\n",
      "      \"helicopter\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the custom matcher for multi-word expressions\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns for multi-word expressions (objects)\n",
    "multi_word_patterns = [\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"fighter\"}, {\"LOWER\": \"jet\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"light\"}, {\"LOWER\": \"aircraft\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"commercial\"}, {\"LOWER\": \"aircraft\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"fighter\"}, {\"LOWER\": \"plane\"}]},\n",
    "    {\"label\": \"OBJECT\", \"pattern\": [{\"LOWER\": \"cargo\"}, {\"LOWER\": \"aircraft\"}]}\n",
    "]\n",
    "\n",
    "for pattern in multi_word_patterns:\n",
    "    matcher.add(pattern[\"label\"], [pattern[\"pattern\"]])\n",
    "\n",
    "# Add patterns for single-word objects\n",
    "single_word_objects = [\"missile\", \"helicopter\", \"drone\"]\n",
    "for obj in single_word_objects:\n",
    "    matcher.add(\"OBJECT\", [[{\"LOWER\": obj}]])\n",
    "\n",
    "# Define custom colors\n",
    "valid_colors = {\"white\", \"blue\", \"green\", \"black\", \"red\", \"yellow\", \"grey\", \"orange\", \"silver\", \"camouflage\"}\n",
    "\n",
    "excluded_adjectives = {\"light\", \"commercial\"}\n",
    "\n",
    "# Function to identify custom colors\n",
    "def is_valid_color(token):\n",
    "    return token.text.lower() in valid_colors\n",
    "\n",
    "def filter_colors(token):\n",
    "    return (token.pos_ == \"ADJ\" or is_valid_color(token)) and token.text.lower() not in excluded_adjectives\n",
    "\n",
    "# Function to filter out overlapping spans\n",
    "def filter_overlapping_spans(spans):\n",
    "    sorted_spans = sorted(spans, key=lambda span: (span.start, span.end))\n",
    "    filtered_spans = []\n",
    "    last_end = -1\n",
    "    for span in sorted_spans:\n",
    "        if span.start >= last_end:\n",
    "            filtered_spans.append(span)\n",
    "            last_end = span.end\n",
    "    return filtered_spans\n",
    "\n",
    "# Process each string\n",
    "results = {}\n",
    "colors_set = set()\n",
    "objects_set = set()\n",
    "\n",
    "for string in captions_set:\n",
    "    doc = nlp(string)\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=label) for match_id, start, end in matches for label in [nlp.vocab.strings[match_id]]]\n",
    "    \n",
    "    # Filter out overlapping spans\n",
    "    spans = filter_overlapping_spans(spans)\n",
    "    \n",
    "    doc.ents = spans  # Set the identified multi-word expressions as named entities\n",
    "    \n",
    "    colors = set([token.text for token in doc if filter_colors(token)])\n",
    "    objects = set([ent.text for ent in doc.ents if ent.label_ == \"OBJECT\"])\n",
    "    \n",
    "    colors_set.update(colors)\n",
    "    objects_set.update(objects)\n",
    "    \n",
    "    results[string] = {\"colors\": list(colors), \"objects\": list(objects)}\n",
    "\n",
    "first_two_pairs = {k: results[k] for k in list(results.keys())[:2]}\n",
    "print(json.dumps(first_two_pairs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005f3127-fe98-4a04-9e42-cd9b3a5c11cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cargo aircraft',\n",
       " 'commercial aircraft',\n",
       " 'drone',\n",
       " 'fighter jet',\n",
       " 'fighter plane',\n",
       " 'helicopter',\n",
       " 'light aircraft',\n",
       " 'missile'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {\n",
    "#     \"img_paths\": []\n",
    "#     \"labels\": []\n",
    "#     \"bbox\": []\n",
    "#     \"text_features\": []\n",
    "# }\n",
    "\n",
    "objects_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abced297-53b6-4c52-a24c-b2f6e9226a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e83b592-be04-423d-a043-7aa59231ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id_mapping = {\n",
    "    'padding': 0,\n",
    "    'cargo aircraft': 1,\n",
    "    'commercial aircraft': 2,\n",
    "    'drone': 3,\n",
    "    'fighter jet': 4,\n",
    "    'fighter plane': 5,\n",
    "    'helicopter': 6,\n",
    "    'light aircraft': 7,\n",
    "    'missile': 8\n",
    "}\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self, dataset, model, processor, output_dir, batch_size=7, device=\"cuda\", max_caption_length=77):\n",
    "        self.dataset = dataset\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "        self.clip_model = model.to(device)\n",
    "        self.clip_processor = processor\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.padding_image = self.create_dummy_image()\n",
    "        self.dummy_text = self.create_dummy_text()\n",
    "\n",
    "    def preprocess_and_save_batches(self):\n",
    "        images = self.dataset['image']\n",
    "        annotations = self.dataset['annotations']\n",
    "        num_batches = (len(images) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "            batch_images = images[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_annotations = annotations[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_data = list(zip(batch_images, batch_annotations))\n",
    "            img_paths, cropped_images, text_features, labels, bboxes = self.process_batch(batch_data)\n",
    "            self.pad_batch(img_paths, cropped_images, text_features, labels, bboxes)\n",
    "            self.save_batch(batch_idx, img_paths, cropped_images, text_features, labels, bboxes)\n",
    "        return num_batches\n",
    "\n",
    "    def process_batch(self, batch_data):\n",
    "        img_paths = []\n",
    "        cropped_images = []\n",
    "        text_features = []\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        \n",
    "        prev_image_path = None\n",
    "        prev_image = None\n",
    "        for image_path, annotation in batch_data:\n",
    "            try:\n",
    "                if image_path != prev_image_path:\n",
    "                    prev_image = Image.open(image_path).convert(\"RGB\")\n",
    "                    prev_image_path = image_path\n",
    "                    \n",
    "                image = prev_image\n",
    "                bbox = annotation['bbox']\n",
    "                x, y, w, h = bbox\n",
    "                # Crop the image using the bounding box coordinates\n",
    "                cropped_image = image.crop((x, y, x + w, y + h))\n",
    "                bbox = self.correct_bbox_format(bbox)\n",
    "                # Apply the transformation\n",
    "                transformed_image = self.transform(cropped_image).to(self.device)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid image: {image_path}. Error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            caption = annotation['caption']\n",
    "            text_input = self.clip_processor(caption, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_caption_length).to(self.device)\n",
    "            text_feature = self.clip_model.get_text_features(**text_input).to(self.device)\n",
    "            \n",
    "            json_path = image_path.replace('/novice/images/', '/til-24-base/vlm/src/data/imgs/').replace('.jpg', '.npy')\n",
    "            img_paths.append(json_path)\n",
    "            cropped_images.append(transformed_image)\n",
    "            text_features.append(text_feature)\n",
    "            true_label = results.get(caption)\n",
    "            # if not true_label:\n",
    "            #     print(f\"Caption not in results for {caption}\")\n",
    "            # if not true_label['objects'][0]:\n",
    "            #     print(f\"No object for {caption}\")\n",
    "            labels.append(label_to_id_mapping[true_label['objects'][0]])\n",
    "            bboxes.append(bbox)\n",
    "\n",
    "        return img_paths, cropped_images, text_features, labels, bboxes\n",
    "\n",
    "    def pad_batch(self, img_paths, cropped_images, text_features, labels, bboxes):\n",
    "        while len(cropped_images) < self.batch_size:\n",
    "            cropped_images.append(self.padding_image)\n",
    "            text_features.append(self.dummy_text)\n",
    "            labels.append(label_to_id_mapping['padding'])\n",
    "            bboxes.append([0,0,0,0])\n",
    "            file_path = os.path.join(data_dir, 'imgs', 'dummy.npy')\n",
    "            if not os.path.exists(file_path):\n",
    "                image_array = np.array(self.padding_image)\n",
    "                np.save(file_path, image_array)\n",
    "                print(f\"Image saved to {file_path}\")\n",
    "            img_paths.append(file_path)\n",
    "            \n",
    "        return img_paths, cropped_images, text_features, labels, bboxes\n",
    "    \n",
    "    def create_dummy_image(self):\n",
    "        # Create a dummy image with the target size and filled with zeros\n",
    "        dummy_image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        transformed_dummy = self.transform(dummy_image).to(self.device)\n",
    "        if transformed_dummy.dim() == 3:\n",
    "            transformed_dummy = transformed_dummy.permute(1, 2, 0)\n",
    "            array = transformed_dummy.cpu().numpy().astype(np.uint8)\n",
    "            pil_image = Image.fromarray(array)\n",
    "            preprocessed_image = preprocess(pil_image)  # Implement this method based on your needs  \n",
    "            return preprocessed_image\n",
    "\n",
    "    def create_dummy_text(self):\n",
    "        # Create a padding caption tokenized and padded to the max length\n",
    "        padding_text = self.clip_processor.tokenizer.pad_token\n",
    "        text_input = self.clip_processor(\n",
    "            padding_text, \n",
    "            padding=\"max_length\",    # Ensures all tokens are padded to the same length\n",
    "            truncation=True,         # Ensures inputs do not exceed the model's maximum length\n",
    "            max_length=77,           # Optional: specify max length if different from the default\n",
    "            return_tensors=\"pt\"      # Return PyTorch tensors\n",
    "        ).to(self.device)\n",
    "        text_feature = self.clip_model.get_text_features(**text_input)\n",
    "        return text_feature\n",
    "\n",
    "    def save_batch(self, batch_idx, img_paths, cropped_images, text_features, labels, bboxes):\n",
    "        batch_output_dir = os.path.join(self.output_dir, f\"batch_{batch_idx}\")\n",
    "        os.makedirs(batch_output_dir, exist_ok=True)\n",
    "\n",
    "        # Save cropped images as numpy arrays\n",
    "        image_arrays = [img.cpu().numpy() for img in cropped_images]\n",
    "        np.save(os.path.join(batch_output_dir, \"cropped_images.npy\"), np.array(image_arrays))\n",
    "\n",
    "        # Save text_features as numpy arrays\n",
    "        text_feature_arrays = [text.detach().cpu().numpy() for text in text_features]\n",
    "        np.save(os.path.join(batch_output_dir, \"text_features.npy\"), np.array(text_feature_arrays))\n",
    "\n",
    "        # Save bounding boxes as numpy arrays\n",
    "        np.save(os.path.join(batch_output_dir, \"bboxes.npy\"), np.array(bboxes))\n",
    "\n",
    "        # Save labels as numpy arrays\n",
    "        np.save(os.path.join(batch_output_dir, \"labels.npy\"), np.array(labels))\n",
    "\n",
    "        # Save image paths as JSON\n",
    "        with open(os.path.join(batch_output_dir, \"img_paths.json\"), 'w') as f:\n",
    "            json.dump(img_paths, f)\n",
    "\n",
    "        # print(f\"Batch {batch_idx} saved successfully.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def correct_bbox_format(bbox):\n",
    "        x, y, width, height = bbox\n",
    "        xmin = x\n",
    "        ymin = y\n",
    "        xmax = x + width\n",
    "        ymax = y + height\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a4558c-ed74-4914-a6c5-35646777c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41cf840f-cadb-44bc-9725-a6b8f20a05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processor = ImagePreprocessor(train_data, model=clip_model, processor=clip_processor, output_dir=os.path.join(train_dir))\n",
    "val_processor = ImagePreprocessor(val_data, model=clip_model, processor=clip_processor, output_dir=os.path.join(val_dir))\n",
    "test_processor = ImagePreprocessor(test_data, model=clip_model, processor=clip_processor, output_dir=os.path.join(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d982024f-b81b-4cb7-80cb-690bcd3987ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 1709/1709 [13:57<00:00,  2.04it/s]\n",
      "Processing Batches: 100%|██████████| 214/214 [01:27<00:00,  2.45it/s]\n",
      "Processing Batches: 100%|██████████| 214/214 [01:24<00:00,  2.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processor.preprocess_and_save_batches()\n",
    "val_processor.preprocess_and_save_batches()\n",
    "test_processor.preprocess_and_save_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c029a1b9-194a-404f-bd51-623da457fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following code is probably wrong\n",
    "# train_num_batches = (len(train_processor.dataset['image']) + train_processor.batch_size - 1)\n",
    "# val_num_batches = (len(val_processor.dataset['image']) + val_processor.batch_size - 1)\n",
    "# test_num_batches = (len(test_processor.dataset['image']) + test_processor.batch_size - 1)\n",
    "\n",
    "# print(train_num_batches)\n",
    "# print(val_num_batches)\n",
    "# print(test_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f1b67a-6939-40a2-8f14-65a4185c96da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1766079-7888-4944-bc71-68e24f4ca94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageCaptionDataset(IterableDataset):\n",
    "    def __init__(self, image_paths, text_features):\n",
    "        self.image_paths = image_paths\n",
    "        self.text_features = text_features\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        return image, caption\n",
    "    \n",
    "class ASRDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, train_data, val_data, test_data, augmentations=None, collate_fn=None, num_workers=0, transform=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer # can just use the global one?\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.augmentations = augmentations\n",
    "        self.collate_fn = collate_fn\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ASRIterableDataset(self.train_data, self.tokenizer, self.augmentations, shuffle=True, transform=self.transform)\n",
    "        self.val_dataset = ASRIterableDataset(self.val_data, self.tokenizer, self.augmentations, transform=self.transform)\n",
    "        self.test_dataset = ASRIterableDataset(self.test_data, self.tokenizer, self.augmentations, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, collate_fn=self.collate_fn, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c6f72-8c41-4067-864b-1cb26c791798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
