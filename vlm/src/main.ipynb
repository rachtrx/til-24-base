{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60654f-f87c-41ad-b2da-9f55bb2777bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task: `Vision-Language Model`\n",
    "\n",
    "Given an image and a caption describing a target in that image, return a bounding box corresponding to the target’s location within the image.\n",
    "\n",
    "Note that targets within a given image are not uniquely identified by their object class (e.g. ”airplane”, “helicopter”); multiple targets within an image may be members of the same object class. Instead, targets provided will correspond to a particular target description (e.g. “black and white drone”).\n",
    "\n",
    "Not all possible target descriptions will be represented in the training dataset provided to participants. There will also be unseen targets and novel descriptions in the test data used in the hidden test cases of the Virtual Qualifiers, Semi-Finals / Finals. As such, Guardians will have to develop vision models capable of understanding **natural language** to identify the correct target from the scene.\n",
    "\n",
    "For the **image datasets** provided to both Novice and Advanced Guardians, there will be no noise present. However, it is worth noting that your models will have to be adequately robust as the hidden test cases for the Virtual Qualifiers and the Semi-Finals/Finals will have increasing amounts of noise introduced. This is especially crucial for **Advanced Guardians**, due to the degradation of their robot sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace20c2f-1165-4f44-8355-48a1603a1ada",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73ada4-82b8-4359-8063-9604ad683c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U albumentations # Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ad077-8356-48a3-91b3-5c95a2139d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
