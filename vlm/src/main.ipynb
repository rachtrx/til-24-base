{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60654f-f87c-41ad-b2da-9f55bb2777bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task: `Vision-Language Model`\n",
    "\n",
    "Given an image and a caption describing a target in that image, return a bounding box corresponding to the target’s location within the image.\n",
    "\n",
    "Note that targets within a given image are not uniquely identified by their object class (e.g. ”airplane”, “helicopter”); multiple targets within an image may be members of the same object class. Instead, targets provided will correspond to a particular target description (e.g. “black and white drone”).\n",
    "\n",
    "Not all possible target descriptions will be represented in the training dataset provided to participants. There will also be unseen targets and novel descriptions in the test data used in the hidden test cases of the Virtual Qualifiers, Semi-Finals / Finals. As such, Guardians will have to develop vision models capable of understanding **natural language** to identify the correct target from the scene.\n",
    "\n",
    "For the **image datasets** provided to both Novice and Advanced Guardians, there will be no noise present. However, it is worth noting that your models will have to be adequately robust as the hidden test cases for the Virtual Qualifiers and the Semi-Finals/Finals will have increasing amounts of noise introduced. This is especially crucial for **Advanced Guardians**, due to the degradation of their robot sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace20c2f-1165-4f44-8355-48a1603a1ada",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc73ada4-82b8-4359-8063-9604ad683c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install -q -U torchinfo albumentations # Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642ad077-8356-48a3-91b3-5c95a2139d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Enable benchmark mode in cuDNN to find the best algorithm for your hardware\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e6f776-d552-4b79-a4c1-1f79f99452a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/images'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "img_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb25ad-db32-49e1-95af-ff059848c255",
   "metadata": {},
   "source": [
    "# Models: Faster RNN, CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84a15bb-5299-44a8-add6-35db4a2a4b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "# clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccdadba-4def-4748-8501-cdf04c5bc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapIterableDataset(IterableDataset):\n",
    "    def __init__(self, data, shuffle=False):\n",
    "        self.type_dir, self.num_batches = data\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_idx in range(self.num_batches):\n",
    "            batch_path = os.path.join(self.type_dir, f\"batch_{batch_idx}\")\n",
    "\n",
    "\n",
    "            # Load file paths to images from JSON file\n",
    "            image_paths_file = os.path.join(batch_path, \"img_paths.json\")\n",
    "            with open(image_paths_file, 'r') as f:\n",
    "                image_paths_list = json.load(f)\n",
    "\n",
    "            # Unpack the list of image paths\n",
    "            image_paths = [image_path for image_path in image_paths_list]\n",
    "\n",
    "            # Load other batch data\n",
    "            bboxes_path = os.path.join(batch_path, \"bboxes.npy\")\n",
    "            labels_path = os.path.join(batch_path, \"labels.npy\")\n",
    "            text_features_path = os.path.join(batch_path, \"text_features.npy\")\n",
    "\n",
    "            # Load other batch data as numpy arrays\n",
    "            # try:\n",
    "            bboxes_batch = np.load(bboxes_path)\n",
    "            labels_batch = np.load(labels_path)\n",
    "            # except ValueError:\n",
    "            #     # If there's an error suggesting that you need to allow pickling, use allow_pickle=True\n",
    "            #     bboxes_batch = np.load(bboxes_path, allow_pickle=True)\n",
    "            #     labels_batch = np.load(labels_path, allow_pickle=True)\n",
    "            text_features_batch = np.load(text_features_path, mmap_mode='r')\n",
    "\n",
    "            # Convert numpy arrays to torch tensors\n",
    "            bboxes_batch = torch.stack([torch.tensor(b).view(-1, 4) for b in bboxes_batch])\n",
    "            labels_batch = torch.stack([torch.tensor([l]) for l in labels_batch])\n",
    "            # print(bboxes_batch)\n",
    "            # print(labels_batch)\n",
    "            text_features_batch = torch.tensor(text_features_batch) # TO CHECK IF DIM IS TOO HIGH\n",
    "            # print(text_features_batch)\n",
    "\n",
    "            image_tensors = []\n",
    "            for image_path in image_paths:\n",
    "                # Load the image data as a memory-mapped array\n",
    "                image_array = np.load(image_path, mmap_mode='r')\n",
    "\n",
    "                if image_array is None or image_array.shape[0] == 0 or image_array.shape[1] == 0:\n",
    "                    print(f\"Skipping invalid image: {image_path}\")\n",
    "                    continue\n",
    "\n",
    "                image_tensor = torch.from_numpy(image_array).type(torch.float32)\n",
    "    \n",
    "                # Transfer to GPU (if CUDA is available)\n",
    "                image_tensor = image_tensor.to(device)\n",
    "\n",
    "                # Append the tensor to the list\n",
    "                image_tensors.append(image_tensor)\n",
    "\n",
    "            # Now concatenate the list of tensors along dimension 0 to create a batch\n",
    "            if not image_tensors:\n",
    "                print(\"No images to process.\")\n",
    "                continue\n",
    "                \n",
    "            # Stack the list of tensors along dimension 0 to create a batch tensor\n",
    "            image_tensors_batch = torch.stack(image_tensors, dim=0)\n",
    "                \n",
    "            # bboxes_batch = bboxes_batch.to('cpu')\n",
    "            # labels_batch = labels_batch.to('cpu')\n",
    "            # text_features_batch = text_features_batch.to('cpu')\n",
    "\n",
    "            yield image_tensors_batch, bboxes_batch, labels_batch, text_features_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9450bb-ba1a-4fd9-ac0f-97c26eac58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionModule(pl.LightningModule):\n",
    "    def __init__(self, num_classes, train_data, val_data, test_data, learning_rate=1e-3, num_workers=4):\n",
    "        super().__init__()\n",
    "        # Account for an additional dummy class for padding\n",
    "        self.num_classes = num_classes + 1  # Increase number of classes to include a dummy class\n",
    "        # Initialize the Faster R-CNN and CLIP models\n",
    "        self.faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # Replace the classifier in Faster R-CNN\n",
    "        in_features = self.faster_rcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.faster_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n",
    "        \n",
    "        clip_out_dim = 512  # Adjust according to your specific model output\n",
    "        # self.projection = torch.nn.Linear(clip_out_dim, in_features)\n",
    "        \n",
    "        self.text_fc = nn.Linear(clip_out_dim, in_features)  # Assuming text features have 512 dimensions\n",
    "        self.combined_fc = nn.Linear(in_features * 2, in_features)\n",
    "\n",
    "        # Define the fusion layer\n",
    "        # self.fusion_layer = FusionLayer(visual_feature_dim=in_features, text_feature_dim=512, output_dim=num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = MemmapIterableDataset(self.train_data, shuffle=True) # need transform?\n",
    "        self.val_dataset = MemmapIterableDataset(self.val_data)\n",
    "        self.test_dataset = MemmapIterableDataset(self.test_data)\n",
    "\n",
    "    def forward(self, images, text_features):\n",
    "        # Run images through Faster R-CNN\n",
    "        detections = self.faster_rcnn(images)\n",
    "        \n",
    "        # Process text features\n",
    "        text_features = self.text_fc(text_features)\n",
    "        \n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((detections, text_features), dim=1)\n",
    "        combined_features = self.combined_fc(combined_features)\n",
    "        \n",
    "        return combined_features\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, bboxes, labels, text_features = batch\n",
    "        \n",
    "        outputs = self(images, text_features)\n",
    "        \n",
    "        print(isinstance(images, torch.Tensor))\n",
    "        # Filter out dummy data based on labels\n",
    "        labels_1d = labels.squeeze()\n",
    "        valid_indices = labels_1d.nonzero().squeeze()\n",
    "        \n",
    "        if valid_indices.numel() == 0:\n",
    "            # Handle the case where all data might be dummy\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        valid_images = images[valid_indices]\n",
    "        valid_bboxes = bboxes[valid_indices]\n",
    "        valid_labels = labels[valid_indices]\n",
    "        valid_text_features = text_features[valid_indices]\n",
    "\n",
    "        targets = [{'boxes': bbox, 'labels': label} for bbox, label in zip(valid_bboxes, valid_labels)]\n",
    "\n",
    "        # classification_losses = [F.cross_entropy(output['logits'], target['labels']) for output, target in zip(outputs, targets)]\n",
    "        # bbox_losses = [F.mse_loss(output['bbox'], target['boxes']) for output, target in zip(outputs, targets)]\n",
    "        \n",
    "        if self.training and valid_text_features is not None:\n",
    "            outputs = self.augment_losses_with_text(outputs, targets, valid_text_features)\n",
    "\n",
    "        total_loss = sum(outputs.values())\n",
    "        average_loss = total_loss / len(valid_images)  # Normalize by the actual valid batch size\n",
    "        self.log('train_loss', average_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return average_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, bboxes, labels, text_features = batch\n",
    "        # print(isinstance(images, torch.Tensor))\n",
    "        # print(isinstance(labels, torch.Tensor))\n",
    "        # print(isinstance(text_features, torch.Tensor))\n",
    "\n",
    "        # Filter out dummy data based on labels\n",
    "        labels_1d = labels.squeeze()\n",
    "        valid_indices = labels_1d.nonzero().squeeze()\n",
    "        \n",
    "        print(valid_indices)\n",
    "        if valid_indices.numel() == 0:\n",
    "            # Handle the case where all data might be dummy\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        valid_images = images[valid_indices]\n",
    "        valid_bboxes = bboxes[valid_indices]\n",
    "        valid_labels = labels[valid_indices]\n",
    "        valid_text_features = text_features[valid_indices]\n",
    "\n",
    "        targets = [{'boxes': bbox, 'labels': label} for bbox, label in zip(valid_bboxes, valid_labels)]\n",
    "        outputs = self(valid_images, valid_text_features, targets=targets)\n",
    "\n",
    "        # classification_losses = [F.cross_entropy(output['logits'], target['labels']) for output, target in zip(outputs, targets)]\n",
    "        # bbox_losses = [F.mse_loss(output['bbox'], target['boxes']) for output, target in zip(outputs, targets)]\n",
    "\n",
    "        predictions = self.faster_rcnn(valid_images)\n",
    "\n",
    "        # Manually compute the losses from predictions and targets\n",
    "        loss = self.compute_validation_loss(predictions, targets)\n",
    "        \n",
    "        self.log('val_loss', average_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return average_loss\n",
    "    \n",
    "    def compute_validation_loss(self, predictions, targets):\n",
    "        classification_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        bbox_loss_fn = torch.nn.MSELoss()  # Or whatever loss function you use for bbox regression\n",
    "\n",
    "        all_losses = []\n",
    "        for prediction, target in zip(predictions, targets):\n",
    "            # Assuming prediction['scores'] contains the confidence scores\n",
    "            scores = prediction['scores']\n",
    "            pred_labels = prediction['labels']\n",
    "            pred_boxes = prediction['boxes']\n",
    "\n",
    "            # Get the index of the highest scoring prediction\n",
    "            max_score_index = scores.argmax()\n",
    "\n",
    "            # Extract the highest scoring prediction\n",
    "            pred_label = pred_labels[max_score_index].unsqueeze(0)\n",
    "            pred_box = pred_boxes[max_score_index].unsqueeze(0)\n",
    "\n",
    "            true_label = target['labels'].unsqueeze(0)\n",
    "            true_box = target['boxes'].unsqueeze(0)\n",
    "\n",
    "            # Ensure logits (pred_label) are float and targets (true_label) are long\n",
    "            pred_label = pred_label.float()\n",
    "            true_label = true_label.long()\n",
    "\n",
    "            # Calculate classification loss\n",
    "            classification_loss = classification_loss_fn(pred_label, true_label)\n",
    "\n",
    "            # Ensure pred_box and true_box are float\n",
    "            pred_box = pred_box.float()\n",
    "            true_box = true_box.float()\n",
    "\n",
    "            # Calculate bounding box regression loss\n",
    "            bbox_loss = bbox_loss_fn(pred_box, true_box)\n",
    "\n",
    "            # Aggregate losses\n",
    "            total_loss = classification_loss + bbox_loss\n",
    "            all_losses.append(total_loss)\n",
    "\n",
    "        # Return the mean loss across all predictions\n",
    "        total_loss = torch.stack(all_losses).mean()\n",
    "        return total_loss\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, num_workers=self.num_workers)\n",
    "    \n",
    "    # def validation_epoch_end(self, validation_step_outputs):\n",
    "    #     # Collect all batch losses from the validation_step outputs\n",
    "    #     if validation_step_outputs:\n",
    "    #         avg_loss = torch.stack([x for x in validation_step_outputs]).mean()\n",
    "    #         self.log('avg_val_loss', avg_loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "    #         print(f\"Average Validation Loss: {avg_loss.item()}\")\n",
    "    #     else:\n",
    "    #         print(\"No validation data provided or validation_step did not return any outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388cc3f6-1ea2-4b22-a348-c14602bd2ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomProgressBar(ProgressBar):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()  # Initialize the ProgressBar base class\n",
    "\n",
    "#     def init_train_tqdm(self):\n",
    "#         \"\"\"\n",
    "#         This method initializes the tqdm progress bar for training.\n",
    "#         \"\"\"\n",
    "#         bar = super().init_train_tqdm()\n",
    "#         # Adding custom set of metrics to display in the progress bar, e.g., 'train_loss'\n",
    "#         bar.set_description('Training')\n",
    "#         return bar\n",
    "\n",
    "#     def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "#         \"\"\"\n",
    "#         Called when the train batch ends. Updates the progress bar with current loss.\n",
    "#         \"\"\"\n",
    "#         super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)  # Call the base method\n",
    "#         # Here we assume `train_loss` is logged using `self.log('train_loss', loss)` in your training step\n",
    "#         loss = trainer.logged_metrics.get('train_loss', None)\n",
    "#         if loss is not None:\n",
    "#             self.main_progress_bar.set_postfix({'train_loss': f'{loss:.3f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1027af-56f2-4de6-b8af-9e65825bb7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "data_module = ObjectDetectionModule(\n",
    "    num_classes=8,\n",
    "    train_data=(train_dir, 1709),\n",
    "    val_data=(val_dir, 214),\n",
    "    test_data=(test_dir, 214),\n",
    "    learning_rate=1e-3,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # metric to monitor\n",
    "    patience=3,          # no of epochs with no improvement to wait before stopping\n",
    "    verbose=True,        # logging\n",
    "    mode='min'           # minimize or maximize the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize Trainer with model checkpointing\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=1709*10,  # Maximum number of steps (batches) to train for\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback], # CustomProgressBar()\n",
    "    val_check_interval=1709,\n",
    "    limit_val_batches=214,  # Limit the number of validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd81423a-f209-45df-b804-da425a91e700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | faster_rcnn | FasterRCNN | 41.3 M\n",
      "1 | clip_model  | CLIPModel  | 151 M \n",
      "2 | projection  | Linear     | 525 K \n",
      "-------------------------------------------\n",
      "192 M     Trainable params\n",
      "222 K     Non-trainable params\n",
      "193 M     Total params\n",
      "772.551   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de812b159abf4da2b2bdc1a4f0550e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n",
      "[{'boxes': tensor([[708, 252, 852, 316]], device='cuda:0'), 'labels': tensor([4], device='cuda:0')}, {'boxes': tensor([[888,  88, 932, 112]], device='cuda:0'), 'labels': tensor([5], device='cuda:0')}, {'boxes': tensor([[400, 236, 436, 272]], device='cuda:0'), 'labels': tensor([4], device='cuda:0')}, {'boxes': tensor([[580, 104, 680, 148]], device='cuda:0'), 'labels': tensor([6], device='cuda:0')}, {'boxes': tensor([[1016,  420, 1084,  452]], device='cuda:0'), 'labels': tensor([2], device='cuda:0')}, {'boxes': tensor([[1052,   76, 1100,  104]], device='cuda:0'), 'labels': tensor([6], device='cuda:0')}, {'boxes': tensor([[428, 376, 548, 436]], device='cuda:0'), 'labels': tensor([2], device='cuda:0')}]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [100], target: [1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# pl.LightningDataModule can be 2nd parameter\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(data_module)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1021\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    296\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:393\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m, in \u001b[0;36mObjectDetectionModule.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    120\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaster_rcnn(valid_images)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Manually compute the losses from predictions and targets\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_validation_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, average_loss, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m average_loss\n",
      "Cell \u001b[0;32mIn[10], line 144\u001b[0m, in \u001b[0;36mObjectDetectionModule.compute_validation_loss\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m    141\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m true_labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Calculate classification loss\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m classification_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Ensure pred_boxes and true_boxes are float\u001b[39;00m\n\u001b[1;32m    147\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [100], target: [1])"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.fit(data_module) # pl.LightningDataModule can be 2nd parameter\n",
    "\n",
    "# Test the model\n",
    "trainer.test(data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8c0ca-0fb1-4d8f-9365-6e463e2a3ef9",
   "metadata": {},
   "source": [
    "# No of batches (batch_size=8)\n",
    "\n",
    "- Train: 1496\n",
    "- Val: 187\n",
    "- Test: 187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa78c8-1cb2-44c6-bb25-8f448d415893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# file_path = \"/home/jupyter/til-24-base/vlm/src/data/train/batch_0/labels.npy\"\n",
    "file_path = \"/home/jupyter/til-24-base/vlm/src/data/val/batch_46/bboxes.npy\"\n",
    "\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Print the length of the array\n",
    "print(\"Length of the array:\", len(data))\n",
    "\n",
    "# If the array is multidimensional and you want to check the size of each dimension\n",
    "print(\"Shape of the array:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb98f72-a27b-43bf-b50d-a4555e399a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Extract bounding boxes, labels, and scores\n",
    "# boxes = prediction[0]['boxes']\n",
    "# labels = prediction[0]['labels']\n",
    "# scores = prediction[0]['scores']\n",
    "\n",
    "# # Visualize the results\n",
    "# plt.imshow(image)\n",
    "# for box, label, score in zip(boxes, labels, scores):\n",
    "#   if score > 0.1:\n",
    "#     print(id_2_label[label.item()], score.item())\n",
    "#     plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "#                                       fill=False, edgecolor='red', linewidth=2))\n",
    "#     plt.text(box[0], box[1], f\"Class {label.item()} ({score:.2f})\", color='red', fontsize=10,\n",
    "#              bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# print(f\"{id_2_label[labels.item()] =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc35d8-dc7e-447e-bbca-19c94ac1280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_and_save_batches(dataset, augmentations, data_dir='data', batch_size=32):\n",
    "#     clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "#     images = dataset['image']\n",
    "#     annotations = dataset['annotations']\n",
    "#     num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "#     for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "#         batch_images = images[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "#         batch_annotations = annotations[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "#         batch_data = list(zip(batch_images, batch_annotations))\n",
    "#         image_tensors = []\n",
    "#         all_bboxes = []\n",
    "#         all_labels = []\n",
    "#         image_features = []\n",
    "#         text_features = []\n",
    "\n",
    "#         for image_path, image_annotations in batch_data:\n",
    "#             # Load image\n",
    "#             image = cv2.imread(image_path)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             # Apply augmentations\n",
    "#             augmented = augmentations(image=image)\n",
    "#             augmented_image = augmented['image'].permute(1, 2, 0).numpy()\n",
    "#             augmented_image = (augmented_image * 255).astype(np.uint8)\n",
    "            \n",
    "#             bboxes = []\n",
    "#             labels = []\n",
    "\n",
    "#             for annotation in image_annotations:\n",
    "#                 caption = annotation['caption']\n",
    "#                 bbox = annotation['bbox']\n",
    "#                 bboxes.append(bbox)\n",
    "#                 labels.append(caption)\n",
    "\n",
    "#                 # Extract the cropped image\n",
    "#                 cropped_image = augmented_image[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n",
    "#                 cropped_pil_image = Image.fromarray(cropped_image.astype('uint8'))\n",
    "\n",
    "#                 # Preprocess the image for CLIP\n",
    "#                 cropped_preprocessed = preprocess_clip(cropped_pil_image).unsqueeze(0).to(device)\n",
    "                \n",
    "#                 # Encode features using CLIP\n",
    "#                 with torch.no_grad():\n",
    "#                     if cropped_preprocessed.shape[1] == 3:\n",
    "#                         image_feature = clip_model.encode_image(cropped_preprocessed).cpu().numpy()\n",
    "#                         text_feature = clip_model.encode_text(clip.tokenize([caption]).to(device)).cpu().numpy()\n",
    "#                         image_features.append(image_feature)\n",
    "#                         text_features.append(text_feature)\n",
    "#                     else:\n",
    "#                         print(f\"Skipping encoding due to incorrect shape: {cropped_preprocessed.shape}\")\n",
    "\n",
    "#             all_bboxes.append(bboxes)\n",
    "#             all_labels.append(labels)\n",
    "        \n",
    "#         # Save batch to memmap files\n",
    "#         image_batch_memmap_path = os.path.join(data_dir, f\"image_batch_{batch_idx}.npy\")\n",
    "#         np.save(image_batch_memmap_path, np.array(image_tensors))\n",
    "        \n",
    "#         # Save each list of bounding boxes separately\n",
    "#         for i, bboxes in enumerate(all_bboxes):\n",
    "#             bboxes_path = os.path.join(data_dir, f\"bboxes_batch_{batch_idx}_image_{i}.npy\")\n",
    "#             np.save(bboxes_path, np.array(bboxes))\n",
    "        \n",
    "#         labels_path = os.path.join(data_dir, f\"labels_batch_{batch_idx}.npy\")\n",
    "#         np.save(labels_path, np.array(all_labels, dtype=object))\n",
    "        \n",
    "#         image_features_path = os.path.join(data_dir, f\"image_features_batch_{batch_idx}.npy\")\n",
    "#         np.save(image_features_path, np.array(image_features))\n",
    "        \n",
    "#         text_features_path = os.path.join(data_dir, f\"text_features_batch_{batch_idx}.npy\")\n",
    "#         np.save(text_features_path, np.array(text_features))\n",
    "        \n",
    "# def load_image(image_path):\n",
    "#     with Image.open(image_path) as img:\n",
    "#         img = img.convert('RGB')  # Ensure image is in RGB format\n",
    "#         image_array = np.array(img)\n",
    "#     return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edce9f-b63c-45a3-b53c-af6c189f15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = \"/home/jupyter/til-24-base/vlm/src/data/imgs\"\n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg.npy'):\n",
    "        # Construct the full path of the old file\n",
    "        old_file = os.path.join(directory, filename)\n",
    "        \n",
    "        # Create the new file name by replacing '.jpg.npy' with '.npy'\n",
    "        new_file = os.path.join(directory, filename.replace('.jpg.npy', '.npy'))\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file, new_file)\n",
    "        count += 1\n",
    "\n",
    "print(f'Renamed {count} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4129f-6697-4b46-9869-8418f1a05b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(self, images, bboxes=None, labels=None, text_features=None):\n",
    "#         # Check if we are in training or inference mode based on if bboxes and labels are provided\n",
    "#         if bboxes is not None and labels is not None:\n",
    "#             targets = []\n",
    "#             for bbox, label in zip(bboxes, labels):\n",
    "#                 targets.append({\n",
    "#                     'boxes': bbox,   # Tensor of shape [num_objs, 4]\n",
    "#                     'labels': label  # Tensor of shape [num_objs]\n",
    "#                 })\n",
    "#             outputs = self.faster_rcnn(images, targets)\n",
    "#         else:\n",
    "#             outputs = self.faster_rcnn(images)  # Inference mode: No targets are passed\n",
    "\n",
    "#         # Assuming we have a function to extract features from outputs\n",
    "#         if self.training:\n",
    "#             box_features = self.extract_features(outputs, targets)  # Define this method based on your model architecture\n",
    "#             fused_logits = self.fusion_layer(box_features, text_features)\n",
    "#             for output, logits in zip(outputs, fused_logits):\n",
    "#                 output['logits'] = logits\n",
    "\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1cebf-3fc6-41e0-8d5c-bba3439aee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.roi_heads import RoIHeads\n",
    "\n",
    "# class FusionLayer(nn.Module):\n",
    "#     def __init__(self, visual_feature_dim, text_feature_dim, output_dim):\n",
    "#         super(FusionLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(visual_feature_dim + text_feature_dim, output_dim)\n",
    "\n",
    "#     def forward(self, visual_features, text_features):\n",
    "#         combined_features = torch.cat((visual_features, text_features), dim=1)\n",
    "#         output = self.fc(combined_features)\n",
    "#         return output\n",
    "    \n",
    "# class ObjectDetectionModule(pl.LightningModule):\n",
    "#     def __init__(self, num_classes, train_data, val_data, test_data, learning_rate=1e-3, num_workers=4):\n",
    "#         super().__init__()\n",
    "#         # Account for an additional dummy class for padding\n",
    "#         self.num_classes = num_classes + 1  # Increase number of classes to include a dummy class\n",
    "#         # Initialize the Faster R-CNN and CLIP models\n",
    "#         self.faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "#         self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#         self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "#         # Replace the classifier in Faster R-CNN\n",
    "#         in_features = self.faster_rcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "#         self.faster_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "#         # Define the fusion layer\n",
    "#         self.fusion_layer = FusionLayer(visual_feature_dim=in_features, text_feature_dim=512, output_dim=num_classes)\n",
    "#         self.learning_rate = learning_rate\n",
    "        \n",
    "#         self.train_data = train_data\n",
    "#         self.val_data = val_data\n",
    "#         self.test_data = test_data\n",
    "        \n",
    "#         self.num_workers = num_workers\n",
    "        \n",
    "#     def setup(self, stage=None):\n",
    "#         self.train_dataset = MemmapIterableDataset(self.train_data, shuffle=True) # need transform?\n",
    "#         self.val_dataset = MemmapIterableDataset(self.val_data)\n",
    "#         self.test_dataset = MemmapIterableDataset(self.test_data)\n",
    "\n",
    "#     def forward(self, images, bboxes=None, labels=None, text_features=None):\n",
    "#         if bboxes is not None and labels is not None:\n",
    "#             # Targets are provided, so we are in training mode\n",
    "#             targets = [{'boxes': bbox, 'labels': label} for bbox, label in zip(bboxes, labels)]\n",
    "#             loss_dict = self.faster_rcnn(images, targets)  # This will return a dictionary of losses\n",
    "#             predictions = {'loss_dict': loss_dict}  # Encapsulate loss dict in predictions for compatibility\n",
    "#         else:\n",
    "#             # Inference mode: No targets are passed\n",
    "#             predictions = self.faster_rcnn(images)\n",
    "\n",
    "#         if not self.training and text_features is not None:\n",
    "#             # Extract box features for inference mode; adjust this method for training if needed\n",
    "#             box_features = self.extract_box_features(predictions)\n",
    "#             fused_features = self.fusion_layer(box_features, text_features)\n",
    "#             for pred, fused_feature in zip(predictions, fused_features):\n",
    "#                 pred['scores'] = torch.sigmoid(fused_feature)  # Modifying scores based on fused features\n",
    "\n",
    "#         return predictions\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         images, bboxes, labels, text_features = batch\n",
    "#         outputs = self(images, bboxes, labels, text_features)\n",
    "#         loss = self.faster_rcnn.compute_alignment_loss(outputs, bboxes, labels, text_features)  # Adjust the loss computation accordingly\n",
    "#         self.log('train_loss', loss)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         images, bboxes, labels, text_features = batch\n",
    "#         outputs = self(images, bboxes, labels, text_features)\n",
    "#         loss = self.faster_rcnn.compute_alignment_loss(outputs, bboxes, labels, text_features)  # Adjust the loss computation accordingly\n",
    "#         self.log('val_loss', loss)\n",
    "#         return loss\n",
    "    \n",
    "#     def compute_alignment_loss(self, predictions, bboxes, labels, text_features, alpha=0.5):\n",
    "#         # Assuming predictions['loss_dict'] contains Faster R-CNN's native loss components\n",
    "#         ce_loss = predictions['loss_dict']['loss_classifier']\n",
    "#         mse_loss = predictions['loss_dict']['loss_box_reg']\n",
    "\n",
    "#         # Assuming you have a way to get 'fused_features' and it contains logits\n",
    "#         logits = predictions['fused_logits']\n",
    "#         valid_idx = labels > 0  # Assuming labels is a tensor indicating valid data\n",
    "#         valid_logits = logits[valid_idx]\n",
    "#         valid_labels = labels[valid_idx]\n",
    "\n",
    "#         # Cross-entropy loss from logits\n",
    "#         additional_ce_loss = F.cross_entropy(valid_logits, valid_labels)\n",
    "\n",
    "#         # Calculate cosine similarity for alignment loss\n",
    "#         if text_features is not None:\n",
    "#             valid_text_features = text_features[valid_idx]\n",
    "#             cosine_similarity = (valid_logits * valid_text_features).sum(1) / \\\n",
    "#                                 (valid_logits.norm(dim=1) * valid_text_features.norm(dim=1))\n",
    "#             alignment_loss = 1 - cosine_similarity.mean()\n",
    "#         else:\n",
    "#             alignment_loss = 0\n",
    "\n",
    "#         # Combine all losses\n",
    "#         return alpha * (ce_loss + mse_loss + additional_ce_loss) + (1 - alpha) * alignment_loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "#         return optimizer\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.test_dataset, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0630ec-4d2d-42e4-9e55-fe77aa97e3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad8831-a278-4519-bf2b-c8445e218192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validation_step(self, batch, batch_idx):\n",
    "#         images, bboxes, labels, text_features = batch\n",
    "#         valid_indices = [i for i, lbl in enumerate(labels) if lbl != 0]\n",
    "#         if not valid_indices:\n",
    "#             # Handle the case where all data might be dummy\n",
    "#             return 0\n",
    "\n",
    "#         valid_images = images[valid_indices]\n",
    "#         valid_bboxes = [bboxes[i] for i in valid_indices]\n",
    "#         valid_labels = [labels[i] for i in valid_indices]\n",
    "#         valid_text_features = text_features[valid_indices]\n",
    "\n",
    "#         targets = [{'boxes': bbox, 'labels': label} for bbox, label in zip(valid_bboxes, valid_labels)]\n",
    "#         outputs = self(valid_images, valid_text_features, targets=targets)\n",
    "\n",
    "#         losses = []\n",
    "#         for output, target in zip(outputs, targets):\n",
    "#             if 'scores' in output:\n",
    "#                 max_conf_index = output['scores'].argmax()\n",
    "#                 pred_box = output['boxes'][max_conf_index].unsqueeze(0)\n",
    "#                 true_box = target['boxes']\n",
    "#                 loss = F.mse_loss(pred_box, true_box)\n",
    "#                 losses.append(loss)\n",
    "\n",
    "#         average_loss = torch.mean(torch.stack(losses)) if losses else 0\n",
    "#         self.log('val_loss', total_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return average_loss"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
