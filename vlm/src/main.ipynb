{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60654f-f87c-41ad-b2da-9f55bb2777bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task: `Vision-Language Model`\n",
    "\n",
    "Given an image and a caption describing a target in that image, return a bounding box corresponding to the target’s location within the image.\n",
    "\n",
    "Note that targets within a given image are not uniquely identified by their object class (e.g. ”airplane”, “helicopter”); multiple targets within an image may be members of the same object class. Instead, targets provided will correspond to a particular target description (e.g. “black and white drone”).\n",
    "\n",
    "Not all possible target descriptions will be represented in the training dataset provided to participants. There will also be unseen targets and novel descriptions in the test data used in the hidden test cases of the Virtual Qualifiers, Semi-Finals / Finals. As such, Guardians will have to develop vision models capable of understanding **natural language** to identify the correct target from the scene.\n",
    "\n",
    "For the **image datasets** provided to both Novice and Advanced Guardians, there will be no noise present. However, it is worth noting that your models will have to be adequately robust as the hidden test cases for the Virtual Qualifiers and the Semi-Finals/Finals will have increasing amounts of noise introduced. This is especially crucial for **Advanced Guardians**, due to the degradation of their robot sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace20c2f-1165-4f44-8355-48a1603a1ada",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc73ada4-82b8-4359-8063-9604ad683c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install -q -U torchinfo albumentations # Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642ad077-8356-48a3-91b3-5c95a2139d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Enable benchmark mode in cuDNN to find the best algorithm for your hardware\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dcb260-ba1c-40f0-823c-9e5c0aec6512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# for name, param in clip_model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e6f776-d552-4b79-a4c1-1f79f99452a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/images'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "img_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb25ad-db32-49e1-95af-ff059848c255",
   "metadata": {},
   "source": [
    "# Models: Faster RNN, CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84a15bb-5299-44a8-add6-35db4a2a4b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ae6100-e2df-4431-a785-b2cce5812a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDataLoader:\n",
    "    def __init__(self, batch_path):\n",
    "        self.batch_path = batch_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load image paths\n",
    "        with open(os.path.join(self.batch_path, \"rcnn_img_paths.json\"), 'r') as f:\n",
    "            rcnn_image_paths = json.load(f)\n",
    "        with open(os.path.join(self.batch_path, \"clip_img_paths.json\"), 'r') as f:\n",
    "            clip_image_paths = json.load(f)\n",
    "\n",
    "        rcnn_image_tensors = self.load_and_stack_images(rcnn_image_paths)\n",
    "        clip_pixel_values = self.load_and_stack_images(clip_image_paths, img_type='clip')\n",
    "\n",
    "        # Load text data\n",
    "        with open(os.path.join(self.batch_path, \"text_data.json\"), 'r') as f:\n",
    "            text_data = json.load(f)\n",
    "        text_data_tensors = [self.convert_to_tensors(item) for item in text_data]\n",
    "\n",
    "        # Load bounding boxes and labels\n",
    "        bboxes_batch = self.load_bboxes(os.path.join(self.batch_path, \"bboxes.npy\"))\n",
    "        labels_batch = self.load_labels(os.path.join(self.batch_path, \"labels.npy\"))\n",
    "\n",
    "        return rcnn_image_tensors, clip_pixel_values, text_data_tensors, bboxes_batch, labels_batch\n",
    "    \n",
    "    def load_and_stack_images(self, image_paths, img_type='rcnn'):\n",
    "        image_tensors = [self.load_image_to_tensor(image_path) for image_path in image_paths]\n",
    "        # Filter out None values in case of invalid images\n",
    "        image_tensors = [tensor for tensor in image_tensors if tensor is not None]\n",
    "        if not image_tensors:\n",
    "            return None\n",
    "        if img_type == 'rcnn':\n",
    "            return torch.stack(image_tensors)\n",
    "        elif img_type == 'clip':\n",
    "            # Convert all image tensors to a list of dictionaries\n",
    "            clip_inputs = [{\"pixel_values\": tensor.unsqueeze(0)} for tensor in image_tensors]\n",
    "            return clip_inputs\n",
    "        else:\n",
    "            raise ValueError(\"Invalid image type specified. Use 'rcnn' or 'clip'.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image_to_tensor(image_path):\n",
    "        # Load the image data as a memory-mapped array\n",
    "        image_array = np.load(image_path, mmap_mode='r')\n",
    "        if image_array is None or image_array.size == 0:\n",
    "            print(f\"Skipping invalid image: {image_path}\")\n",
    "            return None\n",
    "        # Convert to a PyTorch tensor\n",
    "        image_tensor = torch.from_numpy(image_array).type(torch.float32).to('cuda')\n",
    "        return image_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_tensors(data):\n",
    "        converted_data = {key: torch.tensor(value).to('cuda') for key, value in data.items()}\n",
    "        return converted_data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_bboxes(bboxes_path):\n",
    "        bboxes_batch = np.load(bboxes_path, mmap_mode='r')\n",
    "        # Convert numpy arrays to torch tensors and move to GPU\n",
    "        bboxes_batch = torch.stack([torch.tensor(b).view(-1, 4).to('cuda') for b in bboxes_batch])\n",
    "        return bboxes_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def load_labels(labels_path):\n",
    "        labels_batch = np.load(labels_path, mmap_mode='r')\n",
    "        # Convert numpy arrays to torch tensors and move to GPU\n",
    "        labels_batch = torch.stack([torch.tensor([l]).to('cuda') for l in labels_batch])\n",
    "        return labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccdadba-4def-4748-8501-cdf04c5bc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapIterableDataset(IterableDataset):\n",
    "    def __init__(self, data, shuffle=False):\n",
    "        self.type_dir, self.num_batches = data\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_idx in range(self.num_batches):\n",
    "            batch_path = os.path.join(self.type_dir, f\"batch_{batch_idx}\")\n",
    "            \n",
    "            dataloader = BatchDataLoader(batch_path)\n",
    "            rcnn_image_tensors, clip_pixel_values, text_data_tensors, bboxes_batch, labels_batch = dataloader.load_data()\n",
    "\n",
    "            # Concatenate the list of tensors along dimension 0 to create a batch\n",
    "            if rcnn_image_tensors.size(0) == 0 or len(clip_pixel_values) == 0:\n",
    "                print(\"No images to process.\")\n",
    "                continue\n",
    "\n",
    "            yield rcnn_image_tensors, clip_pixel_values, text_data_tensors, bboxes_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77ecc20-47f8-4393-bcdb-754bae1aeced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDetectionModel(pl.LightningModule):\n",
    "    def __init__(self, train_data, val_data, test_data, num_classes, num_workers):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.rcnn = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "        in_features = self.rcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "        \n",
    "        self.embedding_transform = nn.Linear(512, 256)\n",
    "        \n",
    "        # Allow CLIP model parameters to be trainable (fine-tuning)\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "        # Freeze all parameters first\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze parameters in the last layers of the text model\n",
    "        if 'text_model.encoder.layers.11' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze parameters in the last layers of the vision model\n",
    "        if 'vision_model.encoder.layers.11' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Optionally, adjust parameters related to the output projections if fine-tuning the head is desired\n",
    "        if 'visual_projection.weight' in name or 'text_projection.weight' in name:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = MemmapIterableDataset(self.train_data)\n",
    "        self.val_dataset = MemmapIterableDataset(self.val_data)\n",
    "        self.test_dataset = MemmapIterableDataset(self.test_data)\n",
    "\n",
    "    def forward(self, rcnn_imgs_preprocessed, clip_imgs_preprocessed, clip_texts_preprocessed, targets=None):\n",
    "        batch_feature_maps = []\n",
    "        # losses =[]\n",
    "\n",
    "        for rcnn_img, clip_img, clip_text in zip(rcnn_imgs_preprocessed, clip_imgs_preprocessed, clip_texts_preprocessed):\n",
    "            # Generate embeddings for both image and text from CLIP\n",
    "            image_embeddings, text_embeddings = self.generate_embeddings(clip_img, clip_text)\n",
    "\n",
    "            # Ensure image is in [C, H, W] format and transfer to device\n",
    "            image_tensor = rcnn_img.permute(2, 0, 1).to(self.device)\n",
    "\n",
    "            # Extract feature maps using the RCNN backbone\n",
    "            feature_maps = self.get_feature_maps(image_tensor.unsqueeze(0))['0']\n",
    "\n",
    "            # Modulate feature maps using both image and text embeddings\n",
    "            modulated_feature_maps = self.modulate_features_with_embeddings(feature_maps, image_embeddings, text_embeddings)\n",
    "    \n",
    "            #to remove the batch number\n",
    "            modulated_feature_maps = modulated_feature_maps.squeeze(0)\n",
    "            \n",
    "            # Resize modulated_feature_maps to have size [3, H, W]\n",
    "            modulated_feature_maps = modulated_feature_maps[:3]  # Take the first 3 channels\n",
    "            \n",
    "            # Store processed feature maps\n",
    "            batch_feature_maps.append(modulated_feature_maps)\n",
    "            \n",
    "            # # Compute loss\n",
    "            # loss = compute_loss(modulated_feature_maps, targeted_feature_maps)\n",
    "            # losses.append(loss.item())\n",
    "            \n",
    "        # print(\"Dimensions of batch_image_tensors:\", [t.shape for t in batch_feature_maps])\n",
    "\n",
    "        # Since all operations should be on feature maps post backbone processing\n",
    "        #integrated_features = torch.stack(batch_feature_maps,dim=0)\n",
    "        \n",
    "        integrated_features = batch_feature_maps\n",
    "        \n",
    "        # print(f\"training status: {self.training}\")\n",
    "        \n",
    "        return self.rcnn(integrated_features, targets)\n",
    "    \n",
    "    def generate_embeddings(self, clip_img_preprocessed, clip_text_preprocessed):\n",
    "        with torch.no_grad():\n",
    "            image_embeddings = self.clip_model.get_image_features(**clip_img_preprocessed).to(self.device)\n",
    "            text_embeddings = self.clip_model.get_text_features(**clip_text_preprocessed).to(self.device)\n",
    "        return image_embeddings, text_embeddings\n",
    "    \n",
    "    def get_feature_maps(self, image_tensor):\n",
    "        backbone = self.rcnn.backbone\n",
    "        backbone.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature_maps = backbone(image_tensor)\n",
    "\n",
    "        return feature_maps  # This now returns a dictionary of feature maps\n",
    "    \n",
    "    def modulate_features_with_embeddings(self, feature_maps, image_embeddings, text_embeddings):\n",
    "        # Assuming feature_maps is a batch of feature maps with shape [batch_size, channels, height, width]\n",
    "        # Both image_embeddings and text_embeddings are [batch_size, 512]\n",
    "        \n",
    "        # print(feature_maps.shape)\n",
    "        \n",
    "        image_embeddings_transformed = self.embedding_transform(image_embeddings)  # [batch_size, 256]\n",
    "        text_embeddings_transformed = self.embedding_transform(text_embeddings)    # [batch_size, 256]\n",
    "\n",
    "        # Expand embeddings to match the spatial dimensions of the feature maps\n",
    "        image_embeddings_expanded = image_embeddings_transformed.unsqueeze(-1).unsqueeze(-1)  # [batch_size, 256, 1, 1]\n",
    "        text_embeddings_expanded = text_embeddings_transformed.unsqueeze(-1).unsqueeze(-1)    # [batch_size, 256, 1, 1]\n",
    "        \n",
    "        # print(image_embeddings_expanded.shape)\n",
    "\n",
    "        # Broadcast the embeddings across the spatial dimensions\n",
    "        image_embeddings_expanded = image_embeddings_expanded.expand_as(feature_maps)  # [batch_size, 256, height, width]\n",
    "        text_embeddings_expanded = text_embeddings_expanded.expand_as(feature_maps)    # [batch_size, 256, height, width]\n",
    "\n",
    "        # Concatenate or add embeddings to the feature maps\n",
    "        # Here we choose concatenation for demonstration; dimension 1 is the channel dimension\n",
    "        modulated_feature_maps = torch.cat([feature_maps, image_embeddings_expanded, text_embeddings_expanded], dim=1)\n",
    "\n",
    "        return modulated_feature_maps\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        rcnn_image_tensors, clip_image_tensors, clip_text_data, bboxes_batch, labels_batch = batch\n",
    "\n",
    "        clip_texts_preprocessed = [{key: torch.tensor(value).to(self.device) for key, value in item.items()} for item in clip_text_data]\n",
    "\n",
    "        targets = []\n",
    "        for bboxes, labels in zip(bboxes_batch, labels_batch):\n",
    "            mask = labels != 0\n",
    "            if mask.any():\n",
    "                filtered_bboxes = bboxes[mask]\n",
    "                filtered_labels = labels[mask]\n",
    "\n",
    "                # Construct the target dictionary\n",
    "                target = {\n",
    "                    'boxes': filtered_bboxes.to(self.device),\n",
    "                    'labels': filtered_labels.to(self.device)\n",
    "                }\n",
    "            else:\n",
    "                # Create an empty target dictionary with correct shape and on the correct device\n",
    "                target = {\n",
    "                    'boxes': torch.zeros((0, 4), dtype=torch.float, device=self.device),\n",
    "                    'labels': torch.zeros(0, dtype=torch.int64, device=self.device)\n",
    "                }\n",
    "\n",
    "            targets.append(target)\n",
    "\n",
    "        outputs = self(rcnn_image_tensors, clip_image_tensors, clip_texts_preprocessed, targets)\n",
    "\n",
    "        # print(\"Keys in training\")\n",
    "        # print(outputs.keys())  # Assuming outputs is a dictionary, not a list\n",
    "\n",
    "        # Calculate total loss from various components\n",
    "        if any(t['labels'].numel() > 0 for t in targets):\n",
    "            total_loss = sum(outputs[key] for key in outputs.keys() if 'loss' in key)\n",
    "            self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return total_loss\n",
    "        else:\n",
    "            self.log('train_loss', 0, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return torch.tensor(0.0, requires_grad=True).to(self.device)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        rcnn_image_tensors, clip_image_tensors, clip_text_data, bboxes_batch, labels_batch = batch\n",
    "\n",
    "        clip_texts_preprocessed = [{key: torch.tensor(value).to(self.device) for key, value in item.items()} for item in clip_text_data]\n",
    "\n",
    "        targets = []\n",
    "        for bboxes, labels in zip(bboxes_batch, labels_batch):\n",
    "            # Filter out entries where labels are 0 (masking background or padded elements)\n",
    "            mask = labels != 0\n",
    "            if mask.any():\n",
    "                filtered_bboxes = bboxes[mask]\n",
    "                filtered_labels = labels[mask]\n",
    "\n",
    "                # Construct the target dictionary\n",
    "                target = {\n",
    "                    'boxes': filtered_bboxes.to(self.device),  # Ensure tensors are on the correct device\n",
    "                    'labels': filtered_labels.to(self.device)\n",
    "                }\n",
    "            else:\n",
    "                # Create an empty target dictionary with correct shape and on the correct device\n",
    "                target = {\n",
    "                    'boxes': torch.zeros((0, 4), dtype=torch.float, device=self.device),\n",
    "                    'labels': torch.zeros(0, dtype=torch.int64, device=self.device)\n",
    "                }\n",
    "\n",
    "            targets.append(target)\n",
    "\n",
    "        self.rcnn.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(rcnn_image_tensors, clip_image_tensors, clip_texts_preprocessed, targets)\n",
    "\n",
    "        self.rcnn.eval()\n",
    "\n",
    "#             print(\"Keys in validation\")\n",
    "#             print(outputs.keys())\n",
    "\n",
    "#             print(outputs['loss_classifier'])\n",
    "\n",
    "        if any(t['labels'].numel() > 0 for t in targets):\n",
    "            # Calculate the total validation loss by summing individual components\n",
    "            total_loss = sum(outputs[key] for key in outputs.keys())\n",
    "            self.log('val_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return total_loss\n",
    "        else:\n",
    "            self.log('val_loss', 0, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        rcnn_image_tensors, clip_image_tensors, clip_text_data, _, _ = batch\n",
    "        # Assuming test data might not always have labels available\n",
    "\n",
    "        clip_texts_preprocessed = [{key: torch.tensor(value).to(self.device) for key, value in item.items()} for item in clip_text_data]\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        self.rcnn.eval()\n",
    "\n",
    "        # Disable gradient computation explicitly for safety\n",
    "        with torch.no_grad():\n",
    "            outputs = self(rcnn_image_tensors, clip_image_tensors, clip_texts_preprocessed)\n",
    "\n",
    "        # Extract relevant output details, e.g., predicted boxes, labels, and scores\n",
    "        predictions = {\n",
    "            'boxes': outputs['boxes'],\n",
    "            'labels': outputs['labels'],\n",
    "            'scores': outputs['scores']\n",
    "        }\n",
    "        \n",
    "        return predictions\n",
    "                \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, num_workers=self.num_workers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9450bb-ba1a-4fd9-ac0f-97c26eac58f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES=8\n",
    "TRAIN_NUM_BATCHES = 2991\n",
    "TEST_NUM_BATCHES = 374\n",
    "VAL_NUM_BATCHES = 374\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # metric to monitor\n",
    "    patience=3,          # no of epochs with no improvement to wait before stopping\n",
    "    verbose=True,        # logging\n",
    "    mode='min'           # minimize or maximize the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize Trainer with model checkpointing\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "vlm_model = ImageDetectionModel(\n",
    "    train_data=(train_dir, TRAIN_NUM_BATCHES), \n",
    "    val_data=(val_dir, TEST_NUM_BATCHES), \n",
    "    test_data=(test_dir, VAL_NUM_BATCHES), \n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=TRAIN_NUM_BATCHES*10,  # Maximum number of steps (batches) to train for\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback], # CustomProgressBar()\n",
    "    val_check_interval=TRAIN_NUM_BATCHES,\n",
    "    limit_val_batches=VAL_NUM_BATCHES,  # Limit the number of validation batches\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1027af-56f2-4de6-b8af-9e65825bb7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name                | Type       | Params\n",
      "---------------------------------------------------\n",
      "0 | clip_model          | CLIPModel  | 151 M \n",
      "1 | rcnn                | FasterRCNN | 43.3 M\n",
      "2 | embedding_transform | Linear     | 131 K \n",
      "---------------------------------------------------\n",
      "43.5 M    Trainable params\n",
      "151 M     Non-trainable params\n",
      "194 M     Total params\n",
      "778.803   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/var/tmp/ipykernel_67363/3975651035.py:49: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  image_tensor = torch.from_numpy(image_array).type(torch.float32).to('cuda')\n",
      "/var/tmp/ipykernel_67363/1362314763.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  clip_texts_preprocessed = [{key: torch.tensor(value).to(self.device) for key, value in item.items()} for item in clip_text_data]\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869f5663ea524a48843cb0ce043ac459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_67363/1362314763.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  clip_texts_preprocessed = [{key: torch.tensor(value).to(self.device) for key, value in item.items()} for item in clip_text_data]\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0b7bd5d4de44eb90dec96749c0fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(vlm_model)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvlm_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:742\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    741\u001b[0m _verify_strategy_supports_compile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:785\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(model, test_dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[1;32m    782\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m )\n\u001b[0;32m--> 785\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    787\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-stage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    296\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtest_step_context():\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 210\u001b[0m, in \u001b[0;36mImageDetectionModel.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 210\u001b[0m     rcnn_image_tensors, clip_image_tensors, clip_text_data \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# Assuming test data might not always have labels available\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     clip_texts_preprocessed \u001b[38;5;241m=\u001b[39m [{key: torch\u001b[38;5;241m.\u001b[39mtensor(value)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m item\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m clip_text_data]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.fit(vlm_model)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(vlm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8c0ca-0fb1-4d8f-9365-6e463e2a3ef9",
   "metadata": {},
   "source": [
    "# No of batches (batch_size=8)\n",
    "\n",
    "- Train: 1496\n",
    "- Val: 187\n",
    "- Test: 187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa78c8-1cb2-44c6-bb25-8f448d415893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# file_path = \"/home/jupyter/til-24-base/vlm/src/data/train/batch_0/labels.npy\"\n",
    "file_path = \"/home/jupyter/til-24-base/vlm/src/data/val/batch_46/bboxes.npy\"\n",
    "\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Print the length of the array\n",
    "print(\"Length of the array:\", len(data))\n",
    "\n",
    "# If the array is multidimensional and you want to check the size of each dimension\n",
    "print(\"Shape of the array:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb98f72-a27b-43bf-b50d-a4555e399a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Extract bounding boxes, labels, and scores\n",
    "# boxes = prediction[0]['boxes']\n",
    "# labels = prediction[0]['labels']\n",
    "# scores = prediction[0]['scores']\n",
    "\n",
    "# # Visualize the results\n",
    "# plt.imshow(image)\n",
    "# for box, label, score in zip(boxes, labels, scores):\n",
    "#   if score > 0.1:\n",
    "#     print(id_2_label[label.item()], score.item())\n",
    "#     plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "#                                       fill=False, edgecolor='red', linewidth=2))\n",
    "#     plt.text(box[0], box[1], f\"Class {label.item()} ({score:.2f})\", color='red', fontsize=10,\n",
    "#              bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# print(f\"{id_2_label[labels.item()] =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc35d8-dc7e-447e-bbca-19c94ac1280f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edce9f-b63c-45a3-b53c-af6c189f15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# directory = \"/home/jupyter/til-24-base/vlm/src/data/imgs\"\n",
    "\n",
    "# count = 0\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.jpg.npy'):\n",
    "#         # Construct the full path of the old file\n",
    "#         old_file = os.path.join(directory, filename)\n",
    "        \n",
    "#         # Create the new file name by replacing '.jpg.npy' with '.npy'\n",
    "#         new_file = os.path.join(directory, filename.replace('.jpg.npy', '.npy'))\n",
    "        \n",
    "#         # Rename the file\n",
    "#         os.rename(old_file, new_file)\n",
    "#         count += 1\n",
    "\n",
    "# print(f'Renamed {count} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4129f-6697-4b46-9869-8418f1a05b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1cebf-3fc6-41e0-8d5c-bba3439aee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0630ec-4d2d-42e4-9e55-fe77aa97e3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad8831-a278-4519-bf2b-c8445e218192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataModule(pl.LightningModule):\n",
    "#     def __init__(self, train_data, val_data, test_data, num_classes, num_workers):\n",
    "#         super(CustomDataModule, self).__init__()\n",
    "#         self.model = MultimodalFasterRCNN(num_classes)\n",
    "#         self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#         self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "#         # Allow CLIP model parameters to be trainable (fine-tuning)\n",
    "#         for param in self.clip_model.parameters():\n",
    "#             param.requires_grad = True\n",
    "            \n",
    "#         self.train_data = train_data\n",
    "#         self.val_data = val_data\n",
    "#         self.test_data = test_data\n",
    "#         self.num_workers = num_workers\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         self.train_dataset = MemmapIterableDataset(self.train_data)\n",
    "#         self.val_dataset = MemmapIterableDataset(self.val_data)\n",
    "#         self.test_dataset = MemmapIterableDataset(self.test_data)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         print(self.device)\n",
    "#         images, bboxes, labels, captions = batch\n",
    "#         text_features = self.extract_text_features(captions)\n",
    "        \n",
    "#         outputs = self.model(images, text_features)\n",
    "\n",
    "#         valid_indices = labels.view(-1) != 0  # Flatten labels for masking\n",
    "\n",
    "#         masked_labels = outputs['labels'].view(-1, outputs['labels'].size(-1))[valid_indices]\n",
    "#         masked_boxes = outputs['boxes'].view(-1, outputs['boxes'].size(-1))[valid_indices]\n",
    "        \n",
    "#         targets_labels = labels.view(-1)[valid_indices]\n",
    "#         targets_boxes = bboxes.view(-1, bboxes.size(-1))[valid_indices]\n",
    "\n",
    "#         # Compute loss based on masked outputs and targets\n",
    "#         loss = self.compute_loss(masked_labels, masked_boxes, targets_labels, targets_boxes)\n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         images, bboxes, labels, captions = batch\n",
    "#         text_features = self.extract_text_features(captions)\n",
    "        \n",
    "#         outputs = self.model(images, text_features)\n",
    "\n",
    "#         valid_indices = labels.view(-1) != 0  # Flatten labels for masking\n",
    "\n",
    "#         masked_labels = outputs['labels'].view(-1, outputs['labels'].size(-1))[valid_indices]\n",
    "#         masked_boxes = outputs['boxes'].view(-1, outputs['boxes'].size(-1))[valid_indices]\n",
    "        \n",
    "#         targets_labels = labels.view(-1)[valid_indices]\n",
    "#         targets_boxes = bboxes.view(-1, bboxes.size(-1))[valid_indices]\n",
    "\n",
    "#         # Compute loss based on masked outputs and targets\n",
    "#         loss = self.compute_loss(masked_labels, masked_boxes, targets_labels, targets_boxes)\n",
    "#         self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def compute_loss(self, masked_labels, masked_boxes, targets_labels, targets_boxes):\n",
    "#         # Custom loss computation using masked outputs and targets\n",
    "#         classification_loss = torch.nn.CrossEntropyLoss()(masked_labels, targets_labels)\n",
    "#         bbox_loss = torch.nn.MSELoss()(masked_boxes, targets_boxes)\n",
    "#         total_loss = classification_loss + bbox_loss\n",
    "#         return total_loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             list(self.model.parameters()) + list(self.clip_model.parameters()),\n",
    "#             lr=1e-4\n",
    "#         )\n",
    "#         return optimizer\n",
    "    \n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.test_dataset, num_workers=self.num_workers)\n",
    "    \n",
    "#     def extract_text_features(self, captions):\n",
    "#         inputs = self.clip_processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "#         for key, value in inputs.items():\n",
    "#             print(f\"{key} is on device: {value.device}\")\n",
    "#         text_features = self.clip_model.get_text_features(**inputs)\n",
    "#         print(f\"Text features are on device: {text_features.device}\")\n",
    "#         return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9e80-35c6-459d-9190-2f7b4b96603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# # Get the dimension of the text features\n",
    "# text_feature_dim = clip_model.config.text_config.hidden_size\n",
    "# print(f\"Text feature dimension: {text_feature_dim}\")\n",
    "\n",
    "# print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c648b-be7c-4043-863e-6bde6714c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def modulate_features_with_embeddings(self, feature_maps, image_embeddings, text_embeddings):\n",
    "#         # Assuming feature_maps is a batch of feature maps with shape [batch_size, channels, height, width]\n",
    "#         # Both image_embeddings and text_embeddings are [batch_size, 512]\n",
    "        \n",
    "#         image_embeddings_transformed = self.embedding_transform(image_embeddings)  # [batch_size, 256]\n",
    "#         text_embeddings_transformed = self.embedding_transform(text_embeddings)    # [batch_size, 256]\n",
    "\n",
    "#         # Assume image_embeddings and text_embeddings are already transformed to match the channel dimension, i.e., [batch_size, 256]\n",
    "#         image_embeddings_expanded = image_embeddings.unsqueeze(-1).unsqueeze(-1)  # [batch_size, 256, 1, 1]\n",
    "#         text_embeddings_expanded = text_embeddings.unsqueeze(-1).unsqueeze(-1)    # [batch_size, 256, 1, 1]\n",
    "\n",
    "#         # Calculate the repeat factors for height and width\n",
    "#         height_repeat = feature_maps.size(2)\n",
    "#         width_repeat = feature_maps.size(3)\n",
    "\n",
    "#         # Use repeat to manually expand to the required dimensions\n",
    "#         image_embeddings_expanded = image_embeddings_expanded.repeat(1, 1, height_repeat, width_repeat)\n",
    "#         text_embeddings_expanded = text_embeddings_expanded.repeat(1, 1, height_repeat, width_repeat)\n",
    "\n",
    "#         # Now you can concatenate them with feature maps\n",
    "#         modulated_feature_maps = torch.cat([feature_maps, image_embeddings_expanded, text_embeddings_expanded], dim=1)\n",
    "        \n",
    "#         return modulated_feature_maps"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
