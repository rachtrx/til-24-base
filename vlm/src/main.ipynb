{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60654f-f87c-41ad-b2da-9f55bb2777bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task: `Vision-Language Model`\n",
    "\n",
    "Given an image and a caption describing a target in that image, return a bounding box corresponding to the target’s location within the image.\n",
    "\n",
    "Note that targets within a given image are not uniquely identified by their object class (e.g. ”airplane”, “helicopter”); multiple targets within an image may be members of the same object class. Instead, targets provided will correspond to a particular target description (e.g. “black and white drone”).\n",
    "\n",
    "Not all possible target descriptions will be represented in the training dataset provided to participants. There will also be unseen targets and novel descriptions in the test data used in the hidden test cases of the Virtual Qualifiers, Semi-Finals / Finals. As such, Guardians will have to develop vision models capable of understanding **natural language** to identify the correct target from the scene.\n",
    "\n",
    "For the **image datasets** provided to both Novice and Advanced Guardians, there will be no noise present. However, it is worth noting that your models will have to be adequately robust as the hidden test cases for the Virtual Qualifiers and the Semi-Finals/Finals will have increasing amounts of noise introduced. This is especially crucial for **Advanced Guardians**, due to the degradation of their robot sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace20c2f-1165-4f44-8355-48a1603a1ada",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc73ada4-82b8-4359-8063-9604ad683c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install -q -U torchinfo albumentations # Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642ad077-8356-48a3-91b3-5c95a2139d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Enable benchmark mode in cuDNN to find the best algorithm for your hardware\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dcb260-ba1c-40f0-823c-9e5c0aec6512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# for name, param in clip_model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e6f776-d552-4b79-a4c1-1f79f99452a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/images'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "img_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb25ad-db32-49e1-95af-ff059848c255",
   "metadata": {},
   "source": [
    "# Models: Faster RNN, CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84a15bb-5299-44a8-add6-35db4a2a4b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ae6100-e2df-4431-a785-b2cce5812a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDataLoader:\n",
    "    def __init__(self, batch_path):\n",
    "        self.batch_path = batch_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load image paths\n",
    "        with open(os.path.join(self.batch_path, \"rcnn_img_paths.json\"), 'r') as f:\n",
    "            rcnn_image_paths = json.load(f)\n",
    "        with open(os.path.join(self.batch_path, \"clip_img_paths.json\"), 'r') as f:\n",
    "            clip_image_paths = json.load(f)\n",
    "\n",
    "        rcnn_image_tensors = self.load_and_stack_images(rcnn_image_paths)\n",
    "        clip_pixel_values = self.load_and_stack_images(clip_image_paths, img_type='clip')\n",
    "\n",
    "        # Load text data\n",
    "        with open(os.path.join(self.batch_path, \"text_data.json\"), 'r') as f:\n",
    "            text_data = json.load(f)\n",
    "        text_data_tensors = [self.convert_to_tensors(item) for item in text_data]\n",
    "\n",
    "        # Load bounding boxes and labels\n",
    "        bboxes_batch = self.load_bboxes(os.path.join(self.batch_path, \"bboxes.npy\"))\n",
    "        labels_batch = self.load_labels(os.path.join(self.batch_path, \"labels.npy\"))\n",
    "\n",
    "        return rcnn_image_tensors, clip_pixel_values, text_data_tensors, bboxes_batch, labels_batch\n",
    "    \n",
    "    def load_and_stack_images(self, image_paths, img_type='rcnn'):\n",
    "        image_tensors = [self.load_image_to_tensor(image_path) for image_path in image_paths]\n",
    "        # Filter out None values in case of invalid images\n",
    "        image_tensors = [tensor for tensor in image_tensors if tensor is not None]\n",
    "        if not image_tensors:\n",
    "            return None\n",
    "        if img_type == 'rcnn':\n",
    "            return torch.stack(image_tensors)\n",
    "        elif img_type == 'clip':\n",
    "            # Convert all image tensors to a list of dictionaries\n",
    "            clip_inputs = [{\"pixel_values\": tensor.unsqueeze(0)} for tensor in image_tensors]\n",
    "            return clip_inputs\n",
    "        else:\n",
    "            raise ValueError(\"Invalid image type specified. Use 'rcnn' or 'clip'.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image_to_tensor(image_path):\n",
    "        # Load the image data as a memory-mapped array\n",
    "        image_array = np.load(image_path, mmap_mode='r')\n",
    "        if image_array is None or image_array.size == 0:\n",
    "            print(f\"Skipping invalid image: {image_path}\")\n",
    "            return None\n",
    "        # Convert to a PyTorch tensor\n",
    "        image_tensor = torch.from_numpy(image_array).type(torch.float32).to('cuda')\n",
    "        return image_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_tensors(data):\n",
    "        converted_data = {key: torch.tensor(value).to('cuda') for key, value in data.items()}\n",
    "        return converted_data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_bboxes(bboxes_path):\n",
    "        bboxes_batch = np.load(bboxes_path, mmap_mode='r')\n",
    "        # Convert numpy arrays to torch tensors and move to GPU\n",
    "        bboxes_batch = torch.stack([torch.tensor(b).view(-1, 4).to('cuda') for b in bboxes_batch])\n",
    "        return bboxes_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def load_labels(labels_path):\n",
    "        labels_batch = np.load(labels_path, mmap_mode='r')\n",
    "        # Convert numpy arrays to torch tensors and move to GPU\n",
    "        labels_batch = torch.stack([torch.tensor([l]).to('cuda') for l in labels_batch])\n",
    "        return labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccdadba-4def-4748-8501-cdf04c5bc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapIterableDataset(IterableDataset):\n",
    "    def __init__(self, data, shuffle=False):\n",
    "        self.type_dir, self.num_batches = data\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_idx in range(self.num_batches):\n",
    "            batch_path = os.path.join(self.type_dir, f\"batch_{batch_idx}\")\n",
    "            \n",
    "            dataloader = BatchDataLoader(batch_path)\n",
    "            rcnn_image_tensors, clip_pixel_values, text_data_tensors, bboxes_batch, labels_batch = dataloader.load_data()\n",
    "\n",
    "            # Concatenate the list of tensors along dimension 0 to create a batch\n",
    "            if rcnn_image_tensors.size(0) == 0 or len(clip_pixel_values) == 0:\n",
    "                print(\"No images to process.\")\n",
    "                continue\n",
    "\n",
    "            yield rcnn_image_tensors, clip_pixel_values, text_data_tensors, bboxes_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77ecc20-47f8-4393-bcdb-754bae1aeced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDetectionModel(pl.LightningModule):\n",
    "    def __init__(self, train_data, val_data, test_data, num_classes, num_workers):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.rcnn = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "        in_features = self.rcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "        \n",
    "        self.embedding_transform = nn.Linear(512, 256)\n",
    "        \n",
    "        # Allow CLIP model parameters to be trainable (fine-tuning)\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "        # Freeze all parameters first\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze parameters in the last layers of the text model\n",
    "        if 'text_model.encoder.layers.11' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze parameters in the last layers of the vision model\n",
    "        if 'vision_model.encoder.layers.11' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Optionally, adjust parameters related to the output projections if fine-tuning the head is desired\n",
    "        if 'visual_projection.weight' in name or 'text_projection.weight' in name:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = MemmapIterableDataset(self.train_data)\n",
    "        self.val_dataset = MemmapIterableDataset(self.val_data)\n",
    "        self.test_dataset = MemmapIterableDataset(self.test_data)\n",
    "\n",
    "    def forward(self, rcnn_imgs_preprocessed, clip_imgs_preprocessed, clip_texts_preprocessed, targets=None):\n",
    "        batch_feature_maps = []\n",
    "        # losses =[]\n",
    "\n",
    "        for rcnn_img, clip_img, clip_text in zip(rcnn_imgs_preprocessed, clip_imgs_preprocessed, clip_texts_preprocessed):\n",
    "            # Generate embeddings for both image and text from CLIP\n",
    "            print(type(rcnn_img), type(clip_img), type(clip_text))\n",
    "            image_embeddings, text_embeddings = self.generate_embeddings(clip_img, clip_text)\n",
    "\n",
    "            # Ensure image is in [C, H, W] format and transfer to device\n",
    "            image_tensor = rcnn_img.permute(2, 0, 1).to(self.device)\n",
    "\n",
    "            # Extract feature maps using the RCNN backbone\n",
    "            feature_maps = self.get_feature_maps(image_tensor.unsqueeze(0))['0']\n",
    "\n",
    "            # Modulate feature maps using both image and text embeddings\n",
    "            modulated_feature_maps = self.modulate_features_with_embeddings(feature_maps, image_embeddings, text_embeddings)\n",
    "    \n",
    "            #to remove the batch number\n",
    "            modulated_feature_maps = modulated_feature_maps.squeeze(0)\n",
    "            \n",
    "            # Resize modulated_feature_maps to have size [3, H, W]\n",
    "            modulated_feature_maps = modulated_feature_maps[:3]  # Take the first 3 channels\n",
    "            \n",
    "            # Store processed feature maps\n",
    "            batch_feature_maps.append(modulated_feature_maps)\n",
    "            \n",
    "            # # Compute loss\n",
    "            # loss = compute_loss(modulated_feature_maps, targeted_feature_maps)\n",
    "            # losses.append(loss.item())\n",
    "            \n",
    "        # print(\"Dimensions of batch_image_tensors:\", [t.shape for t in batch_feature_maps])\n",
    "\n",
    "        # Since all operations should be on feature maps post backbone processing\n",
    "        #integrated_features = torch.stack(batch_feature_maps,dim=0)\n",
    "        \n",
    "        integrated_features = batch_feature_maps\n",
    "        \n",
    "        # print(f\"training status: {self.training}\")\n",
    "        \n",
    "        return self.rcnn(integrated_features, targets)\n",
    "    \n",
    "    def generate_embeddings(self, clip_img_preprocessed, clip_text_preprocessed):\n",
    "        with torch.no_grad():\n",
    "            image_embeddings = self.clip_model.get_image_features(**clip_img_preprocessed).to(self.device)\n",
    "            text_embeddings = self.clip_model.get_text_features(**clip_text_preprocessed).to(self.device)\n",
    "        return image_embeddings, text_embeddings\n",
    "    \n",
    "    def get_feature_maps(self, image_tensor):\n",
    "        backbone = self.rcnn.backbone\n",
    "        backbone.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature_maps = backbone(image_tensor)\n",
    "\n",
    "        return feature_maps  # This now returns a dictionary of feature maps\n",
    "    \n",
    "    def modulate_features_with_embeddings(self, feature_maps, image_embeddings, text_embeddings):\n",
    "        # Assuming feature_maps is a batch of feature maps with shape [batch_size, channels, height, width]\n",
    "        # Both image_embeddings and text_embeddings are [batch_size, 512]\n",
    "        \n",
    "        # print(feature_maps.shape)\n",
    "        \n",
    "        image_embeddings_transformed = self.embedding_transform(image_embeddings)  # [batch_size, 256]\n",
    "        text_embeddings_transformed = self.embedding_transform(text_embeddings)    # [batch_size, 256]\n",
    "\n",
    "        # Expand embeddings to match the spatial dimensions of the feature maps\n",
    "        image_embeddings_expanded = image_embeddings_transformed.unsqueeze(-1).unsqueeze(-1)  # [batch_size, 256, 1, 1]\n",
    "        text_embeddings_expanded = text_embeddings_transformed.unsqueeze(-1).unsqueeze(-1)    # [batch_size, 256, 1, 1]\n",
    "        \n",
    "        # print(image_embeddings_expanded.shape)\n",
    "\n",
    "        # Broadcast the embeddings across the spatial dimensions\n",
    "        image_embeddings_expanded = image_embeddings_expanded.expand_as(feature_maps)  # [batch_size, 256, height, width]\n",
    "        text_embeddings_expanded = text_embeddings_expanded.expand_as(feature_maps)    # [batch_size, 256, height, width]\n",
    "\n",
    "        # Concatenate or add embeddings to the feature maps\n",
    "        # Here we choose concatenation for demonstration; dimension 1 is the channel dimension\n",
    "        modulated_feature_maps = torch.cat([feature_maps, image_embeddings_expanded, text_embeddings_expanded], dim=1)\n",
    "\n",
    "        return modulated_feature_maps\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        rcnn_image_tensors, clip_image_tensors, clip_text_data, bboxes_batch, labels_batch = batch\n",
    "        \n",
    "        if len(rcnn_image_tensors) > 0:\n",
    "            print(\"rcnn_image_tensors[0].shape:\", rcnn_image_tensors[0].shape)\n",
    "        if len(clip_image_tensors) > 0:\n",
    "            print(\"clip_image_tensors[0].shape:\", clip_image_tensors[0].shape)\n",
    "        if len(clip_text_data) > 0:\n",
    "            print(\"clip_text_data[0].shape:\", clip_text_data[0].shape)\n",
    "        \n",
    "        # Move tensors to GPU in the training step\n",
    "        rcnn_image_tensors = rcnn_image_tensors.to(self.device)\n",
    "        clip_image_tensors = [{key: val.to(self.device) for key, val in item.items()} for item in clip_image_tensors]\n",
    "        clip_texts_preprocessed = [{key: val.to(self.device) for key, val in item.items()} for item in clip_text_data]\n",
    "        bboxes_batch = bboxes_batch.to(self.device)\n",
    "        labels_batch = labels_batch.to(self.device)\n",
    "        \n",
    "        targets = []\n",
    "        for bboxes, labels in zip(bboxes_batch, labels_batch):\n",
    "            mask = labels != 0\n",
    "            if mask.any():\n",
    "                filtered_bboxes = bboxes[mask]\n",
    "                filtered_labels = labels[mask]\n",
    "\n",
    "                # Construct the target dictionary\n",
    "                target = {\n",
    "                    'boxes': filtered_bboxes.to(self.device),\n",
    "                    'labels': filtered_labels.to(self.device)\n",
    "                }\n",
    "            else:\n",
    "                # Create an empty target dictionary with correct shape and on the correct device\n",
    "                target = {\n",
    "                    'boxes': torch.zeros((0, 4), dtype=torch.float, device=self.device),\n",
    "                    'labels': torch.zeros(0, dtype=torch.int64, device=self.device)\n",
    "                }\n",
    "\n",
    "            targets.append(target)\n",
    "\n",
    "        outputs = self(rcnn_image_tensors, clip_image_tensors, clip_texts_preprocessed, targets)\n",
    "\n",
    "        # print(\"Keys in training\")\n",
    "        # print(outputs.keys())  # Assuming outputs is a dictionary, not a list\n",
    "\n",
    "        # Calculate total loss from various components\n",
    "        if any(t['labels'].numel() > 0 for t in targets):\n",
    "            total_loss = sum(outputs[key] for key in outputs.keys() if 'loss' in key)\n",
    "            self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return total_loss\n",
    "        else:\n",
    "            self.log('train_loss', 0, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return torch.tensor(0.0, requires_grad=True).to(self.device)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        rcnn_image_tensors, clip_image_tensors, clip_text_data, bboxes_batch, labels_batch = batch\n",
    "        \n",
    "        # Move tensors to GPU in the training step\n",
    "        rcnn_image_tensors = rcnn_image_tensors.to(self.device)\n",
    "        clip_image_tensors = [{key: val.to(self.device) for key, val in item.items()} for item in clip_image_tensors]\n",
    "        clip_texts_preprocessed = [{key: val.to(self.device) for key, val in item.items()} for item in clip_text_data]\n",
    "        bboxes_batch = bboxes_batch.to(self.device)\n",
    "        labels_batch = labels_batch.to(self.device)\n",
    "\n",
    "        targets = []\n",
    "        for bboxes, labels in zip(bboxes_batch, labels_batch):\n",
    "            # Filter out entries where labels are 0 (masking background or padded elements)\n",
    "            mask = labels != 0\n",
    "            if mask.any():\n",
    "                filtered_bboxes = bboxes[mask]\n",
    "                filtered_labels = labels[mask]\n",
    "\n",
    "                # Construct the target dictionary\n",
    "                target = {\n",
    "                    'boxes': filtered_bboxes.to(self.device),  # Ensure tensors are on the correct device\n",
    "                    'labels': filtered_labels.to(self.device)\n",
    "                }\n",
    "            else:\n",
    "                # Create an empty target dictionary with correct shape and on the correct device\n",
    "                target = {\n",
    "                    'boxes': torch.zeros((0, 4), dtype=torch.float, device=self.device),\n",
    "                    'labels': torch.zeros(0, dtype=torch.int64, device=self.device)\n",
    "                }\n",
    "\n",
    "            targets.append(target)\n",
    "\n",
    "        self.rcnn.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(rcnn_image_tensors, clip_image_tensors, clip_texts_preprocessed, targets)\n",
    "\n",
    "        self.rcnn.eval()\n",
    "\n",
    "#             print(\"Keys in validation\")\n",
    "#             print(outputs.keys())\n",
    "\n",
    "#             print(outputs['loss_classifier'])\n",
    "\n",
    "        if any(t['labels'].numel() > 0 for t in targets):\n",
    "            # Calculate the total validation loss by summing individual components\n",
    "            total_loss = sum(outputs[key] for key in outputs.keys())\n",
    "            self.log('val_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return total_loss\n",
    "        else:\n",
    "            self.log('val_loss', 0, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        rcnn_image_tensors, clip_image_tensors, clip_text_data, _, _ = batch\n",
    "        # Assuming test data might not always have labels available\n",
    "\n",
    "        rcnn_image_tensors = rcnn_image_tensors.to(self.device)\n",
    "        clip_image_tensors = [{key: val.to(self.device) for key, val in item.items()} for item in clip_image_tensors]\n",
    "        clip_texts_preprocessed = [{key: val.to(self.device) for key, val in item.items()} for item in clip_text_data]\n",
    "        \n",
    "        # Put model in evaluation mode\n",
    "        self.rcnn.eval()\n",
    "\n",
    "        # Disable gradient computation explicitly for safety\n",
    "        with torch.no_grad():\n",
    "            outputs = self(rcnn_image_tensors, clip_image_tensors, clip_texts_preprocessed)\n",
    "\n",
    "        # Extract relevant output details, e.g., predicted boxes, labels, and scores\n",
    "        for output in outputs:\n",
    "            print(\"Boxes: \", output['boxes'])\n",
    "            print(\"Labels: \", output['labels'])\n",
    "            print(\"Scores: \", output['scores'])\n",
    "        # predictions = {\n",
    "        #     'boxes': outputs['boxes'],\n",
    "        #     'labels': outputs['labels'],\n",
    "        #     'scores': outputs['scores']\n",
    "        # }\n",
    "        \n",
    "        return output\n",
    "                \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9450bb-ba1a-4fd9-ac0f-97c26eac58f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES=8\n",
    "TRAIN_NUM_BATCHES = 2991\n",
    "TEST_NUM_BATCHES = 374\n",
    "VAL_NUM_BATCHES = 374\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # metric to monitor\n",
    "    patience=3,          # no of epochs with no improvement to wait before stopping\n",
    "    verbose=True,        # logging\n",
    "    mode='min'           # minimize or maximize the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize Trainer with model checkpointing\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "vlm_model = ImageDetectionModel(\n",
    "    train_data=(train_dir, TRAIN_NUM_BATCHES), \n",
    "    val_data=(val_dir, TEST_NUM_BATCHES), \n",
    "    test_data=(test_dir, VAL_NUM_BATCHES), \n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=TRAIN_NUM_BATCHES*10,  # Maximum number of steps (batches) to train for\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback], # CustomProgressBar()\n",
    "    val_check_interval=TRAIN_NUM_BATCHES,\n",
    "    limit_val_batches=VAL_NUM_BATCHES,  # Limit the number of validation batches\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea1027af-56f2-4de6-b8af-9e65825bb7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory model_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name                | Type       | Params\n",
      "---------------------------------------------------\n",
      "0 | clip_model          | CLIPModel  | 151 M \n",
      "1 | rcnn                | FasterRCNN | 43.3 M\n",
      "2 | embedding_transform | Linear     | 131 K \n",
      "---------------------------------------------------\n",
      "43.5 M    Trainable params\n",
      "151 M     Non-trainable params\n",
      "194 M     Total params\n",
      "778.803   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'dict'> <class 'dict'>\n",
      "<class 'torch.Tensor'> <class 'dict'> <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'dict'> <class 'dict'>\n",
      "<class 'torch.Tensor'> <class 'dict'> <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71878e32a4424509a9d3d35adad99f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rcnn_image_tensors[0].shape: torch.Size([870, 1520, 3])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvlm_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# trainer.test(vlm_model)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:219\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:188\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         closure()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:266\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:145\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 145\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    148\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1270\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1234\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;124;03m    calls the optimizer.\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:161\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:231\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:116\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:148\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 148\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    151\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:103\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:142\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:128\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 128\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    296\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:380\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 128\u001b[0m, in \u001b[0;36mImageDetectionModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrcnn_image_tensors[0].shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rcnn_image_tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clip_image_tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_image_tensors[0].shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mclip_image_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clip_text_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_text_data[0].shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, clip_text_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# # Train the model\n",
    "trainer.fit(vlm_model)\n",
    "\n",
    "# Test the model\n",
    "# trainer.test(vlm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8c0ca-0fb1-4d8f-9365-6e463e2a3ef9",
   "metadata": {},
   "source": [
    "# No of batches (batch_size=8)\n",
    "\n",
    "- Train: 1496\n",
    "- Val: 187\n",
    "- Test: 187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa78c8-1cb2-44c6-bb25-8f448d415893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# file_path = \"/home/jupyter/til-24-base/vlm/src/data/train/batch_0/labels.npy\"\n",
    "file_path = \"/home/jupyter/til-24-base/vlm/src/data/val/batch_46/bboxes.npy\"\n",
    "\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Print the length of the array\n",
    "print(\"Length of the array:\", len(data))\n",
    "\n",
    "# If the array is multidimensional and you want to check the size of each dimension\n",
    "print(\"Shape of the array:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb98f72-a27b-43bf-b50d-a4555e399a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Extract bounding boxes, labels, and scores\n",
    "# boxes = prediction[0]['boxes']\n",
    "# labels = prediction[0]['labels']\n",
    "# scores = prediction[0]['scores']\n",
    "\n",
    "# # Visualize the results\n",
    "# plt.imshow(image)\n",
    "# for box, label, score in zip(boxes, labels, scores):\n",
    "#   if score > 0.1:\n",
    "#     print(id_2_label[label.item()], score.item())\n",
    "#     plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "#                                       fill=False, edgecolor='red', linewidth=2))\n",
    "#     plt.text(box[0], box[1], f\"Class {label.item()} ({score:.2f})\", color='red', fontsize=10,\n",
    "#              bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# print(f\"{id_2_label[labels.item()] =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc35d8-dc7e-447e-bbca-19c94ac1280f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edce9f-b63c-45a3-b53c-af6c189f15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# directory = \"/home/jupyter/til-24-base/vlm/src/data/imgs\"\n",
    "\n",
    "# count = 0\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.jpg.npy'):\n",
    "#         # Construct the full path of the old file\n",
    "#         old_file = os.path.join(directory, filename)\n",
    "        \n",
    "#         # Create the new file name by replacing '.jpg.npy' with '.npy'\n",
    "#         new_file = os.path.join(directory, filename.replace('.jpg.npy', '.npy'))\n",
    "        \n",
    "#         # Rename the file\n",
    "#         os.rename(old_file, new_file)\n",
    "#         count += 1\n",
    "\n",
    "# print(f'Renamed {count} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4129f-6697-4b46-9869-8418f1a05b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1cebf-3fc6-41e0-8d5c-bba3439aee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0630ec-4d2d-42e4-9e55-fe77aa97e3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad8831-a278-4519-bf2b-c8445e218192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataModule(pl.LightningModule):\n",
    "#     def __init__(self, train_data, val_data, test_data, num_classes, num_workers):\n",
    "#         super(CustomDataModule, self).__init__()\n",
    "#         self.model = MultimodalFasterRCNN(num_classes)\n",
    "#         self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#         self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "#         # Allow CLIP model parameters to be trainable (fine-tuning)\n",
    "#         for param in self.clip_model.parameters():\n",
    "#             param.requires_grad = True\n",
    "            \n",
    "#         self.train_data = train_data\n",
    "#         self.val_data = val_data\n",
    "#         self.test_data = test_data\n",
    "#         self.num_workers = num_workers\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         self.train_dataset = MemmapIterableDataset(self.train_data)\n",
    "#         self.val_dataset = MemmapIterableDataset(self.val_data)\n",
    "#         self.test_dataset = MemmapIterableDataset(self.test_data)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         print(self.device)\n",
    "#         images, bboxes, labels, captions = batch\n",
    "#         text_features = self.extract_text_features(captions)\n",
    "        \n",
    "#         outputs = self.model(images, text_features)\n",
    "\n",
    "#         valid_indices = labels.view(-1) != 0  # Flatten labels for masking\n",
    "\n",
    "#         masked_labels = outputs['labels'].view(-1, outputs['labels'].size(-1))[valid_indices]\n",
    "#         masked_boxes = outputs['boxes'].view(-1, outputs['boxes'].size(-1))[valid_indices]\n",
    "        \n",
    "#         targets_labels = labels.view(-1)[valid_indices]\n",
    "#         targets_boxes = bboxes.view(-1, bboxes.size(-1))[valid_indices]\n",
    "\n",
    "#         # Compute loss based on masked outputs and targets\n",
    "#         loss = self.compute_loss(masked_labels, masked_boxes, targets_labels, targets_boxes)\n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         images, bboxes, labels, captions = batch\n",
    "#         text_features = self.extract_text_features(captions)\n",
    "        \n",
    "#         outputs = self.model(images, text_features)\n",
    "\n",
    "#         valid_indices = labels.view(-1) != 0  # Flatten labels for masking\n",
    "\n",
    "#         masked_labels = outputs['labels'].view(-1, outputs['labels'].size(-1))[valid_indices]\n",
    "#         masked_boxes = outputs['boxes'].view(-1, outputs['boxes'].size(-1))[valid_indices]\n",
    "        \n",
    "#         targets_labels = labels.view(-1)[valid_indices]\n",
    "#         targets_boxes = bboxes.view(-1, bboxes.size(-1))[valid_indices]\n",
    "\n",
    "#         # Compute loss based on masked outputs and targets\n",
    "#         loss = self.compute_loss(masked_labels, masked_boxes, targets_labels, targets_boxes)\n",
    "#         self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def compute_loss(self, masked_labels, masked_boxes, targets_labels, targets_boxes):\n",
    "#         # Custom loss computation using masked outputs and targets\n",
    "#         classification_loss = torch.nn.CrossEntropyLoss()(masked_labels, targets_labels)\n",
    "#         bbox_loss = torch.nn.MSELoss()(masked_boxes, targets_boxes)\n",
    "#         total_loss = classification_loss + bbox_loss\n",
    "#         return total_loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             list(self.model.parameters()) + list(self.clip_model.parameters()),\n",
    "#             lr=1e-4\n",
    "#         )\n",
    "#         return optimizer\n",
    "    \n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.test_dataset, num_workers=self.num_workers)\n",
    "    \n",
    "#     def extract_text_features(self, captions):\n",
    "#         inputs = self.clip_processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "#         for key, value in inputs.items():\n",
    "#             print(f\"{key} is on device: {value.device}\")\n",
    "#         text_features = self.clip_model.get_text_features(**inputs)\n",
    "#         print(f\"Text features are on device: {text_features.device}\")\n",
    "#         return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9e80-35c6-459d-9190-2f7b4b96603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# # Get the dimension of the text features\n",
    "# text_feature_dim = clip_model.config.text_config.hidden_size\n",
    "# print(f\"Text feature dimension: {text_feature_dim}\")\n",
    "\n",
    "# print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c648b-be7c-4043-863e-6bde6714c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def modulate_features_with_embeddings(self, feature_maps, image_embeddings, text_embeddings):\n",
    "#         # Assuming feature_maps is a batch of feature maps with shape [batch_size, channels, height, width]\n",
    "#         # Both image_embeddings and text_embeddings are [batch_size, 512]\n",
    "        \n",
    "#         image_embeddings_transformed = self.embedding_transform(image_embeddings)  # [batch_size, 256]\n",
    "#         text_embeddings_transformed = self.embedding_transform(text_embeddings)    # [batch_size, 256]\n",
    "\n",
    "#         # Assume image_embeddings and text_embeddings are already transformed to match the channel dimension, i.e., [batch_size, 256]\n",
    "#         image_embeddings_expanded = image_embeddings.unsqueeze(-1).unsqueeze(-1)  # [batch_size, 256, 1, 1]\n",
    "#         text_embeddings_expanded = text_embeddings.unsqueeze(-1).unsqueeze(-1)    # [batch_size, 256, 1, 1]\n",
    "\n",
    "#         # Calculate the repeat factors for height and width\n",
    "#         height_repeat = feature_maps.size(2)\n",
    "#         width_repeat = feature_maps.size(3)\n",
    "\n",
    "#         # Use repeat to manually expand to the required dimensions\n",
    "#         image_embeddings_expanded = image_embeddings_expanded.repeat(1, 1, height_repeat, width_repeat)\n",
    "#         text_embeddings_expanded = text_embeddings_expanded.repeat(1, 1, height_repeat, width_repeat)\n",
    "\n",
    "#         # Now you can concatenate them with feature maps\n",
    "#         modulated_feature_maps = torch.cat([feature_maps, image_embeddings_expanded, text_embeddings_expanded], dim=1)\n",
    "        \n",
    "#         return modulated_feature_maps"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
