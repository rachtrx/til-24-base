{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288a7ad3-d9eb-4473-8af9-deb747af039b",
   "metadata": {},
   "source": [
    "# Task: `Vision-Language Model`\n",
    "\n",
    "Given an image and a caption describing a target in that image, return a bounding box corresponding to the target’s location within the image.\n",
    "\n",
    "Note that targets within a given image are not uniquely identified by their object class (e.g. ”airplane”, “helicopter”); multiple targets within an image may be members of the same object class. Instead, targets provided will correspond to a particular target description (e.g. “black and white drone”).\n",
    "\n",
    "Not all possible target descriptions will be represented in the training dataset provided to participants. There will also be unseen targets and novel descriptions in the test data used in the hidden test cases of the Virtual Qualifiers, Semi-Finals / Finals. As such, Guardians will have to develop vision models capable of understanding **natural language** to identify the correct target from the scene.\n",
    "\n",
    "For the **image datasets** provided to both Novice and Advanced Guardians, there will be no noise present. However, it is worth noting that your models will have to be adequately robust as the hidden test cases for the Virtual Qualifiers and the Semi-Finals/Finals will have increasing amounts of noise introduced. This is especially crucial for **Advanced Guardians**, due to the degradation of their robot sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a4669e-5535-4412-96c7-d76c0be37a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##import all the libraries\n",
    "\n",
    "import albumentations\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90047db4-d2ee-4c36-9cbc-fd6999d9d32c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/images'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random directories that will be needed\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "##training data to be added to tune the models\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "\n",
    "img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "074e51e7-98bc-452c-b977-f90306b79016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your image preprocessing transformations\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "# Define function to preprocess captions (you may need to customize this based on your data)\n",
    "def preprocess_caption(caption):\n",
    "    # Tokenize the caption, handle punctuation, lowercase, etc.\n",
    "    # You may use tokenization libraries like nltk or spaCy for this task\n",
    "    # For simplicity, let's assume the caption is already preprocessed\n",
    "    return caption\n",
    "\n",
    "# Load captions from JSON file with preprocessing\n",
    "def load_captions_from_json(json_file):\n",
    "    captions_data = []\n",
    "    with open(json_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            annotations = data.get('annotations', [])\n",
    "            for annotation in annotations:\n",
    "                caption = preprocess_caption(annotation['caption'])\n",
    "                captions_data.append((data['image'], caption))\n",
    "    return captions_data\n",
    "\n",
    "# Load images from a folder with preprocessing\n",
    "def load_image(image_path):\n",
    "    return image_transform(Image.open(image_path))\n",
    "\n",
    "\n",
    "# Load captions from JSON with preprocessing\n",
    "captions_data = load_captions_from_json(metadata_path)\n",
    "\n",
    "# Split the combined data into training and test sets\n",
    "train_data, test_data = train_test_split(captions_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your custom datasets and data loaders for training and test sets\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename, caption = self.data[idx]\n",
    "        image_path = os.path.join(img_dir, image_filename)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = load_image(image_path)\n",
    "        \n",
    "        return image, caption\n",
    "\n",
    "# Define your custom datasets and data loaders for training and test sets\n",
    "train_dataset = CustomDataset(data=train_data)\n",
    "test_dataset = CustomDataset(data=test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424f1f99-7386-491a-8d1a-343639d1f1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "##load the model\n",
    "## use 2 model to take into account both pictures and text\n",
    "\n",
    "import torch\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "detr_model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50').to(device)\n",
    "detr_processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "594043dc-8443-42f8-9b68-141bae1b5d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f20dd-ed74-4b31-9801-8674f37431c4",
   "metadata": {},
   "source": [
    "## Transfer Learning to make new model adapt to current army data set\n",
    "Feature extraction\n",
    "\n",
    "1. **Instantiating a Pre-Trained Model with Weights**\n",
    "   - Initialize a pre-trained model with its pre-existing weights.\n",
    "\n",
    "2. **Replacing Classifier Heads**\n",
    "   - Replace the output layer with a new one that corresponds to the number of categories in our target dataset.\n",
    "\n",
    "3. **Task-Specific Training**\n",
    "   - Freeze all the layers from the pre-trained model, leaving only the outer layer (classifier head) to be trained.\n",
    "\n",
    "since smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1121a899-0722-464b-a02d-3c726fa42c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clip_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m detr_loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Define optimizer\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m clip_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(clip_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[43mclip_lr\u001b[49m)\n\u001b[1;32m     53\u001b[0m detr_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(detr_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mdetr_lr)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Define your dataset and data loader\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_lr' is not defined"
     ]
    }
   ],
   "source": [
    "### placeholder\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "# Define image preprocessing transformations\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),  # Resize image to a fixed size\n",
    "    ToTensor(),          # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Define your dataset class with feature selection and preprocessing\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, image_transform):\n",
    "        self.data = data\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, caption, selected_features, bounding_box = self.data[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path)\n",
    "        image = self.image_transform(image)\n",
    "        \n",
    "        return image, caption, selected_features, bounding_box\n",
    "\n",
    "# Define your custom data loader\n",
    "def collate_fn(batch):\n",
    "    images, captions, selected_features, bounding_boxes = zip(*batch)\n",
    "    return images, captions, selected_features, bounding_boxes\n",
    "\n",
    "# Initialize CLIP and DETR models\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "detr_model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\n",
    "detr_processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "# Define loss functions\n",
    "clip_loss_fn = ...\n",
    "detr_loss_fn = ...\n",
    "\n",
    "# Define optimizer\n",
    "clip_optimizer = torch.optim.Adam(clip_model.parameters(), lr=clip_lr)\n",
    "detr_optimizer = torch.optim.Adam(detr_model.parameters(), lr=detr_lr)\n",
    "\n",
    "# Define your dataset and data loader\n",
    "train_dataset = CustomDataset(data=your_data, image_transform=image_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, captions, selected_features, bounding_boxes = batch\n",
    "        \n",
    "        # CLIP forward pass\n",
    "        clip_inputs = clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True)\n",
    "        clip_outputs = clip_model(**clip_inputs)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        image_embedding = clip_outputs.last_hidden_state[:, 0, :]\n",
    "        caption_embedding = clip_outputs.last_hidden_state[:, 1, :]\n",
    "        combined_embedding = torch.cat((image_embedding, caption_embedding, selected_features), dim=1)\n",
    "        \n",
    "        # DETR forward pass\n",
    "        detr_outputs = detr_model.forward(features=combined_embedding)\n",
    "        \n",
    "        # Compute losses\n",
    "        clip_loss = clip_loss_fn(...)\n",
    "        detr_loss = detr_loss_fn(...)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        clip_optimizer.zero_grad()\n",
    "        detr_optimizer.zero_grad()\n",
    "        clip_loss.backward()\n",
    "        detr_loss.backward()\n",
    "        clip_optimizer.step()\n",
    "        detr_optimizer.step()\n",
    "\n",
    "# Save trained models\n",
    "torch.save(clip_model.state_dict(), \"clip_model.pth\")\n",
    "torch.save(detr_model.state_dict(), \"detr_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467037fd-0295-46f9-8777-e7bfb8490e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
