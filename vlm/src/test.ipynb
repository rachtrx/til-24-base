{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288a7ad3-d9eb-4473-8af9-deb747af039b",
   "metadata": {},
   "source": [
    "pip install torch torchvision\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install transformers\n",
    "pip install datasets\n",
    "# Task: `Vision-Language Model`\n",
    "\n",
    "Given an image and a caption describing a target in that image, return a bounding box corresponding to the target’s location within the image.\n",
    "\n",
    "Note that targets within a given image are not uniquely identified by their object class (e.g. ”airplane”, “helicopter”); multiple targets within an image may be members of the same object class. Instead, targets provided will correspond to a particular target description (e.g. “black and white drone”).\n",
    "\n",
    "Not all possible target descriptions will be represented in the training dataset provided to participants. There will also be unseen targets and novel descriptions in the test data used in the hidden test cases of the Virtual Qualifiers, Semi-Finals / Finals. As such, Guardians will have to develop vision models capable of understanding **natural language** to identify the correct target from the scene.\n",
    "\n",
    "For the **image datasets** provided to both Novice and Advanced Guardians, there will be no noise present. However, it is worth noting that your models will have to be adequately robust as the hidden test cases for the Virtual Qualifiers and the Semi-Finals/Finals will have increasing amounts of noise introduced. This is especially crucial for **Advanced Guardians**, due to the degradation of their robot sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc93ae19-cd80-4137-bea7-cb48265b7628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchvision\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a4669e-5535-4412-96c7-d76c0be37a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##import all the libraries\n",
    "\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import torch\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import urllib\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90047db4-d2ee-4c36-9cbc-fd6999d9d32c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## random directories that will be needed\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "vlm_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(vlm_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "img_dir = os.path.join(test_dir, 'images')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "\n",
    "##training data to be added to tune the models\n",
    "metadata_path = os.path.join(test_dir, 'vlm.jsonl')\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7e6bd-257a-4c29-ba77-f375e3800c83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue;\">Prepare the data set merge the image and captions together</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e38e5e-1a4d-46a3-b3a5-5d819b524060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    total_examples = len(data['image'])\n",
    "    indices = list(range(total_examples))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(train_ratio * total_examples)\n",
    "    val_end = train_end + int(val_ratio * total_examples)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "    \n",
    "    train_data = {'image': [data['image'][i] for i in train_indices], 'annotations': [data['annotations'][i] for i in train_indices]}\n",
    "    val_data = {'image': [data['image'][i] for i in val_indices], 'annotations': [data['annotations'][i] for i in val_indices]}\n",
    "    test_data = {'image': [data['image'][i] for i in test_indices], 'annotations': [data['annotations'][i] for i in test_indices]}\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'image': [], 'annotations': []}\n",
    "data_path = os.path.join(test_dir, \"vlm.jsonl\")\n",
    "with jsonlines.open(metadata_path) as reader:\n",
    "    for obj in reader:\n",
    "        if MAX_FILE_COUNT and len(data['image']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        data['image'].append(os.path.join(img_dir, obj['image']))\n",
    "        data['annotations'].append(obj['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e1850f-03f5-4c10-ae13-d3558aba3d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "print(len(captions_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a553007-affb-4e56-8d9b-0982708d497f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: blue and red light aircraft\n",
      "Colors: ['blue', 'red']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: black drone\n",
      "Colors: ['black']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: white, black, and grey missile\n",
      "Colors: ['white', 'grey', 'black']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: yellow helicopter\n",
      "Colors: ['yellow']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white fighter jet\n",
      "Colors: ['white']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: grey fighter jet\n",
      "Colors: ['grey']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: red helicopter\n",
      "Colors: ['red']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white and orange light aircraft\n",
      "Colors: ['orange', 'white']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: green and black camouflage helicopter\n",
      "Colors: ['green', 'camouflage', 'black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white and blue cargo aircraft\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: red fighter plane\n",
      "Colors: ['red']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: blue and yellow fighter jet\n",
      "Colors: ['yellow', 'blue']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: white and red missile\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: green helicopter\n",
      "Colors: ['green']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white and red commercial aircraft\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: green and yellow fighter plane\n",
      "Colors: ['green', 'yellow']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white fighter plane\n",
      "Colors: ['white']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: grey and white fighter plane\n",
      "Colors: ['white', 'grey']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: green and brown camouflage fighter jet\n",
      "Colors: ['green', 'brown', 'camouflage']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: green and black missile\n",
      "Colors: ['green', 'black']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: grey and red missile\n",
      "Colors: ['grey', 'red']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: yellow fighter jet\n",
      "Colors: ['yellow']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: black and yellow drone\n",
      "Colors: ['yellow', 'black']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: blue, yellow, and green fighter plane\n",
      "Colors: ['yellow', 'blue', 'green']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: green camouflage helicopter\n",
      "Colors: ['green', 'camouflage']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: grey and red fighter jet\n",
      "Colors: ['grey', 'red']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: yellow commercial aircraft\n",
      "Colors: ['yellow']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: yellow and black fighter plane\n",
      "Colors: ['yellow', 'black']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: red and white fighter jet\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: yellow, black, and red helicopter\n",
      "Colors: ['yellow', 'red', 'black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: grey and black fighter plane\n",
      "Colors: ['grey', 'black']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white missile\n",
      "Colors: ['white']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: black and brown camouflage helicopter\n",
      "Colors: ['brown', 'camouflage', 'black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white and black drone\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: silver fighter plane\n",
      "Colors: ['silver']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white commercial aircraft\n",
      "Colors: ['white']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: yellow missile\n",
      "Colors: ['yellow']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: blue and yellow helicopter\n",
      "Colors: ['yellow', 'blue']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: red light aircraft\n",
      "Colors: ['red']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: white and blue light aircraft\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: white and black light aircraft\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: blue and white commercial aircraft\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: green and white fighter plane\n",
      "Colors: ['green', 'white']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: grey and yellow fighter plane\n",
      "Colors: ['yellow', 'grey']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: blue helicopter\n",
      "Colors: ['blue']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white, red, and green fighter plane\n",
      "Colors: ['green', 'white', 'red']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white and blue helicopter\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: grey fighter plane\n",
      "Colors: ['grey']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: blue and green fighter plane\n",
      "Colors: ['green', 'blue']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white and red fighter jet\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: red, white, and blue fighter jet\n",
      "Colors: ['blue', 'white', 'red']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: white and red light aircraft\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: black and orange drone\n",
      "Colors: ['orange', 'black']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: blue, yellow, and white cargo aircraft\n",
      "Colors: ['yellow', 'blue', 'white']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: yellow fighter plane\n",
      "Colors: ['yellow']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: black fighter plane\n",
      "Colors: ['black']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: black helicopter\n",
      "Colors: ['black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: black camouflage fighter jet\n",
      "Colors: ['camouflage', 'black']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: white, blue, and red commercial aircraft\n",
      "Colors: ['blue', 'white', 'red']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: yellow, red, and grey helicopter\n",
      "Colors: ['yellow', 'grey', 'red']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: blue and red commercial aircraft\n",
      "Colors: ['blue', 'red']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: green and brown camouflage fighter plane\n",
      "Colors: ['green', 'brown', 'camouflage']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: black fighter jet\n",
      "Colors: ['black']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: white and blue fighter plane\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: orange and black fighter jet\n",
      "Colors: ['orange', 'black']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: blue and grey fighter jet\n",
      "Colors: ['blue', 'grey']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: green light aircraft\n",
      "Colors: ['green']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: white helicopter\n",
      "Colors: ['white']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: grey, red, and blue commercial aircraft\n",
      "Colors: ['blue', 'grey', 'red']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: red and white helicopter\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: blue commercial aircraft\n",
      "Colors: ['blue']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: blue missile\n",
      "Colors: ['blue']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: white and grey helicopter\n",
      "Colors: ['white', 'grey']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: grey and green cargo aircraft\n",
      "Colors: ['green', 'grey']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: red and black drone\n",
      "Colors: ['black', 'red']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: orange light aircraft\n",
      "Colors: ['orange']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: black and yellow missile\n",
      "Colors: ['yellow', 'black']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: grey light aircraft\n",
      "Colors: ['grey']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: grey cargo aircraft\n",
      "Colors: ['grey']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: yellow and green helicopter\n",
      "Colors: ['green', 'yellow']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white and black cargo aircraft\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: grey drone\n",
      "Colors: ['grey']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: red and grey missile\n",
      "Colors: ['grey', 'red']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: black and white cargo aircraft\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: white cargo aircraft\n",
      "Colors: ['white']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: green missile\n",
      "Colors: ['green']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: red, white, and blue light aircraft\n",
      "Colors: ['blue', 'white', 'red']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: white drone\n",
      "Colors: ['white']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: white light aircraft\n",
      "Colors: ['white']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: white and blue commercial aircraft\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: grey commercial aircraft\n",
      "Colors: ['grey']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: grey missile\n",
      "Colors: ['grey']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: white and black helicopter\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: white, black, and red drone\n",
      "Colors: ['white', 'red', 'black']\n",
      "Objects: ['drone']\n",
      "------------------------------\n",
      "Original: white and red helicopter\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: blue and white light aircraft\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: silver and blue fighter plane\n",
      "Colors: ['blue', 'silver']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white and red fighter plane\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: blue camouflage fighter jet\n",
      "Colors: ['blue', 'camouflage']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: white and black fighter jet\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: yellow light aircraft\n",
      "Colors: ['yellow']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: grey helicopter\n",
      "Colors: ['grey']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: green and grey helicopter\n",
      "Colors: ['green', 'grey']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: blue and white helicopter\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: red and white missile\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: white and black fighter plane\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: green fighter plane\n",
      "Colors: ['green']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: red fighter jet\n",
      "Colors: ['red']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: grey and white light aircraft\n",
      "Colors: ['white', 'grey']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: grey and black helicopter\n",
      "Colors: ['grey', 'black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: green and brown camouflage helicopter\n",
      "Colors: ['green', 'brown', 'camouflage']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: red and white fighter plane\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: white and blue fighter jet\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: black cargo aircraft\n",
      "Colors: ['black']\n",
      "Objects: ['cargo aircraft']\n",
      "------------------------------\n",
      "Original: grey and red commercial aircraft\n",
      "Colors: ['grey', 'red']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: red and white light aircraft\n",
      "Colors: ['white', 'red']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: blue and white missile\n",
      "Colors: ['blue', 'white']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: white, red, and blue commercial aircraft\n",
      "Colors: ['blue', 'white', 'red']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: white and orange commercial aircraft\n",
      "Colors: ['orange', 'white']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: yellow and red light aircraft\n",
      "Colors: ['yellow', 'red']\n",
      "Objects: ['light aircraft']\n",
      "------------------------------\n",
      "Original: blue, yellow, and black helicopter\n",
      "Colors: ['yellow', 'blue', 'black']\n",
      "Objects: ['helicopter']\n",
      "------------------------------\n",
      "Original: grey camouflage fighter jet\n",
      "Colors: ['grey', 'camouflage']\n",
      "Objects: ['fighter jet']\n",
      "------------------------------\n",
      "Original: yellow, red, and blue fighter plane\n",
      "Colors: ['yellow', 'blue', 'red']\n",
      "Objects: ['fighter plane']\n",
      "------------------------------\n",
      "Original: black and white commercial aircraft\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n",
      "Original: black and white missile\n",
      "Colors: ['white', 'black']\n",
      "Objects: ['missile']\n",
      "------------------------------\n",
      "Original: white and yellow commercial aircraft\n",
      "Colors: ['yellow', 'white']\n",
      "Objects: ['commercial aircraft']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addc5d86-f56a-45d1-8e89-7e3fe35f5273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cargo aircraft',\n",
       " 'commercial aircraft',\n",
       " 'drone',\n",
       " 'fighter jet',\n",
       " 'fighter plane',\n",
       " 'helicopter',\n",
       " 'light aircraft',\n",
       " 'missile'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8931aa28-b6f5-4fb2-91df-cbacc7f60efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'color'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c657acbe-9e1b-4c74-9e8f-935287d0b74c",
   "metadata": {},
   "source": [
    "## We will train on 126 classes\n",
    "\n",
    "We can create a mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c3ce99-a5ed-4228-b9b2-2bbe0e7dec16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption to Label Mapping:\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from captions to integers\n",
    "caption_to_label = {caption: idx for idx, caption in enumerate(captions_set)}\n",
    "\n",
    "print(\"Caption to Label Mapping:\")\n",
    "# print(json.dumps(caption_to_label, indent=2))\n",
    "\n",
    "# Apply the mapping to the annotations\n",
    "for img_annotations in data['annotations']:\n",
    "    for annotation in img_annotations:\n",
    "        annotation['label'] = caption_to_label[annotation['caption']]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ebd72-7cf1-43cd-9399-7368bfafd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e780b17-6b42-49ae-948d-ea410c1c4312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first_image_path = train_data['image'][0]\n",
    "# first_image_annotations = train_data['annotations'][0]\n",
    "\n",
    "# print(f\"First image path: {first_image_path}\")\n",
    "# print(f\"Annotations: {first_image_annotations}\")\n",
    "\n",
    "# # Load and display the image\n",
    "# image = cv2.imread(first_image_path)\n",
    "# if image is None:\n",
    "#     print(f\"Failed to load image at {first_image_path}\")\n",
    "# else:\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(f\"Annotations: {first_image_annotations}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbf84ca-65f1-49b4-9132-2d82fc4cd1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPPreprocessor:\n",
    "    def __init__(self, dataset, output_dir, batch_size=32, device=\"cuda\"):\n",
    "        self.dataset = dataset\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.clip_model, self.preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.augmentations = A.Compose([\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.2)\n",
    "        ])\n",
    "        self.unique_labels = set()\n",
    "\n",
    "    def preprocess_and_save_batches(self):\n",
    "        images = self.dataset['image']\n",
    "        annotations = self.dataset['annotations']\n",
    "        num_batches = (len(images) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "            batch_images = images[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_annotations = annotations[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_data = list(zip(batch_images, batch_annotations))\n",
    "            image_features, text_features, cropped_image_features, bboxes, labels = self.process_batch(batch_data)\n",
    "\n",
    "            self.pad_batch(image_features, bboxes, labels, botext_featuresxes, cropped_image_features)\n",
    "            self.save_batch(batch_idx, image_tensors, image_features, text_features, boxes, labels)\n",
    "\n",
    "        return num_batches\n",
    "\n",
    "    def process_batch(self, batch_data):\n",
    "        image_tensors = []\n",
    "        all_image_features = []\n",
    "        all_text_features = []\n",
    "        all_boxes = []\n",
    "        all_labels = []\n",
    "\n",
    "        for image_path, image_annotations in batch_data:\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                image_np = np.array(image)  # Convert PIL image to numpy array\n",
    "                augmented = self.augmentations(image=image_np)  # Apply Gaussian blur\n",
    "                image_np = augmented['image']\n",
    "                raw_image_tensor = T.ToTensor()(image_np)  # Convert numpy array to tensor\n",
    "                image_tensor = self.preprocess_clip(Image.fromarray(image_np)).unsqueeze(0).to(self.device)  # Preprocess image for CLIP\n",
    "                image_features = clip_model.encode_image(image_tensor).squeeze(0).cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid image: {image_path}. Error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            image_failed = False\n",
    "            valid_annotations = 0\n",
    "            image_features = []\n",
    "            text_features = []\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            \n",
    "            for idx, annotation in enumerate(image_annotations):\n",
    "                caption = annotation['caption']\n",
    "                bbox = annotation['bbox']\n",
    "                x, y, w, h = bbox\n",
    "\n",
    "                # Crop the image according to the bounding box\n",
    "                cropped_image_np = image_np[y:y + h, x:x + w]\n",
    "                if cropped_image_np.size == 0:\n",
    "                    if idx == len(image_annotations) - 1 and valid_annotations == 0:\n",
    "                        image_failed = True\n",
    "                        print(f\"Skipping image with no valid annotations: {image_path}\")\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "                cropped_image_tensor = self.preprocess_clip(cropped_image_pil).unsqueeze(0).to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_feature = self.clip_model.encode_image(cropped_image_tensor).cpu().numpy()\n",
    "                    text_feature = self.clip_model.encode_text(clip.tokenize([caption]).to(self.device)).cpu().numpy()\n",
    "\n",
    "                image_features.append(image_feature)\n",
    "                text_features.append(text_feature)\n",
    "                boxes.append(bbox)\n",
    "                label = annotation.get('label', 0)\n",
    "                labels.append(label)\n",
    "                self.unique_labels.add(label)\n",
    "\n",
    "            if not image_failed:\n",
    "                image_tensors.append(raw_image_tensor)\n",
    "                all_image_features.append(image_features)\n",
    "                all_text_features.append(text_features)\n",
    "                all_boxes.append(boxes)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "        return image_tensors, all_image_features, all_text_features, all_boxes, all_labels\n",
    "\n",
    "    def pad_batch(self, image_tensors, image_features, text_features, boxes, labels):\n",
    "        while len(image_tensors) < self.batch_size:\n",
    "            dummy_image = torch.zeros_like(image_tensors[0])\n",
    "            image_tensors.append(dummy_image)\n",
    "            dummy_feature = np.zeros_like(image_features[0][0])\n",
    "            image_features.append([dummy_feature])\n",
    "            text_features.append([dummy_feature])\n",
    "            boxes.append([[0, 0, 0, 0]])\n",
    "            labels.append([0])\n",
    "\n",
    "    def save_batch(self, batch_idx, image_tensors, image_features, text_features, boxes, labels):\n",
    "        batch_output_dir = os.path.join(self.output_dir, f\"batch_{batch_idx}\")\n",
    "        os.makedirs(batch_output_dir, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(batch_output_dir, \"image_tensors.npy\"), np.array([img.cpu().numpy() for img in image_tensors]))\n",
    "        np.save(os.path.join(batch_output_dir, \"image_features.npy\"), image_features, allow_pickle=True)\n",
    "        np.save(os.path.join(batch_output_dir, \"text_features.npy\"), text_features, allow_pickle=True)\n",
    "        np.save(os.path.join(batch_output_dir, \"boxes.npy\"), boxes, allow_pickle=True)\n",
    "        np.save(os.path.join(batch_output_dir, \"labels.npy\"), text_features, allow_pickle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4581a72-73cf-4dae-b121-8bb8eaedd6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_preprocessor = CLIPPreprocessor(train_data, train_dir)\n",
    "val_preprocessor = CLIPPreprocessor(val_data, val_dir)\n",
    "test_preprocessor = CLIPPreprocessor(test_data, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a91abadb-85dd-411b-b7f3-7a15ebb5f41b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Batches:   0%|          | 0/2 [00:13<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (64,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(data_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m train_num_batches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_preprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_and_save_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m val_num_batches \u001b[38;5;241m=\u001b[39m val_preprocessor\u001b[38;5;241m.\u001b[39mpreprocess_and_save_batches()\n\u001b[1;32m      4\u001b[0m test_num_batches \u001b[38;5;241m=\u001b[39m test_preprocessor\u001b[38;5;241m.\u001b[39mpreprocess_and_save_batches()\n",
      "Cell \u001b[0;32mIn[51], line 25\u001b[0m, in \u001b[0;36mCLIPPreprocessor.preprocess_and_save_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m     image_tensors, image_features, text_features, boxes, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_batch(batch_data)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_batch(image_tensors, image_features, text_features, boxes, labels)\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m num_batches\n",
      "Cell \u001b[0;32mIn[51], line 107\u001b[0m, in \u001b[0;36mCLIPPreprocessor.save_batch\u001b[0;34m(self, batch_idx, image_tensors, image_features, text_features, boxes, labels)\u001b[0m\n\u001b[1;32m    104\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(batch_output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_tensors.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), np\u001b[38;5;241m.\u001b[39marray([img\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_tensors]))\n\u001b[0;32m--> 107\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_features.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_features.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), text_features)\n\u001b[1;32m    109\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), np\u001b[38;5;241m.\u001b[39marray(boxes, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:545\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m--> 545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_array(fid, arr, allow_pickle\u001b[38;5;241m=\u001b[39mallow_pickle,\n\u001b[1;32m    547\u001b[0m                        pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(fix_imports\u001b[38;5;241m=\u001b[39mfix_imports))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (64,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "train_num_batches = train_preprocessor.preprocess_and_save_batches()\n",
    "val_num_batches = val_preprocessor.preprocess_and_save_batches()\n",
    "test_num_batches = test_preprocessor.preprocess_and_save_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13cbef78-9e32-488b-8b1d-043a28acc732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def preprocess_and_save_batches(dataset, augmentations, output_dir, batch_size=64, device=\"cuda\"):\n",
    "#     clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "#     images = dataset['image']\n",
    "#     annotations = dataset['annotations']\n",
    "#     num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "#     for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "#         batch_images = images[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "#         batch_annotations = annotations[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "#         batch_data = list(zip(batch_images, batch_annotations))\n",
    "#         image_tensors = []\n",
    "#         all_bboxes = []\n",
    "#         all_labels = []\n",
    "#         image_features = []\n",
    "#         text_features = []\n",
    "\n",
    "#         for image_path, image_annotations in batch_data:\n",
    "#             # Load image\n",
    "#             image = cv2.imread(image_path)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             if image is None or image.shape[0] == 0 or image.shape[1] == 0:\n",
    "#                 print(f\"Skipping invalid image: {image_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Apply augmentations\n",
    "#             augmented = augmentations(image=image)\n",
    "#             augmented_image = augmented['image'].permute(1, 2, 0).numpy()\n",
    "#             augmented_image = (augmented_image * 255).astype(np.uint8)\n",
    "            \n",
    "#             bboxes = []\n",
    "#             labels = []\n",
    "\n",
    "#             num_annotations = len(image_annotations)\n",
    "#             annotated = 0\n",
    "#             image_failed = False\n",
    "#             for idx, annotation in enumerate(image_annotations):\n",
    "#                 caption = annotation['caption']\n",
    "#                 bbox = annotation['bbox']\n",
    "\n",
    "#                 x, y, w, h = bbox\n",
    "#                 if w == 0 or h == 0:\n",
    "#                     print(f\"Skipping zero-sized bbox: {bbox} in image: {image_path}\")\n",
    "#                     if idx == num_annotations - 1 and annotated == 0:\n",
    "#                         image_failed = True\n",
    "#                         break\n",
    "#                     else:\n",
    "#                         continue\n",
    "\n",
    "#                 # Attempt to crop augmented image\n",
    "#                 cropped_image = augmented_image[y:y + h, x:x + w]\n",
    "#                 if cropped_image.size == 0:\n",
    "#                     # Fall back to original image if augmented crop is invalid\n",
    "#                     print(f\"Fallback to original image for bbox: {bbox} in image: {image_path}\")\n",
    "#                     cropped_image = image[y:y + h, x:x + w]\n",
    "\n",
    "#                 if cropped_image.size == 0:\n",
    "#                     print(f\"Skipping empty cropped image for bbox: {bbox} in image: {image_path}\")\n",
    "#                     if idx == num_annotations - 1 and annotated == 0:\n",
    "#                         image_failed = True\n",
    "#                         break\n",
    "#                     else:\n",
    "#                         continue\n",
    "                    \n",
    "#                 bboxes.append(bbox)\n",
    "#                 labels.append(caption)\n",
    "                \n",
    "#                 annotated += 1\n",
    "\n",
    "#                 cropped_pil_image = Image.fromarray(cropped_image.astype('uint8'))\n",
    "\n",
    "#                 # Preprocess the image for CLIP\n",
    "#                 cropped_preprocessed = preprocess_clip(cropped_pil_image).unsqueeze(0).to(device)\n",
    "                \n",
    "#                 # Encode features using CLIP\n",
    "#                 with torch.no_grad():\n",
    "#                     if cropped_preprocessed.shape[1] == 3:\n",
    "#                         image_feature = clip_model.encode_image(cropped_preprocessed).cpu().numpy()\n",
    "#                         text_feature = clip_model.encode_text(clip.tokenize([caption]).to(device)).cpu().numpy()\n",
    "#                         image_features.append(image_feature)\n",
    "#                         text_features.append(text_feature)\n",
    "#                     else:\n",
    "#                         print(f\"Skipping encoding due to incorrect shape: {cropped_preprocessed.shape}\")\n",
    "\n",
    "#             if image_failed:\n",
    "#                 continue\n",
    "\n",
    "#             image_tensors.append(augmented['image'])\n",
    "#             all_bboxes.append(bboxes)\n",
    "#             all_labels.append(labels)\n",
    "        \n",
    "#         if len(image_tensors) < batch_size:\n",
    "#             print(f\"Padding batch {batch_idx} with dummy data to reach target batch size of {batch_size}\")\n",
    "#         while len(image_tensors) < batch_size:\n",
    "#             dummy_image = torch.zeros_like(augmented['image'])\n",
    "#             image_tensors.append(dummy_image)\n",
    "#             all_bboxes.append([])\n",
    "#             all_labels.append(\"\")\n",
    "#             image_features.append(np.zeros_like(image_features[0]))\n",
    "#             text_features.append(np.zeros_like(text_features[0]))\n",
    "        \n",
    "#         # Create batch directory\n",
    "#         batch_output_dir = os.path.join(output_dir, f\"batch_{batch_idx}\")\n",
    "#         os.makedirs(batch_output_dir, exist_ok=True)\n",
    "\n",
    "#         # Save batch data\n",
    "#         image_batch_memmap_path = os.path.join(batch_output_dir, \"image_batch.npy\")\n",
    "#         np.save(image_batch_memmap_path, np.array([img.cpu().numpy() for img in image_tensors]))\n",
    "\n",
    "#         bboxes_path = os.path.join(batch_output_dir, \"bboxes_batch.npy\")\n",
    "#         np.save(bboxes_path, np.array(all_bboxes, dtype=object), allow_pickle=True)\n",
    "\n",
    "#         labels_path = os.path.join(batch_output_dir, \"labels_batch.npy\")\n",
    "#         np.save(labels_path, np.array(all_labels, dtype=object), allow_pickle=True)\n",
    "\n",
    "#         image_features_path = os.path.join(batch_output_dir, \"image_features_batch.npy\")\n",
    "#         np.save(image_features_path, np.array(image_features))\n",
    "\n",
    "#         text_features_path = os.path.join(batch_output_dir, \"text_features_batch.npy\")\n",
    "#         np.save(text_features_path, np.array(text_features))\n",
    "        \n",
    "#     return num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a5b1ae6-e96d-4d79-a5d9-ed87896f7c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "# train_num_batches = preprocess_and_save_batches(train_data, augmentations, train_dir)\n",
    "# val_num_batches = preprocess_and_save_batches(val_data, augmentations, val_dir)\n",
    "# test_num_batches = preprocess_and_save_batches(test_data, augmentations, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0fb2655-2903-4bc5-a729-a1bf0c0b052e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"Number of batches for train: {train_num_batches}\")\n",
    "# print(f\"Number of batches for val: {val_num_batches}\")\n",
    "# print(f\"Number of batches for test: {test_num_batches}\")\n",
    "\n",
    "train_num_batches = 63\n",
    "val_num_batches = 8\n",
    "test_num_batches = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28c25b62-76df-4073-927c-57ede2131ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# def visualize_image_with_bbox(image_tensor, bbox, label):\n",
    "#     image = image_tensor.permute(1, 2, 0).numpy()  # Convert from CHW to HWC format\n",
    "#     image = (image * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image[..., ::-1])\n",
    "    \n",
    "#     # Draw the bounding box\n",
    "#     x, y, w, h = bbox\n",
    "#     rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "#     ax.add_patch(rect)\n",
    "    \n",
    "#     # Add the label\n",
    "#     plt.text(x, y - 10, label, color='red', fontsize=12, backgroundcolor='black')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# # Load one batch and visualize\n",
    "# def load_and_visualize_batch(batch_dir):\n",
    "#     image_batch_path = os.path.join(batch_dir, \"image_batch.npy\")\n",
    "#     bboxes_path = os.path.join(batch_dir, \"bboxes_batch.npy\")\n",
    "#     labels_path = os.path.join(batch_dir, \"labels_batch.npy\")\n",
    "\n",
    "#     images = np.load(image_batch_path)\n",
    "#     bboxes = np.load(bboxes_path, allow_pickle=True)\n",
    "#     labels = np.load(labels_path, allow_pickle=True)\n",
    "\n",
    "#     # Assuming the first image in the batch\n",
    "#     image_tensor = torch.tensor(images[0])\n",
    "#     bbox = bboxes[0][0]\n",
    "#     label = labels[0][0]\n",
    "\n",
    "#     # Visualize the image with the bounding box\n",
    "#     visualize_image_with_bbox(image_tensor, bbox, label)\n",
    "\n",
    "# # Example usage\n",
    "# batch_output_dir = \"./data/train\"  # Adjust this to your actual batch output directory\n",
    "# batch_idx = 0  # Index of the batch to visualize\n",
    "# load_and_visualize_batch(os.path.join(batch_output_dir, f\"batch_{batch_idx}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14bd5da6-59fc-4d71-9cdc-91f2270998d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapIterableDataset(IterableDataset):\n",
    "    def __init__(self, type_dir, num_batches, shuffle=False):\n",
    "        self.type_dir = type_dir\n",
    "        self.num_batches = num_batches\n",
    "        self.shuffle=shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_idx in range(self.num_batches):\n",
    "            batch_path = os.path.join(self.type_dir, f\"batch_{batch_idx}\")\n",
    "\n",
    "            image_batch_memmap_path = os.path.join(batch_path, \"image_batch.npy\")\n",
    "            image_batch = np.load(image_batch_memmap_path, mmap_mode='r')\n",
    "            image_batch = torch.tensor(image_batch)\n",
    "\n",
    "            bboxes_path = os.path.join(batch_path, \"bboxes_batch.npy\")\n",
    "            bboxes_batch = np.load(bboxes_path, allow_pickle=True)\n",
    "            bboxes_batch = [torch.tensor(b) for b in bboxes_batch]\n",
    "\n",
    "            labels_path = os.path.join(batch_path, \"labels_batch.npy\")\n",
    "            labels_batch = np.load(labels_path, allow_pickle=True)\n",
    "            labels_batch = [l for l in labels_batch]\n",
    "            \n",
    "            print(labels_batch[0])\n",
    "\n",
    "            image_features_path = os.path.join(batch_path, \"image_features_batch.npy\")\n",
    "            image_features_batch = np.load(image_features_path, mmap_mode='r')\n",
    "            image_features_batch = torch.tensor(image_features_batch)\n",
    "\n",
    "            text_features_path = os.path.join(batch_path, \"text_features_batch.npy\")\n",
    "            text_features_batch = np.load(text_features_path, mmap_mode='r')\n",
    "            text_features_batch = torch.tensor(text_features_batch)\n",
    "\n",
    "            yield image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7af987-fc96-477e-9df4-0b52f03d5cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOCLIPModel(pl.LightningModule):\n",
    "    def __init__(self, yolo_model_path):\n",
    "        super(YOLOCLIPModel, self).__init__()\n",
    "        self.yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.yolo_model(image)\n",
    "    \n",
    "    def compute_yolo_loss(self, yolo_results, bboxes, labels):\n",
    "        # This is a simplified YOLO loss function\n",
    "        # You should replace it with the actual YOLO loss function used in your project\n",
    "        yolo_pred = yolo_results.xyxy[0]  # Example extraction of YOLO prediction\n",
    "        bboxes = torch.stack(bboxes).to(yolo_pred.device)\n",
    "        \n",
    "        # Assuming yolo_pred and bboxes are of the same shape for simplicity\n",
    "        bbox_loss = F.mse_loss(yolo_pred[:, :4], bboxes)\n",
    "        obj_loss = F.binary_cross_entropy(yolo_pred[:, 4], torch.ones_like(yolo_pred[:, 4]))\n",
    "        \n",
    "        # If labels are provided, compute classification loss\n",
    "        class_loss = 0\n",
    "        if labels:\n",
    "            labels = torch.stack(labels).to(yolo_pred.device)\n",
    "            class_loss = F.cross_entropy(yolo_pred[:, 5:], labels)\n",
    "        \n",
    "        total_loss = bbox_loss + obj_loss + class_loss\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # print(\"In training Step\")\n",
    "        image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch = batch\n",
    "        total_loss = 0\n",
    "\n",
    "        for image_tensor, bboxes, labels, image_features, text_features in zip(image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch):\n",
    "            yolo_results = self(image_tensor)  # YOLO inference\n",
    "            boxes = yolo_results.xyxy[0]  # Extract bounding boxes\n",
    "\n",
    "            yolo_loss = self.compute_yolo_loss(yolo_results, bboxes, labels)\n",
    "            total_loss += yolo_loss\n",
    "\n",
    "            for image_feature, text_feature in zip(image_features, text_features):\n",
    "                image_feature = image_feature.to(\"cuda\")\n",
    "                text_feature = text_feature.to(\"cuda\")\n",
    "\n",
    "                clip_loss = 1 - F.cosine_similarity(image_feature, text_feature)\n",
    "                total_loss += clip_loss\n",
    "\n",
    "        total_loss = total_loss / (len(image_batch) * len(image_features_batch))\n",
    "        self.log(\"train_loss\", total_loss)\n",
    "        \n",
    "        # # Free memory after processing the batch\n",
    "        # del image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch = batch\n",
    "        total_loss = 0\n",
    "\n",
    "        for image_tensor, bboxes, labels, image_features, text_features in zip(image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch):\n",
    "            yolo_results = self(image_tensor)  # YOLO inference\n",
    "            boxes = yolo_results.xyxy[0]  # Extract bounding boxes\n",
    "\n",
    "            yolo_loss = self.compute_yolo_loss(yolo_results, bboxes, labels)\n",
    "            total_loss += yolo_loss\n",
    "\n",
    "            for image_feature, text_feature in zip(image_features, text_features):\n",
    "                image_feature = image_feature.to(\"cuda\")\n",
    "                text_feature = text_feature.to(\"cuda\")\n",
    "\n",
    "                clip_loss = 1 - F.cosine_similarity(image_feature, text_feature)\n",
    "                total_loss += clip_loss\n",
    "\n",
    "        total_loss = total_loss / (len(image_batch) * len(image_features_batch))\n",
    "        self.log(\"val_loss\", total_loss)\n",
    "        \n",
    "        # # Free memory after processing the batch\n",
    "        # del image_batch, bboxes_batch, labels_batch, image_features_batch, text_features_batch\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6728928d-153f-4db8-bf9e-329261d5e925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VLMDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, test_data, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.train_dir, self.train_num_batches = train_data\n",
    "        self.val_dir, self.val_num_batches = val_data\n",
    "        self.test_dir, self.test_num_batches = test_data\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = MemmapIterableDataset(self.train_dir, self.train_num_batches, shuffle=True)\n",
    "        self.val_dataset = MemmapIterableDataset(self.val_dir, self.val_num_batches)\n",
    "        self.test_dataset = MemmapIterableDataset(self.test_dir, self.test_num_batches)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=None, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b7d167b-7a49-4fdd-ada6-3ae127d9edce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "data_module = VLMDataModule(\n",
    "    train_data=(train_dir, train_num_batches),\n",
    "    val_data=(val_dir, val_num_batches),\n",
    "    test_data=(test_dir, test_num_batches),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = YOLOCLIPModel(\"yolov8n.pt\")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # metric to monitor\n",
    "    patience=3,          # no of epochs with no improvement to wait before stopping\n",
    "    verbose=True,        # logging\n",
    "    mode='min'           # minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=100*train_num_batches,  # Maximum number of steps (batches) to train for\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    val_check_interval=train_num_batches,  # Validation check interval\n",
    "    limit_val_batches=val_num_batches,  # Limit the number of validation batches\n",
    "    logger=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbda0231-1d84-4346-bc71-6be4d0ca94d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/til-24-base/vlm/src/data/train\n"
     ]
    }
   ],
   "source": [
    "print(data_module.train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a20335-8597-440d-8b4e-4ea7ecb0ae9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb092fb-b537-4783-878f-a3bc9fbe9981",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue;\">Finetune the clip model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e51e7-98bc-452c-b977-f90306b79016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer\n",
    "clip_criterion = torch.nn.CrossEntropyLoss()\n",
    "clip_optimizer = optim.Adam(clip_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fine-tune the CLIP model\n",
    "clip_model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for clip_inputs, captions, _, _ in unified_dataloader:\n",
    "        clip_inputs = clip_inputs.to(device)\n",
    "        caption_tokens = torch.cat([clip.tokenize(caption) for caption_list in captions for caption in caption_list]).to(device)\n",
    "        \n",
    "        image_features = clip_model.encode_image(clip_inputs)\n",
    "        text_features = clip_model.encode_text(caption_tokens)\n",
    "        \n",
    "        logits_per_image, logits_per_text = clip_model(clip_inputs, caption_tokens)\n",
    "        loss = clip_criterion(logits_per_image, torch.arange(len(image_features)).to(device))\n",
    "        \n",
    "        clip_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, CLIP Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415aa512-6bb8-4f1c-a30d-ed3a20bea630",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue;\">Finetune the DETR model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f1f99-7386-491a-8d1a-343639d1f1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "detr_optimizer = optim.AdamW(detr_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Fine-tune the DETR model\n",
    "detr_model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for _, _, pixel_values, target in unified_dataloader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        target = {k: v.to(device) for k, v in target.items()}\n",
    "        \n",
    "        outputs = detr_model(pixel_values=pixel_values, labels=target)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        detr_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        detr_optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, DETR Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f20dd-ed74-4b31-9801-8674f37431c4",
   "metadata": {},
   "source": [
    "## Transfer Learning to make new model adapt to current army data set\n",
    "Feature extraction\n",
    "\n",
    "1. **Instantiating a Pre-Trained Model with Weights**\n",
    "   - Initialize a pre-trained model with its pre-existing weights.\n",
    "\n",
    "2. **Replacing Classifier Heads**\n",
    "   - Replace the output layer with a new one that corresponds to the number of categories in our target dataset.\n",
    "\n",
    "3. **Task-Specific Training**\n",
    "   - Freeze all the layers from the pre-trained model, leaving only the outer layer (classifier head) to be trained.\n",
    "\n",
    "since smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121a899-0722-464b-a02d-3c726fa42c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### placeholder\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "# Define image preprocessing transformations\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),  # Resize image to a fixed size\n",
    "    ToTensor(),          # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Define your dataset class with feature selection and preprocessing\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, image_transform):\n",
    "        self.data = data\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, caption, selected_features, bounding_box = self.data[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path)\n",
    "        image = self.image_transform(image)\n",
    "        \n",
    "        return image, caption, selected_features, bounding_box\n",
    "\n",
    "# Define your custom data loader\n",
    "def collate_fn(batch):\n",
    "    images, captions, selected_features, bounding_boxes = zip(*batch)\n",
    "    return images, captions, selected_features, bounding_boxes\n",
    "\n",
    "# Initialize CLIP and DETR models\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "detr_model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\n",
    "detr_processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "# Define loss functions\n",
    "clip_loss_fn = ...\n",
    "detr_loss_fn = ...\n",
    "\n",
    "# Define optimizer\n",
    "clip_optimizer = torch.optim.Adam(clip_model.parameters(), lr=clip_lr)\n",
    "detr_optimizer = torch.optim.Adam(detr_model.parameters(), lr=detr_lr)\n",
    "\n",
    "# Define your dataset and data loader\n",
    "train_dataset = CustomDataset(data=your_data, image_transform=image_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, captions, selected_features, bounding_boxes = batch\n",
    "        \n",
    "        # CLIP forward pass\n",
    "        clip_inputs = clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True)\n",
    "        clip_outputs = clip_model(**clip_inputs)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        image_embedding = clip_outputs.last_hidden_state[:, 0, :]\n",
    "        caption_embedding = clip_outputs.last_hidden_state[:, 1, :]\n",
    "        combined_embedding = torch.cat((image_embedding, caption_embedding, selected_features), dim=1)\n",
    "        \n",
    "        # DETR forward pass\n",
    "        detr_outputs = detr_model.forward(features=combined_embedding)\n",
    "        \n",
    "        # Compute losses\n",
    "        clip_loss = clip_loss_fn(...)\n",
    "        detr_loss = detr_loss_fn(...)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        clip_optimizer.zero_grad()\n",
    "        detr_optimizer.zero_grad()\n",
    "        clip_loss.backward()\n",
    "        detr_loss.backward()\n",
    "        clip_optimizer.step()\n",
    "        detr_optimizer.step()\n",
    "\n",
    "# Save trained models\n",
    "torch.save(clip_model.state_dict(), \"clip_model.pth\")\n",
    "torch.save(detr_model.state_dict(), \"detr_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae952cf9-42b2-4c58-b859-e3dc9dcd27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "import albumentations as A\n",
    "\n",
    "def preprocess_and_save_batches(dataset, augmentations, data_dir, batch_size=64):\n",
    "    clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=\"cpu\")  # Use CPU for preprocessing\n",
    "\n",
    "    images = dataset['image']\n",
    "    annotations = dataset['annotations']\n",
    "    num_batches = (len(annotations) + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "        batch_images = images[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "        batch_annotations = annotations[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "        batch_data = list(zip(batch_images, batch_annotations))\n",
    "        image_tensors = []\n",
    "        cropped_images = []\n",
    "        tokenized_captions = []\n",
    "        image_features = []\n",
    "        text_features = []\n",
    "\n",
    "        for image_path, annotations in batch_data:\n",
    "            # Load image\n",
    "            image = load_image(image_path)\n",
    "\n",
    "            for annotation in annotations:\n",
    "                caption = annotation['caption']\n",
    "                bbox = annotation['bbox']\n",
    "                cropped_image = image[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n",
    "                \n",
    "                # Apply augmentations to cropped image\n",
    "                augmented = augmentations(image=cropped_image)\n",
    "                cropped_image_tensor = torch.tensor(augmented[\"image\"]).permute(2, 0, 1)  # Change to (C, H, W) format for PyTorch\n",
    "                \n",
    "                # Preprocess using CLIP's preprocessor\n",
    "                cropped_image_preprocessed = preprocess_clip(cropped_image_tensor).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Encode features using CLIP\n",
    "                with torch.no_grad():\n",
    "                    image_feature = clip_model.encode_image(cropped_image_preprocessed).numpy()\n",
    "                    text_feature = clip_model.encode_text(clip.tokenize([caption])).numpy()\n",
    "\n",
    "                cropped_images.append(cropped_image_preprocessed.numpy())\n",
    "                tokenized_captions.append(clip.tokenize([caption]).numpy())\n",
    "                image_features.append(image_feature)\n",
    "                text_features.append(text_feature)\n",
    "        \n",
    "        # Save batch to memmap files\n",
    "        image_batch_memmap_path = os.path.join(data_dir, f\"image_batch_{batch_idx}.npy\")\n",
    "        np.save(image_batch_memmap_path, np.array(image_tensors))\n",
    "        \n",
    "        cropped_images_memmap_path = os.path.join(data_dir, f\"cropped_images_batch_{batch_idx}.npy\")\n",
    "        np.save(cropped_images_memmap_path, np.array(cropped_images))\n",
    "        \n",
    "        tokenized_captions_path = os.path.join(data_dir, f\"tokenized_captions_batch_{batch_idx}.npy\")\n",
    "        np.save(tokenized_captions_path, np.array(tokenized_captions))\n",
    "        \n",
    "        image_features_path = os.path.join(data_dir, f\"image_features_batch_{batch_idx}.npy\")\n",
    "        np.save(image_features_path, np.array(image_features))\n",
    "        \n",
    "        text_features_path = os.path.join(data_dir, f\"text_features_batch_{batch_idx}.npy\")\n",
    "        np.save(text_features_path, np.array(text_features))\n",
    "\n",
    "def load_image(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert('RGB')  # Ensure image is in RGB format\n",
    "        image_array = np.array(img)\n",
    "    return image_array\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    \"image\": [\"image_0.jpg\", \"image_1.jpg\"],\n",
    "    \"annotations\": [\n",
    "        [{\"caption\": \"blue and white missile\", \"bbox\": [1224, 284, 44, 36]}, {\"caption\": \"green light aircraft\", \"bbox\": [688, 400, 56, 36]}, {\"caption\": \"blue and white commercial aircraft\", \"bbox\": [800, 320, 128, 36]}],\n",
    "        [{\"caption\": \"red car\", \"bbox\": [100, 150, 200, 100]}, {\"caption\": \"black bike\", \"bbox\": [300, 350, 50, 50]}]\n",
    "    ]\n",
    "}\n",
    "\n",
    "augmentations = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "data_dir = 'data/'\n",
    "preprocess_and_save_batches(img_dir, augmentations, metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fadeeb-22c6-49ec-b4f5-a4ae15b780ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom unified dataset class\n",
    "class UnifiedDataset(Dataset):\n",
    "    def __init__(self, annotations, image_folder, clip_preprocess, detr_processor):\n",
    "        self.annotations = annotations\n",
    "        self.image_folder = image_folder\n",
    "        self.clip_preprocess = clip_preprocess\n",
    "        self.detr_processor = detr_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.annotations[idx]\n",
    "        image_path = os.path.join(self.image_folder, item['image'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        captions = [ann['caption'] for ann in item['annotations']]\n",
    "        \n",
    "        clip_input = self.clip_preprocess(image)\n",
    "        \n",
    "        boxes = torch.tensor([ann['bbox'] for ann in item['annotations']], dtype=torch.float32)\n",
    "        labels = torch.tensor([0] * len(item['annotations']), dtype=torch.int64)  # Assuming all annotations belong to the same class\n",
    "        target = {\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        detr_input = self.detr_processor(images=image, annotations=target, return_tensors=\"pt\")\n",
    "        detr_input[\"labels\"] = {k: v.squeeze() for k, v in detr_input[\"labels\"].items()}\n",
    "        \n",
    "        return clip_input, captions, detr_input[\"pixel_values\"].squeeze(), detr_input[\"labels\"]\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "# Load DETR model and processor\n",
    "detr_model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50').to(device)\n",
    "detr_processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "# Create unified dataset and dataloader\n",
    "unified_dataset = UnifiedDataset(annotations, image_folder, clip_preprocess, detr_processor)\n",
    "unified_dataloader = DataLoader(unified_dataset, batch_size=2, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
