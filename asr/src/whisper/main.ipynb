{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61a4d66-8b7a-4825-ac51-75260964c562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers jiwer torchaudio jsonlines datasets accelerate audiomentations # Audio Augmentation\n",
    "# !pip install -q Cython\n",
    "# !pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704bd2f1-28f4-4b65-9875-959e88f9e744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "from typing import Optional, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "from jiwer import wer\n",
    "import whisper\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ce7c8-f1eb-4e17-b889-12527e885908",
   "metadata": {},
   "source": [
    "### Defining Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b19dc2-ad9f-4c75-a659-f56ddbf68105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rachtrx/workspace/til-ai/novice'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "src_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(os.path.dirname(src_dir))\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "novice_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(novice_dir, 'audio')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "model_path = os.path.join(src_dir, \"models\", \"whisper\")\n",
    "metadata_path = os.path.join(novice_dir, \"asr.jsonl\")\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "novice_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035013-5786-4cc9-8272-3aec40df4659",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99109fd9-efc4-4bb8-8741-37e37d2649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    total_examples = len(data['audios'])\n",
    "    indices = list(range(total_examples))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(train_ratio * total_examples)\n",
    "    val_end = train_end + int(val_ratio * total_examples)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "    \n",
    "    train_data = {'audios': [data['audios'][i] for i in train_indices], 'sentences': [data['sentences'][i] for i in train_indices]}\n",
    "    val_data = {'audios': [data['audios'][i] for i in val_indices], 'sentences': [data['sentences'][i] for i in val_indices]}\n",
    "    test_data = {'audios': [data['audios'][i] for i in test_indices], 'sentences': [data['sentences'][i] for i in test_indices]}\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'audios': [], 'sentences': []}\n",
    "data_path = os.path.join(novice_dir, \"asr.jsonl\")\n",
    "with jsonlines.open(metadata_path) as reader:\n",
    "    for obj in reader:\n",
    "        if MAX_FILE_COUNT and len(data['audio']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        data['audios'].append(os.path.join(audio_dir, obj['audio']))\n",
    "        data['sentences'].append(obj['transcript'])\n",
    "\n",
    "train_data, val_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b38e5de-fae9-4f44-9226-71b13518b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    \n",
    "    def __init__(self, dataset, output_dir, tokenizer, batch_size=4, max_length=30):\n",
    "        self.dataset = dataset\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augmentations = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.15),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.15),\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.15),\n",
    "        ])\n",
    "        self.padding_audio = np.zeros((80, 3000))  # Example shape for mel spectrogram\n",
    "        self.padding_sentence = [self.tokenizer.pad_token_id] * self.max_length\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        audios = self.dataset['audios']\n",
    "        sentences = self.dataset['sentences']\n",
    "        num_batches = (len(audios) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "            batch_audio = audios[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_sentences = sentences[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n",
    "            batch_data = list(zip(batch_audio, batch_sentences))\n",
    "            \n",
    "            input_ids_arr, decoded_input_ids_arr, labels_arr = self.process_batch(batch_data)\n",
    "            input_ids_arr, decoded_input_ids_arr, labels_arr = self.pad_batch(input_ids_arr, decoded_input_ids_arr, labels_arr)\n",
    "            self.save_batch(batch_idx, input_ids_arr, decoded_input_ids_arr, labels_arr)\n",
    "        return num_batches\n",
    "\n",
    "    def process_batch(self, batch_data):\n",
    "        \n",
    "        input_ids_arr = []\n",
    "        decoded_input_ids_arr = []\n",
    "        labels_arr = []\n",
    "        \n",
    "        for file_path, sentence in batch_data:\n",
    "        \n",
    "            sample = self.load_audio(file_path)\n",
    "            # if self.transform:\n",
    "            #     input_ids = self.transform(sample['input_ids'])\n",
    "            # else:\n",
    "            input_ids = sample['input_ids']\n",
    "\n",
    "            tokenized_output = self.tokenizer(\n",
    "                sentence,\n",
    "                padding='max_length',  # Pad to max_length\n",
    "                max_length=self.max_length,  # Specify the maximum length\n",
    "                truncation=True,  # Truncate if longer than max_length\n",
    "                return_tensors='pt'  # Return PyTorch tensors\n",
    "            )\n",
    "            labels = tokenized_output['input_ids'][0].numpy().tolist()  # Convert tensor to list\n",
    "\n",
    "            # Generate decoder_input_ids from labels by shifting them to the right\n",
    "            decoder_input_ids = [self.tokenizer.pad_token_id] + labels[:-1]\n",
    "            \n",
    "            assert len(decoder_input_ids) == len(labels)\n",
    "            \n",
    "            input_ids_arr.append(input_ids)\n",
    "            decoded_input_ids_arr.append(decoder_input_ids)\n",
    "            labels_arr.append(labels)\n",
    "            \n",
    "        return input_ids_arr, decoded_input_ids_arr, labels_arr\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        waveform = waveform.numpy().flatten() # waveform is a list\n",
    "        \n",
    "        # Apply augmentations if provided\n",
    "        waveform = self.augmentations(samples=waveform, sample_rate=sample_rate)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(torch.tensor(waveform)).numpy().flatten()\n",
    "            \n",
    "        # Compute log-mel spectrogram\n",
    "        input_ids = self.to_pad_to_mel(waveform)\n",
    "\n",
    "        # Extract audio features\n",
    "        return { 'input_ids': input_ids }\n",
    "    \n",
    "    def save_batch(self, batch_idx, input_ids_arr, decoded_input_ids_arr, labels_arr):\n",
    "        batch_output_dir = os.path.join(self.output_dir, f\"batch_{batch_idx}\")\n",
    "        os.makedirs(batch_output_dir, exist_ok=True)\n",
    "\n",
    "        # Save input_ids\n",
    "        input_ids_path = os.path.join(batch_output_dir, \"input_ids.npy\")\n",
    "        np.save(input_ids_path, input_ids_arr)\n",
    "\n",
    "        # Save decoder_input_ids\n",
    "        decoder_input_ids_path = os.path.join(batch_output_dir, \"decoder_input_ids.npy\")\n",
    "        np.save(decoder_input_ids_path, decoded_input_ids_arr)\n",
    "\n",
    "        # Save labels\n",
    "        labels_path = os.path.join(batch_output_dir, \"labels.npy\")\n",
    "        np.save(labels_path, labels_arr)\n",
    "        \n",
    "    def pad_batch(self, input_ids_arr, decoded_input_ids_arr, labels_arr):\n",
    "        while len(input_ids_arr) < self.batch_size:\n",
    "            input_ids_arr.append(self.padding_audio)\n",
    "            decoded_input_ids_arr.append(self.padding_sentence)\n",
    "            labels_arr.append(self.padding_sentence)\n",
    "        \n",
    "        return input_ids_arr, decoded_input_ids_arr, labels_arr\n",
    "    \n",
    "    ## Referred to https://huggingface.co/sanchit-gandhi/whisper-medium-switchboard-5k/blob/main/run_speech_recognition_whisper.py by sanchit-gandhi\n",
    "    @staticmethod\n",
    "    def to_pad_to_mel(array):\n",
    "        \"\"\"Static function which:\n",
    "            1. Pads/trims a list of audio arrays to a max length of 30s\n",
    "            2. Computes log-mel filter coefficients from padded/trimmed audio sequences\n",
    "            Inputs:\n",
    "                array: list of audio arrays\n",
    "            Returns:\n",
    "                input_ids: torch.tensor of log-mel filter bank coefficients\n",
    "        \"\"\"\n",
    "        padded_input = whisper.pad_or_trim(np.asarray(array, dtype=np.float32))\n",
    "        input_ids = whisper.log_mel_spectrogram(padded_input)\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f5e53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"distil-whisper/distil-medium.en\"  # You can change this to any model you want to use\n",
    "# save_directory = \"../models/whisper\"  # Path to save the model and processor\n",
    "\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "# model.save_pretrained(save_directory)\n",
    "# processor = AutoProcessor.from_pretrained(model_name)\n",
    "# processor.save_pretrained(save_directory)\n",
    "\n",
    "model_path = \"../models/whisper\"  # Path where the model and processor are saved\n",
    "# Load the model\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model = model.float()\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b2623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processor = AudioPreprocessor(train_data, output_dir=train_dir, tokenizer=processor.tokenizer)\n",
    "val_processor = AudioPreprocessor(val_data, output_dir=val_dir, tokenizer=processor.tokenizer)\n",
    "test_processor = AudioPreprocessor(test_data,output_dir=test_dir, tokenizer=processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c93e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 700/700 [06:06<00:00,  1.91it/s]\n",
      "Processing Batches: 100%|██████████| 88/88 [00:47<00:00,  1.85it/s]\n",
      "Processing Batches: 100%|██████████| 88/88 [00:48<00:00,  1.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processor.preprocess_data()\n",
    "val_processor.preprocess_data()\n",
    "test_processor.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823651-9fc2-4f90-b51a-c9b052055490",
   "metadata": {},
   "source": [
    "### Setup Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "758030cc-2e92-49f4-bc00-83440920d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your ASR model\n",
    "class ASRModel(pl.LightningModule):\n",
    "    def __init__(self, model, processor):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids, decoder_input_ids=None):\n",
    "        return self.model(input_features=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    \n",
    "    def compute_wer(self, logits, labels):\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        pred_str = self.processor.batch_decode(pred_ids)\n",
    "        label_str = self.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        return wer(label_str, pred_str), pred_str, label_str\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, decoder_input_ids, labels = batch\n",
    "        outputs = self(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        wer_value = self.compute_wer(logits, labels)[0]\n",
    "        self.log('train_wer', wer_value, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, decoder_input_ids, labels = batch\n",
    "        outputs = self(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        wer_value = self.compute_wer(logits, labels)[0]\n",
    "        self.log('val_wer', wer_value, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        input_ids, decoder_input_ids, labels = batch\n",
    "        self.test_results = []\n",
    "        with torch.no_grad():\n",
    "            outputs = self(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            test_loss = self.loss_function(logits, labels)\n",
    "            self.log('test_loss', test_loss)\n",
    "\n",
    "            wer_value, pred_str, label_str = self.compute_wer(logits, labels)\n",
    "\n",
    "            # Store results\n",
    "            for pred, actual in zip(pred_str, label_str):\n",
    "                self.test_results.append({'predicted': pred, 'actual': actual})\n",
    "            \n",
    "            wer_value = self.compute_wer(logits, labels)[0]\n",
    "            self.log('test_wer', wer_value, prog_bar=True)\n",
    "            \n",
    "            return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Implement your optimizer configuration here\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, model, processor):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        # Initialize the model\n",
    "        instance = cls(model, processor)\n",
    "        # Load the state dict into the model\n",
    "        instance.load_state_dict(checkpoint['state_dict'])\n",
    "        return instance\n",
    "    \n",
    "class ASRIterableDataset(IterableDataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.type_dir, self.num_batches = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __iter__(self):\n",
    "        device = torch.device(\"cuda\")  # Define the device as GPU\n",
    "        for batch_idx in range(self.num_batches):\n",
    "            batch_output_dir = os.path.join(self.type_dir, f\"batch_{batch_idx}\")\n",
    "\n",
    "            # Load input_ids\n",
    "            input_ids_path = os.path.join(batch_output_dir, \"input_ids.npy\")\n",
    "            input_ids_arr = np.load(input_ids_path)\n",
    "\n",
    "            # Load decoder_input_ids\n",
    "            decoded_input_ids_path = os.path.join(batch_output_dir, \"decoder_input_ids.npy\")\n",
    "            decoded_input_ids_arr = np.load(decoded_input_ids_path)\n",
    "\n",
    "            # Load labels\n",
    "            labels_path = os.path.join(batch_output_dir, \"labels.npy\")\n",
    "            labels_arr = np.load(labels_path)\n",
    "\n",
    "            # Convert to tensors, adjust data types, and move to GPU\n",
    "            input_ids = torch.tensor(input_ids_arr, dtype=torch.float16).to(device)\n",
    "            decoder_input_ids = torch.tensor(decoded_input_ids_arr, dtype=torch.long).to(device)\n",
    "            labels = torch.tensor(labels_arr, dtype=torch.long).to(device)\n",
    "\n",
    "            yield input_ids, decoder_input_ids, labels\n",
    "\n",
    "class ASRDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, train_data, val_data, test_data, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer # can just use the global one?\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ASRIterableDataset(self.train_data, self.tokenizer)\n",
    "        self.val_dataset = ASRIterableDataset(self.val_data, self.tokenizer,)\n",
    "        self.test_dataset = ASRIterableDataset(self.test_data, self.tokenizer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=None, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=None, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46dc9e-2a9b-4e9f-a7d7-a2874caddf01",
   "metadata": {},
   "source": [
    "## Set Configs and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32a9a5fc-ad34-4ef3-8809-587488c77132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # metric to monitor\n",
    "    patience=3,          # no of epochs with no improvement to wait before stopping\n",
    "    verbose=True,        # logging\n",
    "    mode='min'           # minimize or maximize the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize Trainer with model checkpointing\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=700*100,  # Maximum number of steps (batches) to train for\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    val_check_interval=700,\n",
    "    limit_val_batches=88,  # Limit the number of validation batches\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# model_path = \"../models/whisper\"  # Path where the model and processor are saved\n",
    "# # Load the model\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "# # Load the processor\n",
    "# processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "data_module = ASRDataModule(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    train_data=(train_dir, 700),\n",
    "    val_data=(val_dir, 88),\n",
    "    test_data=(test_dir, 88),\n",
    "    num_workers=4,\n",
    "    # batch_size=1, # Removed param as setting to 2 causes errors, probably due to IterableDataset? Perhaps need to manually handle using arrays in Dataset class and update collate function.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cf6f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                            | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model         | WhisperForConditionalGeneration | 394 M \n",
      "1 | loss_function | CrossEntropyLoss                | 0     \n",
      "------------------------------------------------------------------\n",
      "392 M     Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "394 M     Total params\n",
      "1,577.501 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 41, in fetch\n    data = next(self.dataset_iter)\n  File \"/tmp/ipykernel_2998/585947386.py\", line 115, in __iter__\n    input_ids = torch.tensor(input_ids_arr, dtype=torch.float16).to(device)\n  File \"/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m asr_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43masr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# pl.LightningDataModule can be 2nd parameter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(asr_model, data_module)\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1021\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:101\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_start()\n\u001b[1;32m    103\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:217\u001b[0m, in \u001b[0;36m_EvaluationLoop.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m         _set_sampler_epoch(dl, trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_progress\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mprocessed)\n\u001b[1;32m    216\u001b[0m data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(combined_loader\u001b[38;5;241m.\u001b[39m_iterator, _Sequential)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# set the per-dataloader limits\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py:114\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_batches):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# this would only happen when prefetch_batches > the number of batches available and makes\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;66;03m# `__next__` jump directly to the empty iterator case without trying to fetch again\u001b[39;00m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py:151\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py:285\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py:123\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 41, in fetch\n    data = next(self.dataset_iter)\n  File \"/tmp/ipykernel_2998/585947386.py\", line 115, in __iter__\n    input_ids = torch.tensor(input_ids_arr, dtype=torch.float16).to(device)\n  File \"/home/rachtrx/mambaforge/envs/til-ai/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "asr_model = ASRModel(model, processor)\n",
    "asr_model.to('cuda')\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(asr_model, data_module) # pl.LightningDataModule can be 2nd parameter\n",
    "\n",
    "# Test the model\n",
    "trainer.test(asr_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b7239-7572-4511-ad33-db5c7d30ce12",
   "metadata": {},
   "source": [
    "#### Old notes\n",
    "Maximum length for padding: 219847\n",
    "<br>\n",
    "Use Max Length = 220000, which is around 13.75s for a video at 16000 samples/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a42c31-b09d-4bc6-b0aa-b9d90ed77f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = calculate_max_length(dataset, audio_dir)\n",
    "# print(f\"Maximum length for padding: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ff9fb29-daa5-4022-9c91-95d995b0d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fe613cb9da4011953e8bc472789af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6162114143371582     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_wer          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.24530015885829926    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6162114143371582    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_wer         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24530015885829926   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <|startoftranscript|>, Actual: <|startoftranscript|>\n",
      "Predicted: <|notimestamps|>, Actual: <|notimestamps|>\n",
      "Predicted: Head, Actual: Head\n",
      "Predicted: ing, Actual: ing\n",
      "Predicted:  is, Actual:  is\n",
      "Predicted:  one, Actual:  three\n",
      "Predicted:  one, Actual:  one\n",
      "Predicted:  five, Actual:  five\n",
      "Predicted: ,, Actual: ,\n",
      "Predicted:  target, Actual:  target\n",
      "Predicted:  is, Actual:  is\n",
      "Predicted:  red, Actual:  black\n",
      "Predicted:  and, Actual: ,\n",
      "Predicted:  black, Actual:  green\n",
      "Predicted: ,, Actual: ,\n",
      "Predicted:  and, Actual:  and\n",
      "Predicted:  green, Actual:  grey\n",
      "Predicted:  fighter, Actual:  drone\n",
      "Predicted: ,, Actual: ,\n",
      "Predicted:  tool, Actual:  tool\n",
      "Predicted:  to, Actual:  to\n",
      "Predicted:  deploy, Actual:  deploy\n",
      "Predicted:  is, Actual:  is\n",
      "Predicted:  surface, Actual:  drone\n",
      "Predicted:  catcher, Actual:  catcher\n",
      "Predicted: ., Actual: .\n",
      "Predicted: <|endoftext|>, Actual: <|endoftext|>\n",
      "Predicted: <|endoftext|>, Actual: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'model_checkpoints/asr_model-epoch=04-val_loss=0.61.ckpt'\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "asr_model = ASRModel.load_from_checkpoint(checkpoint_path, model=model, processor=processor)\n",
    "asr_model.to(device)\n",
    "\n",
    "# Initialize Trainer for testing (no callbacks needed)\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "# Test the model\n",
    "trainer.test(asr_model, data_module)\n",
    "\n",
    "# Print the results\n",
    "for result in asr_model.test_results:\n",
    "    print(f\"Predicted: {result['predicted']}, Actual: {result['actual']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8238dc3-5316-4c10-8f76-04400d435663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
