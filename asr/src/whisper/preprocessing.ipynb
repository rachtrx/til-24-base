{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61a4d66-8b7a-4825-ac51-75260964c562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers librosa jiwer torchaudio jsonlines datasets accelerate audiomentations # Audio Augmentation\n",
    "# !pip install -q Cython\n",
    "# !pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704bd2f1-28f4-4b65-9875-959e88f9e744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import jsonlines\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import librosa\n",
    "import jiwer\n",
    "import json\n",
    "import re\n",
    "# from tqdm import tqdm\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import whisper\n",
    "from typing import Optional, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ce7c8-f1eb-4e17-b889-12527e885908",
   "metadata": {},
   "source": [
    "### Defining Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b19dc2-ad9f-4c75-a659-f56ddbf68105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/asr.jsonl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "src_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(os.path.dirname(src_dir))\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(test_dir, 'audio')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "model_path = os.path.join(src_dir, \"models\", \"whisper\")\n",
    "metadata_path = os.path.join(test_dir, \"asr.jsonl\")\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "metadata_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035013-5786-4cc9-8272-3aec40df4659",
   "metadata": {},
   "source": [
    "### Get max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99109fd9-efc4-4bb8-8741-37e37d2649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_indices(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    total_examples = len(data['audio'])\n",
    "    indices = list(range(total_examples))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_split = int(total_examples * train_ratio)\n",
    "    val_split = int(total_examples * (train_ratio + val_ratio))\n",
    "\n",
    "    train_indices = indices[:train_split]\n",
    "    val_indices = indices[train_split:val_split]\n",
    "    test_indices = indices[val_split:]\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "def split_data(data):\n",
    "    train_indices, val_indices, test_indices = split_data_indices(data)\n",
    "\n",
    "    train_data = ([data['audio'][i] for i in train_indices], [data['sentence'][i] for i in train_indices])\n",
    "    val_data = ([data['audio'][i] for i in val_indices], [data['sentence'][i] for i in val_indices])\n",
    "    test_data = ([data['audio'][i] for i in test_indices], [data['sentence'][i] for i in test_indices])\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb03e59e-4b2d-4be6-9c97-a8439afd154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pad_to_mel(array):\n",
    "    \"\"\"Static function which:\n",
    "        1. Pads/trims a list of audio arrays to a max length of 30s\n",
    "        2. Computes log-mel filter coefficients from padded/trimmed audio sequences\n",
    "        Inputs:\n",
    "            array: list of audio arrays\n",
    "        Returns:\n",
    "            input_ids: torch.tensor of log-mel filter bank coefficients\n",
    "    \"\"\"\n",
    "    padded_input = whisper.pad_or_trim(np.asarray(array, dtype=np.float32))\n",
    "    input_ids = whisper.log_mel_spectrogram(padded_input)\n",
    "    return input_ids\n",
    "\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the audio inputs received. An EOS token is appended to the labels sequences.\n",
    "    They are then dynamically padded to max length.\n",
    "    Args:\n",
    "        eos_token_id (`int`)\n",
    "            The end-of-sentence token for the Whisper tokenizer. Ensure to set for sequences to terminate before\n",
    "            generation max length.\n",
    "    \"\"\"\n",
    "\n",
    "    eos_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Since Whisper models don't have a HF processor defined (feature extractor + tokenizer), we'll pad by hand...\n",
    "        \"\"\"\n",
    "        # print(features[0])\n",
    "        # split inputs and labels since they have to be of different lengths\n",
    "        # and need different padding methods\n",
    "        input_ids = [feature[\"input_ids\"] for feature in features]\n",
    "        decoder_input_ids = [feature[\"decoder_input_ids\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # first, pad the audio inputs to max_len\n",
    "        input_ids = torch.concat([to_pad_to_mel(input_val)[None, :] for input_val in input_ids])\n",
    "\n",
    "        # Append the eos token to the sequence of decoder input ids and labels\n",
    "        decoder_input_ids = [ids + [self.eos_token_id] for ids in decoder_input_ids]\n",
    "        labels = [lab + [self.eos_token_id] for lab in labels]\n",
    "        \n",
    "        # Pad decoder input ids and labels to max length\n",
    "        decoder_input_lengths = [len(ids) for ids in decoder_input_ids]\n",
    "        max_decoder_input_len = max(decoder_input_lengths)\n",
    "        decoder_input_ids = [np.pad(ids, (0, max_decoder_input_len - len(ids)), 'constant', constant_values=-100) for ids in decoder_input_ids]\n",
    "        \n",
    "        # finally, pad the target labels to max_len\n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        max_label_len = max(label_lengths)\n",
    "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
    "\n",
    "        batch = {\"labels\": labels, \"decoder_input_ids\": decoder_input_ids}\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823651-9fc2-4f90-b51a-c9b052055490",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "758030cc-2e92-49f4-bc00-83440920d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your ASR model\n",
    "class ASRModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_features, decoder_input_ids=None):\n",
    "        return self.model(input_features=input_features, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "        logits = outputs.logits\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "        logits = outputs.logits\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        val_loss = self.loss_function(logits, labels)\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         outputs = self(batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "#         test_loss = self.loss_function(outputs.logits, batch['labels'])\n",
    "#         self.log('test_loss', test_loss)\n",
    "#         return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Implement your optimizer configuration here\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "class ASRIterableDataset(IterableDataset):\n",
    "    def __init__(self, data, tokenizer, batch_size, transform=None, shuffle=False):\n",
    "        self.file_paths, self.sentences = data\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.file_paths, self.sentences))\n",
    "            random.shuffle(combined)\n",
    "            self.file_paths, self.sentences = zip(*combined)\n",
    "            \n",
    "        batch = []\n",
    "        \n",
    "        for file_path, transcript in zip(self.file_paths, self.sentences):\n",
    "            sample = self.load_audio(file_path)\n",
    "            if self.transform:\n",
    "                sample['input_ids'] = self.transform(audio_sample)\n",
    "            \n",
    "            tokenized_output = self.tokenizer(transcript)\n",
    "            sample['labels'] = tokenized_output.input_ids\n",
    "\n",
    "            # Generate decoder_input_ids from labels by shifting them to the right\n",
    "            decoder_input_ids = [self.tokenizer.pad_token_id] + tokenized_output.input_ids[:-1]\n",
    "            sample['decoder_input_ids'] = decoder_input_ids\n",
    "            \n",
    "            yield sample\n",
    "#             batch.append(sample)\n",
    "            \n",
    "#             if len(batch) == self.batch_size:\n",
    "#                 yield batch\n",
    "#                 batch = []\n",
    "\n",
    "        # Yield the remaining samples if they do not make up a full batch\n",
    "        # if batch:\n",
    "        #     yield batch\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        waveform = waveform.numpy().flatten() # waveform is a list\n",
    "\n",
    "        # Resample if needed\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(torch.tensor(waveform)).numpy().flatten()\n",
    "            \n",
    "        # use regex on string if needed\n",
    "\n",
    "        # Extract audio features\n",
    "        return { 'input_ids': waveform }\n",
    "        # to return sample_rate? in audio? 'audio': {'array': waveform.tolist(), 'sampling_rate': 16000},\n",
    "        # 'input_lengths': len(waveform), # seems like not needed since we are not filtering for inputs within the acceptable duration\n",
    "\n",
    "class ASRDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, train_data, val_data, test_data, batch_size=1, transform=None, collate_fn=None):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer # can just use the global one?\n",
    "        self.collate_fn = collate_fn\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ASRIterableDataset(self.train_data, self.tokenizer, self.batch_size, self.transform)\n",
    "        self.val_dataset = ASRIterableDataset(self.val_data, self.tokenizer, self.batch_size, self.transform)\n",
    "        self.test_dataset = ASRIterableDataset(self.test_data, self.tokenizer, self.batch_size, self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d3e8e3f-cada-4430-8ec0-661877c0ce53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'audio': [], 'sentence': []}\n",
    "data_path = os.path.join(test_dir, \"asr.jsonl\")\n",
    "with jsonlines.open(metadata_path) as reader:\n",
    "    for obj in reader:\n",
    "        if MAX_FILE_COUNT and len(data['audio']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        data['audio'].append(os.path.join(audio_dir, obj['audio']))\n",
    "        data['sentence'].append(obj['transcript'])\n",
    "\n",
    "train_data, val_data, test_data = split_data(data)\n",
    "\n",
    "torch_dtype = torch.float32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Instantiate your ASR model\n",
    "model = ASRModel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize Trainer with model checkpointing\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='my_model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9a5fc-ad34-4ef3-8809-587488c77132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                            | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model         | WhisperForConditionalGeneration | 394 M \n",
      "1 | loss_function | CrossEntropyLoss                | 0     \n",
      "------------------------------------------------------------------\n",
      "392 M     Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "394 M     Total params\n",
      "1,577.501 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4465bb8742464af186d3e57f2efedc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collator = WhisperDataCollatorWithPadding(\n",
    "    eos_token_id=processor.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "data_module = ASRDataModule(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    batch_size=1, # Removed param as setting to 2 causes errors, probably due to IterableDataset? Perhaps need to manually handle using arrays in Dataset class and update collate function.\n",
    "    transform=None,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=2, callbacks=[checkpoint_callback])\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module) # pl.LightningDataModule can be 2nd parameter\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6498901-f05f-40d5-b4bb-4ed9a2f9e450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def preprocess_audio(audio_path, transcript):\n",
    "#     try:\n",
    "#         waveform, sample_rate = torchaudio.load(os.path.join(audio_dir, audio_path))\n",
    "#         waveform = waveform.numpy().flatten()\n",
    "\n",
    "#         # Resample if needed\n",
    "#         if sample_rate != 16000:\n",
    "#             resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "#             waveform = resampler(torch.tensor(waveform)).numpy().flatten()\n",
    "\n",
    "#         return {\n",
    "#             'audio': {'array': waveform.tolist(), 'sampling_rate': 16000},\n",
    "#             'sentence': transcript,\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing audio: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def process_and_save_dataset(data, output_dir):\n",
    "#     dataset_dict = {'audio': [], 'sentence': []}\n",
    "#     for audio_path, transcript in tqdm(zip(data['audio'], data['sentence']), total=len(data['audio']), desc=\"Processing audio\"):\n",
    "#         processed_example = preprocess_audio(audio_path, transcript)\n",
    "#         if processed_example is not None:\n",
    "#             dataset_dict['audio'].append(processed_example['audio'])\n",
    "#             dataset_dict['sentence'].append(processed_example['sentence'])\n",
    "            \n",
    "#     # Convert dictionary to Dataset\n",
    "#     dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "#     # Save dataset to disk\n",
    "#     dataset.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af355c0-7248-4762-9613-73fee45efb21",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ec6d0-b61e-438d-a1ca-9ec6aaafaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# # model_id = \"distil-whisper/distil-medium.en\"\n",
    "\n",
    "# # model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "# #     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "# # )\n",
    "# # model.to(device)\n",
    "# # processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# # model.save_pretrained(model_path)\n",
    "# # processor.save_pretrained(model_path)\n",
    "\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "# model.to(device)\n",
    "# processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b13334-b92a-4a5d-bd72-aeb9b08d26cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab = processor.tokenizer.get_vocab()\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78971e-16d7-43ec-a644-d991c73ebe05",
   "metadata": {},
   "source": [
    "## Load and preprocess data - Ran once to create manifest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3697b-c85e-404b-a3c1-98808f270240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def split_data_indices(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "#     total_examples = len(data['audio'])\n",
    "#     indices = list(range(total_examples))\n",
    "#     random.shuffle(indices)\n",
    "\n",
    "#     train_split = int(total_examples * train_ratio)\n",
    "#     val_split = int(total_examples * (train_ratio + val_ratio))\n",
    "\n",
    "#     train_indices = indices[:train_split]\n",
    "#     val_indices = indices[train_split:val_split]\n",
    "#     test_indices = indices[val_split:]\n",
    "\n",
    "#     return train_indices, val_indices, test_indices\n",
    "\n",
    "# def preprocess_and_save(data):\n",
    "#     train_indices, val_indices, test_indices = split_data_indices(data)\n",
    "\n",
    "#     train_set = {'audio': [data['audio'][i] for i in train_indices], 'sentence': [data['sentence'][i] for i in train_indices]}\n",
    "#     val_set = {'audio': [data['audio'][i] for i in val_indices], 'sentence': [data['sentence'][i] for i in val_indices]}\n",
    "#     test_set = {'audio': [data['audio'][i] for i in test_indices], 'sentence': [data['sentence'][i] for i in test_indices]}\n",
    "\n",
    "#     process_and_save_dataset(train_set, train_dir)\n",
    "#     process_and_save_dataset(val_set, val_dir)\n",
    "#     process_and_save_dataset(test_set, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1f0a6-fcf0-4388-9042-f4380d1940c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "# data = {'audio': [], 'sentence': []}\n",
    "# data_path = os.path.join(test_dir, \"asr.jsonl\")\n",
    "# with jsonlines.open(metadata_path) as reader:\n",
    "#     for obj in reader:\n",
    "#         if MAX_FILE_COUNT and len(data['audio']) >= MAX_FILE_COUNT:\n",
    "#             break\n",
    "#         data['audio'].append(obj['audio'])\n",
    "#         data['sentence'].append(obj['transcript'])\n",
    "        \n",
    "# preprocess_and_save(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab08bdd-1e5c-4264-920a-6368b7987bbd",
   "metadata": {},
   "source": [
    "Reason for Max Length = 220000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a42c31-b09d-4bc6-b0aa-b9d90ed77f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = calculate_max_length(dataset, audio_dir)\n",
    "# print(f\"Maximum length for padding: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b7239-7572-4511-ad33-db5c7d30ce12",
   "metadata": {},
   "source": [
    "Maximum length for padding: 219847\n",
    "<br>\n",
    "Use Max Length = 220000, which is around 13.75s for a video at 16000 samples/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b825f8-1959-446b-be06-7f6457cd2043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
