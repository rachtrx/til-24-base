{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61a4d66-8b7a-4825-ac51-75260964c562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers librosa jiwer torchaudio jsonlines datasets accelerate audiomentations # Audio Augmentation\n",
    "# !pip install -q Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704bd2f1-28f4-4b65-9875-959e88f9e744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import jsonlines\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import librosa\n",
    "import jiwer\n",
    "import json\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ce7c8-f1eb-4e17-b889-12527e885908",
   "metadata": {},
   "source": [
    "### Defining Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b19dc2-ad9f-4c75-a659-f56ddbf68105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "src_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(os.path.dirname(src_dir))\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(test_dir, 'audio')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "model_path = os.path.join(src_dir, \"models\", \"whisper\")\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_manifest_path = os.path.join(data_dir, 'train.json')\n",
    "val_manifest_path = os.path.join(data_dir, 'val.json')\n",
    "test_manifest_path = os.path.join(data_dir, 'test.json')\n",
    "\n",
    "test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035013-5786-4cc9-8272-3aec40df4659",
   "metadata": {},
   "source": [
    "### Get max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94e00ff-00b2-4f7a-9938-4cacca0786d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def calculate_max_length(dataset, audio_dir):\n",
    "    max_length = 0\n",
    "    all_tensors = []\n",
    "\n",
    "    for example in dataset:\n",
    "        audio_path = os.path.join(audio_dir, example['audio'])\n",
    "        try:\n",
    "            speech_tensor, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "            # Resample to 16kHz if necessary\n",
    "            if sampling_rate != 16000:\n",
    "                resample_transform = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "                speech_tensor = resample_transform(speech_tensor)\n",
    "                del resample_transform\n",
    "\n",
    "            length = speech_tensor.shape[1]\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            # Append the tensors to the list\n",
    "            all_tensors.append((speech_tensor, sampling_rate))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    # Clear the list and call garbage collector\n",
    "    del all_tensors\n",
    "    gc.collect()\n",
    "\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# # Running on Test Dataset gave \n",
    "# max_length = calculate_max_length(dataset, audio_dir) # Maximum length for padding: 219847\n",
    "# print(f\"Maximum length for padding: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823651-9fc2-4f90-b51a-c9b052055490",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6498901-f05f-40d5-b4bb-4ed9a2f9e450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "chars_to_ignore_regex = '[,?.!\\-;:\\\"]'\n",
    "max_length = 220000  # 13.75 seconds at 16kHz\n",
    "\n",
    "# MelSpectrogram transformation\n",
    "mel_transform = transforms.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=400,\n",
    "    win_length=400,\n",
    "    hop_length=160,\n",
    "    n_mels=80\n",
    ")\n",
    "\n",
    "def preprocess_audio(example, processor, max_length):\n",
    "    audio_path = os.path.join(audio_dir, example['audio'])\n",
    "    transcript = example['transcript']\n",
    "    \n",
    "    try:\n",
    "        # Load and process the audio file\n",
    "        speech_tensor, sampling_rate = torchaudio.load(audio_path)\n",
    "        duration = speech_tensor.shape[1] / sampling_rate\n",
    "\n",
    "        # Resample to 16kHz if necessary\n",
    "        if sampling_rate != 16000:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            speech_tensor = resample_transform(speech_tensor)\n",
    "\n",
    "        # Pad or truncate the audio tensor to the maximum length\n",
    "        if speech_tensor.shape[1] < max_length:\n",
    "            padding = torch.zeros((speech_tensor.shape[0], max_length - speech_tensor.shape[1]))\n",
    "            speech_tensor = torch.cat((speech_tensor, padding), dim=1)\n",
    "        elif speech_tensor.shape[1] > max_length:\n",
    "            speech_tensor = speech_tensor[:, :max_length]\n",
    "\n",
    "        # Convert to Mel spectrogram\n",
    "        mel_spectrogram = mel_transform(speech_tensor)\n",
    "\n",
    "        # Process the transcript\n",
    "        transcript = re.sub(chars_to_ignore_regex, '', transcript)\n",
    "        labels = processor.tokenizer(transcript, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "        return {\n",
    "            'audio_filepath': audio_path,\n",
    "            'duration': duration,\n",
    "            'text': transcript,\n",
    "            'labels': labels.tolist(),\n",
    "            'mel_spectrogram': mel_spectrogram.squeeze(0).tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(dataset, output_path, processor, max_length, batch_size=100):\n",
    "    # Clear the output file at the start\n",
    "    open(output_path, 'w').close()\n",
    "\n",
    "    # Split the dataset into individual examples\n",
    "    keys = dataset['key']\n",
    "    audios = dataset['audio']\n",
    "    transcripts = dataset['transcript']\n",
    "    \n",
    "    examples = [{'key': key, 'audio': audio, 'transcript': transcript}\n",
    "                for key, audio, transcript in zip(keys, audios, transcripts)]\n",
    "    \n",
    "    # Process the dataset in batches\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i:i+batch_size]\n",
    "        manifest = []\n",
    "        \n",
    "        for example in batch:\n",
    "            result = preprocess_audio(example, processor, max_length)\n",
    "            if result:\n",
    "                manifest.append(result)\n",
    "\n",
    "        # Write the manifest to the output file after each batch\n",
    "        with open(output_path, 'a') as f:\n",
    "            for entry in manifest:\n",
    "                try:\n",
    "                    json.dump(entry, f)\n",
    "                    f.write('\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error writing entry to file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af355c0-7248-4762-9613-73fee45efb21",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84ec6d0-b61e-438d-a1ca-9ec6aaafaeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# model_id = \"distil-whisper/distil-medium.en\"\n",
    "\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "#     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "# )\n",
    "# model.to(device)\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# model.save_pretrained(model_path)\n",
    "# processor.save_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b13334-b92a-4a5d-bd72-aeb9b08d26cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab = processor.tokenizer.get_vocab()\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78971e-16d7-43ec-a644-d991c73ebe05",
   "metadata": {},
   "source": [
    "## Load and preprocess data - Ran once to create manifest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca3697b-c85e-404b-a3c1-98808f270240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 220000\n",
    "\n",
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'key': [], 'audio': [], 'transcript': []}\n",
    "data_path = os.path.join(test_dir, \"asr.jsonl\")\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    for obj in reader:\n",
    "        # for key, value in obj.items():\n",
    "        #     print(value)\n",
    "        if MAX_FILE_COUNT and len(data['key']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        for key, value in obj.items():\n",
    "            data[key].append(value)\n",
    "\n",
    "# Convert to a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, train_size + val_size + test_size))\n",
    "\n",
    "preprocess_data(train_dataset, train_manifest_path, processor, max_length)\n",
    "preprocess_data(val_dataset, val_manifest_path, processor, max_length)\n",
    "preprocess_data(test_dataset, test_manifest_path, processor, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab08bdd-1e5c-4264-920a-6368b7987bbd",
   "metadata": {},
   "source": [
    "Reason for Max Length = 220000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a42c31-b09d-4bc6-b0aa-b9d90ed77f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = calculate_max_length(dataset, audio_dir)\n",
    "# print(f\"Maximum length for padding: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b7239-7572-4511-ad33-db5c7d30ce12",
   "metadata": {},
   "source": [
    "Maximum length for padding: 219847\n",
    "<br>\n",
    "Use Max Length = 220000, which is around 13.75s for a video at 16000 samples/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b825f8-1959-446b-be06-7f6457cd2043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
