{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61a4d66-8b7a-4825-ac51-75260964c562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers jiwer torchaudio jsonlines datasets accelerate audiomentations # Audio Augmentation\n",
    "# !pip install -q Cython\n",
    "# !pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704bd2f1-28f4-4b65-9875-959e88f9e744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "from typing import Optional, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "from jiwer import wer\n",
    "import whisper\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ce7c8-f1eb-4e17-b889-12527e885908",
   "metadata": {},
   "source": [
    "### Defining Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b19dc2-ad9f-4c75-a659-f56ddbf68105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/asr.jsonl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "src_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(os.path.dirname(src_dir))\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(test_dir, 'audio')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "model_path = os.path.join(src_dir, \"models\", \"whisper\")\n",
    "metadata_path = os.path.join(test_dir, \"asr.jsonl\")\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "metadata_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035013-5786-4cc9-8272-3aec40df4659",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99109fd9-efc4-4bb8-8741-37e37d2649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_indices(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    total_examples = len(data['audio'])\n",
    "    indices = list(range(total_examples))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(train_ratio * total_examples)\n",
    "    val_end = train_end + int(val_ratio * total_examples)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "    \n",
    "    train_data = {'audio': [data['audio'][i] for i in train_indices],\n",
    "                  'sentence': [data['sentence'][i] for i in train_indices]}\n",
    "    val_data = {'audio': [data['audio'][i] for i in val_indices],\n",
    "                'sentence': [data['sentence'][i] for i in val_indices]}\n",
    "    test_data = {'audio': [data['audio'][i] for i in test_indices],\n",
    "                 'sentence': [data['sentence'][i] for i in test_indices]}\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb03e59e-4b2d-4be6-9c97-a8439afd154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspir\n",
    "\n",
    "def to_pad_to_mel(array):\n",
    "    \"\"\"Static function which:\n",
    "        1. Pads/trims a list of audio arrays to a max length of 30s\n",
    "        2. Computes log-mel filter coefficients from padded/trimmed audio sequences\n",
    "        Inputs:\n",
    "            array: list of audio arrays\n",
    "        Returns:\n",
    "            input_ids: torch.tensor of log-mel filter bank coefficients\n",
    "    \"\"\"\n",
    "    padded_input = whisper.pad_or_trim(np.asarray(array, dtype=np.float32))\n",
    "    input_ids = whisper.log_mel_spectrogram(padded_input)\n",
    "    return input_ids\n",
    "\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the audio inputs received. An EOS token is appended to the labels sequences.\n",
    "    They are then dynamically padded to max length.\n",
    "    Args:\n",
    "        eos_token_id (`int`)\n",
    "            The end-of-sentence token for the Whisper tokenizer. Ensure to set for sequences to terminate before\n",
    "            generation max length.\n",
    "    \"\"\"\n",
    "\n",
    "    eos_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Since Whisper models don't have a HF processor defined (feature extractor + tokenizer), we'll pad by hand...\n",
    "        \"\"\"\n",
    "        # print(features[0])\n",
    "        # split inputs and labels since they have to be of different lengths\n",
    "        # and need different padding methods\n",
    "        input_ids = [feature[\"input_ids\"] for feature in features]\n",
    "        decoder_input_ids = [feature[\"decoder_input_ids\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # first, pad the audio inputs to max_len\n",
    "        input_ids = torch.concat([to_pad_to_mel(input_val)[None, :] for input_val in input_ids])\n",
    "\n",
    "        # Append the eos token to the sequence of decoder input ids and labels\n",
    "        decoder_input_ids = [ids + [self.eos_token_id] for ids in decoder_input_ids]\n",
    "        labels = [lab + [self.eos_token_id] for lab in labels]\n",
    "        \n",
    "        # Pad decoder input ids and labels to max length\n",
    "        decoder_input_lengths = [len(ids) for ids in decoder_input_ids]\n",
    "        max_decoder_input_len = max(decoder_input_lengths)\n",
    "        decoder_input_ids = [np.pad(ids, (0, max_decoder_input_len - len(ids)), 'constant', constant_values=-100) for ids in decoder_input_ids]\n",
    "        \n",
    "        # finally, pad the target labels to max_len\n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        max_label_len = max(label_lengths)\n",
    "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
    "\n",
    "        batch = {\"labels\": labels, \"decoder_input_ids\": decoder_input_ids}\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823651-9fc2-4f90-b51a-c9b052055490",
   "metadata": {},
   "source": [
    "### Setup Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758030cc-2e92-49f4-bc00-83440920d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your ASR model\n",
    "class ASRModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids, decoder_input_ids=None):\n",
    "        return self.model(input_features=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    \n",
    "    def compute_wer(self, logits, labels):\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        pred_str = processor.batch_decode(pred_ids)\n",
    "        label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        return wer(label_str, pred_str)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "        logits = outputs.logits\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        wer_value = self.compute_wer(logits, labels)\n",
    "        self.log('train_wer', wer_value, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "        logits = outputs.logits\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        val_loss = self.loss_function(logits, labels)\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "        wer_value = self.compute_wer(logits, labels)\n",
    "        self.log('val_wer', wer_value, prog_bar=True)\n",
    "        \n",
    "        return val_loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "        logits = outputs.logits\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Reshape logits to (batch_size * sequence_length, vocab_size)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        test_loss = self.loss_function(logits, labels)\n",
    "        self.log('test_loss', test_loss)\n",
    "        \n",
    "        wer_value = self.compute_wer(logits, labels)\n",
    "        self.log('test_wer', wer_value, prog_bar=True)\n",
    "        \n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Implement your optimizer configuration here\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "class ASRIterableDataset(IterableDataset):\n",
    "    def __init__(self, data, tokenizer, augmentations=None, shuffle=False, transform=None):\n",
    "        self.file_paths, self.sentences = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentations = augmentations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.file_paths, self.sentences))\n",
    "            random.shuffle(combined)\n",
    "            self.file_paths, self.sentences = zip(*combined)\n",
    "            \n",
    "        batch = []\n",
    "        \n",
    "        for file_path, transcript in zip(self.file_paths, self.sentences):\n",
    "            sample = self.load_audio(file_path)\n",
    "            if self.transform:\n",
    "                sample['input_ids'] = self.transform(audio_sample)\n",
    "            \n",
    "            tokenized_output = self.tokenizer(transcript)\n",
    "            sample['labels'] = tokenized_output.input_ids\n",
    "\n",
    "            # Generate decoder_input_ids from labels by shifting them to the right\n",
    "            decoder_input_ids = [self.tokenizer.pad_token_id] + tokenized_output.input_ids[:-1]\n",
    "            sample['decoder_input_ids'] = decoder_input_ids\n",
    "            \n",
    "            yield sample\n",
    "#             batch.append(sample)\n",
    "            \n",
    "#             if len(batch) == self.batch_size:\n",
    "#                 yield batch\n",
    "#                 batch = []\n",
    "\n",
    "        # Yield the remaining samples if they do not make up a full batch\n",
    "        # if batch:\n",
    "        #     yield batch\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        waveform = waveform.numpy().flatten() # waveform is a list\n",
    "        \n",
    "        # Apply augmentations if provided\n",
    "        if self.augmentations:\n",
    "            waveform = self.augmentations(samples=waveform, sample_rate=sample_rate)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(torch.tensor(waveform)).numpy().flatten()\n",
    "            \n",
    "        # use regex on string if needed\n",
    "\n",
    "        # Extract audio features\n",
    "        return { 'input_ids': waveform }\n",
    "        # to return sample_rate? in audio? 'audio': {'array': waveform.tolist(), 'sampling_rate': 16000},\n",
    "        # 'input_lengths': len(waveform), # seems like not needed since we are not filtering for inputs within the acceptable duration\n",
    "\n",
    "class ASRDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, train_data, val_data, test_data, augmentations=None, collate_fn=None, num_workers=0, transform=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer # can just use the global one?\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.augmentations = augmentations\n",
    "        self.collate_fn = collate_fn\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ASRIterableDataset(self.train_data, self.tokenizer, self.augmentations, shuffle=True, transform=self.transform)\n",
    "        self.val_dataset = ASRIterableDataset(self.val_data, self.tokenizer, self.augmentations, transform=self.transform)\n",
    "        self.test_dataset = ASRIterableDataset(self.test_data, self.tokenizer, self.augmentations, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, collate_fn=self.collate_fn, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17790a-19a7-4405-9182-1fec53e8ddb3",
   "metadata": {},
   "source": [
    "## Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3e8e3f-cada-4430-8ec0-661877c0ce53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MAX_FILE_COUNT = None # Set if only want max files\n",
    "\n",
    "data = {'audio': [], 'sentence': []}\n",
    "data_path = os.path.join(test_dir, \"asr.jsonl\")\n",
    "with jsonlines.open(metadata_path) as reader:\n",
    "    for obj in reader:\n",
    "        if MAX_FILE_COUNT and len(data['audio']) >= MAX_FILE_COUNT:\n",
    "            break\n",
    "        data['audio'].append(os.path.join(audio_dir, obj['audio']))\n",
    "        data['sentence'].append(obj['transcript'])\n",
    "\n",
    "train_data, val_data, test_data = split_data(data)\n",
    "\n",
    "torch_dtype = torch.float32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Instantiate your ASR model\n",
    "model = ASRModel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273457a6-3426-4d45-bd88-bbb3a7f45af1",
   "metadata": {},
   "source": [
    "### Define Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fba9fc-baa6-404a-b9f0-f64dff918eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46dc9e-2a9b-4e9f-a7d7-a2874caddf01",
   "metadata": {},
   "source": [
    "## Set Configs and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a9a5fc-ad34-4ef3-8809-587488c77132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                            | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model         | WhisperForConditionalGeneration | 394 M \n",
      "1 | loss_function | CrossEntropyLoss                | 0     \n",
      "------------------------------------------------------------------\n",
      "392 M     Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "394 M     Total params\n",
      "1,577.501 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafab6be80d5409cb463adad92bf7303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398067d2418046569514a98fa14dd10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collator = WhisperDataCollatorWithPadding(\n",
    "    eos_token_id=processor.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "data_module = ASRDataModule(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    augmentations=augmentations,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    transform=None,\n",
    "    # batch_size=1, # Removed param as setting to 2 causes errors, probably due to IterableDataset? Perhaps need to manually handle using arrays in Dataset class and update collate function.\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # metric to monitor\n",
    "    patience=3,          # no of epochs with no improvement to wait before stopping\n",
    "    verbose=True,        # logging\n",
    "    mode='min'           # minimize or maximize the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize Trainer with model checkpointing\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='my_model_checkpoints',\n",
    "    filename='asr_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],  # Add early stopping callback here\n",
    "    val_check_interval=0.5,  # Check validation twice every epoch\n",
    "    check_val_every_n_epoch=1  # Ensure validation runs every epoch\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module) # pl.LightningDataModule can be 2nd parameter\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab08bdd-1e5c-4264-920a-6368b7987bbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7b7239-7572-4511-ad33-db5c7d30ce12",
   "metadata": {},
   "source": [
    "#### Old notes\n",
    "Maximum length for padding: 219847\n",
    "<br>\n",
    "Use Max Length = 220000, which is around 13.75s for a video at 16000 samples/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a42c31-b09d-4bc6-b0aa-b9d90ed77f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = calculate_max_length(dataset, audio_dir)\n",
    "# print(f\"Maximum length for padding: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b825f8-1959-446b-be06-7f6457cd2043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
