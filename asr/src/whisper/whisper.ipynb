{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704bd2f1-28f4-4b65-9875-959e88f9e744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from tqdm import tqdm\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ce7c8-f1eb-4e17-b889-12527e885908",
   "metadata": {},
   "source": [
    "### Defining Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b19dc2-ad9f-4c75-a659-f56ddbf68105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "src_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(os.path.dirname(src_dir))\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(test_dir, 'audio')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "model_path = os.path.join(src_dir, \"models\", \"whisper\")\n",
    "config_path = os.path.join(cur_dir, \"config.yaml\")\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_manifest_path = os.path.join(data_dir, 'train.json')\n",
    "val_manifest_path = os.path.join(data_dir, 'val.json')\n",
    "test_manifest_path = os.path.join(data_dir, 'test.json')\n",
    "\n",
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc58fe9-8b1f-4a4a-9626-6e766ddc23ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def validate_jsonl(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         for i, line in enumerate(f, 1):\n",
    "#             try:\n",
    "#                 json.loads(line)\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"Error decoding JSON on line {i}: {e}\")\n",
    "#                 break\n",
    "\n",
    "# validate_jsonl('./data/train.json')\n",
    "# validate_jsonl('./data/val.json')\n",
    "# validate_jsonl('./data/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea42a2-c3b2-4c63-bd44-ff20f3d8a114",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select subsets (if any) and find max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6fbc16-0b69-4815-8983-1c0d791ace7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manifest_in_chunks(manifest_path, chunk_size=1000):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(json.loads(line))\n",
    "            if len(chunk) >= chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e34553-c131-4c14-9e8f-669a9981089f",
   "metadata": {},
   "source": [
    "### Define Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16116bf-1440-4bd0-a736-62fce4fe326f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae654d-55b0-4e25-b94a-02a670ced6db",
   "metadata": {},
   "source": [
    "## Define Dataset Class and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69436a4d-32f2-4fa3-a40c-af2223379939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, manifest_chunk, augmentations=None, max_length=3000):\n",
    "        self.manifest = manifest_chunk\n",
    "        self.augmentations = augmentations\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.manifest[idx]\n",
    "\n",
    "        # Load precomputed Mel spectrogram\n",
    "        if 'mel_spectrogram' in example:\n",
    "            mel_spectrogram = torch.tensor(example['mel_spectrogram'])\n",
    "        else:\n",
    "            raise Exception(\"mel_spectrogram not found in the example.\")\n",
    "        \n",
    "        transcript = example['text']\n",
    "\n",
    "        # Apply augmentations if any\n",
    "        if self.augmentations:\n",
    "            mel_spectrogram = self.augmentations(samples=mel_spectrogram.numpy(), sample_rate=16000)\n",
    "            mel_spectrogram = torch.tensor(mel_spectrogram)\n",
    "\n",
    "        # Ensure Mel spectrogram has the correct length\n",
    "        if mel_spectrogram.shape[1] < self.max_length:\n",
    "            padding = torch.zeros((mel_spectrogram.shape[0], self.max_length - mel_spectrogram.shape[1]))\n",
    "            mel_spectrogram = torch.cat((mel_spectrogram, padding), dim=1)\n",
    "        elif mel_spectrogram.shape[1] > self.max_length:\n",
    "            mel_spectrogram = mel_spectrogram[:, :self.max_length]\n",
    "\n",
    "        return {\n",
    "            'audio_filepath': example['audio_filepath'],\n",
    "            'duration': example['duration'],\n",
    "            'text': transcript,\n",
    "            'mel_spectrogram': mel_spectrogram,\n",
    "            'labels': torch.tensor(example['labels'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78c28-2214-4617-8f0b-53947f4642a6",
   "metadata": {},
   "source": [
    "### Define Collate Function and Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c954f81-f2d6-45b9-9736-54ff0826b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Stack Mel spectrograms and labels into batches\n",
    "    mel_spectrograms = torch.stack([item['mel_spectrogram'] for item in batch])\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    max_label_length = max(len(label) for label in labels)\n",
    "    padded_labels = torch.zeros((len(labels), max_label_length), dtype=torch.long)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        padded_labels[i, :len(label)] = label\n",
    "\n",
    "    audio_filepaths = [item['audio_filepath'] for item in batch]\n",
    "    durations = [item['duration'] for item in batch]\n",
    "    texts = [item['text'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_values': mel_spectrograms.to(dtype=torch.float16),  # Convert to float16\n",
    "        'labels': padded_labels,\n",
    "        'audio_filepaths': audio_filepaths,\n",
    "        'durations': durations,\n",
    "        'texts': texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a667237-8ef6-48c8-9ed1-36a2aa2f039e",
   "metadata": {},
   "source": [
    "## Define Model Class and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb632c18-1698-4e7f-878b-534a1bb46fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperASRModel(pl.LightningModule):\n",
    "    def __init__(self, model, processor, lr):\n",
    "        super(WhisperASRModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.lr = lr\n",
    "        self.test_predictions = []\n",
    "        self.test_references = []\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.model(input_values)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_values = batch['input_values'].to(dtype=torch.float16)\n",
    "        labels = batch['labels']\n",
    "        outputs = self.model(input_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_values = batch['input_values'].to(dtype=torch.float16)\n",
    "        labels = batch['labels']\n",
    "        outputs = self.model(input_values, labels=labels)\n",
    "        val_loss = outputs.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_values = batch['input_values'].to(dtype=torch.float16)\n",
    "        labels = batch['labels']\n",
    "        outputs = self.model.generate(input_values)  # Generate predictions\n",
    "        predicted_texts = self.processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Decode the actual labels\n",
    "        actual_texts = self.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Collect predictions and actual texts\n",
    "        self.test_predictions.extend(predicted_texts)\n",
    "        self.test_references.extend(actual_texts)\n",
    "\n",
    "        # Log the test loss\n",
    "        outputs = self.model(input_values, labels=labels)\n",
    "        test_loss = outputs.loss\n",
    "        self.log('test_loss', test_loss)\n",
    "        return test_loss\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # Print predictions and actual texts\n",
    "        for pred, actual in zip(self.test_predictions, self.test_references):\n",
    "            print(f\"PRED: {pred}\")\n",
    "            print(f\"ACTUAL: {actual}\")\n",
    "            print('-' * 40)  # Separator for readability\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e98db4-b8cd-4111-8298-7d100d4a3539",
   "metadata": {},
   "source": [
    "### Setup and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79fd6ce5-a3b7-41b9-ad35-74abd21f717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataloader(manifest_chunk, batch_size, shuffle, num_workers):\n",
    "#     dataset = AudioDataset(manifest_chunk)\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=custom_collate_fn)\n",
    "\n",
    "# def train_and_evaluate(train_manifest_path, val_manifest_path, test_manifest_path, processor, model, config, chunk_size=50):\n",
    "#     whisper_asr_model = WhisperASRModel(model, processor, config.optim.lr)\n",
    "#     trainer = pl.Trainer(**config.trainer)\n",
    "\n",
    "#     for epoch in range(config.trainer.max_epochs):\n",
    "#         print(f\"Epoch {epoch + 1}/{config.trainer.max_epochs}\")\n",
    "\n",
    "#         # Load training data in chunks\n",
    "#         for train_chunk in load_manifest_in_chunks(train_manifest_path, chunk_size):\n",
    "#             train_loader = get_dataloader(train_chunk, config.model.train_ds.batch_size, config.model.train_ds.shuffle, config.model.train_ds.num_workers)\n",
    "#             trainer.fit(whisper_asr_model, train_dataloaders=train_loader)\n",
    "\n",
    "#         # Validate\n",
    "#         for val_chunk in load_manifest_in_chunks(val_manifest_path, chunk_size):\n",
    "#             val_loader = get_dataloader(val_chunk, config.model.validation_ds.batch_size, config.model.validation_ds.shuffle, config.model.validation_ds.num_workers)\n",
    "#             trainer.validate(whisper_asr_model, val_dataloaders=val_loader)\n",
    "\n",
    "#     # Test\n",
    "#     for test_chunk in load_manifest_in_chunks(test_manifest_path, chunk_size):\n",
    "#         test_loader = get_dataloader(test_chunk, config.model.test_ds.batch_size, config.model.test_ds.shuffle, config.model.test_ds.num_workers)\n",
    "#         trainer.test(whisper_asr_model, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3d87618-c350-440f-859f-b54ab9622a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import IterableDataset\n",
    "\n",
    "class ChunkedDataset(IterableDataset):\n",
    "    def __init__(self, manifest_path, chunk_size):\n",
    "        self.manifest_path = manifest_path\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for chunk in load_manifest_in_chunks(self.manifest_path, self.chunk_size):\n",
    "            dataset = AudioDataset(chunk, augmentations)\n",
    "            for data in dataset:\n",
    "                yield data\n",
    "                \n",
    "def get_dataloader(manifest_path, processor, batch_size, num_workers, chunk_size):\n",
    "    dataset = ChunkedDataset(manifest_path, chunk_size)\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=custom_collate_fn)\n",
    "\n",
    "def train_and_evaluate(train_manifest_path, val_manifest_path, test_manifest_path, processor, model, config, chunk_size=50):\n",
    "    whisper_asr_model = WhisperASRModel(model, processor, config.optim.lr)\n",
    "    trainer = pl.Trainer(**config.trainer)\n",
    "\n",
    "    # Prepare the train DataLoader\n",
    "    train_loader = get_dataloader(train_manifest_path, processor, config.model.train_ds.batch_size, config.model.train_ds.num_workers, chunk_size)\n",
    "\n",
    "    # Prepare the validation DataLoader\n",
    "    val_loader = get_dataloader(val_manifest_path, processor, config.model.validation_ds.batch_size, config.model.validation_ds.num_workers, chunk_size)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(whisper_asr_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # Prepare the test DataLoader\n",
    "    test_loader = get_dataloader(test_manifest_path, processor, config.model.test_ds.batch_size, config.model.test_ds.num_workers, chunk_size)\n",
    "\n",
    "    # Test the model\n",
    "    trainer.test(whisper_asr_model, test_dataloaders=test_loader)\n",
    "\n",
    "    # Print the test results\n",
    "    whisper_asr_model.test_epoch_end(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd65e5ae-fecc-4459-9c7e-114675cd4985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cff66b94-21ff-4591-9e6a-64996fcb3f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | WhisperForConditionalGeneration | 394 M \n",
      "----------------------------------------------------------\n",
      "392 M     Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "394 M     Total params\n",
      "1,577.501 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75fc6d6504a41f0aa468c7b9728437d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.test() got an unexpected keyword argument 'test_dataloaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_ds\u001b[38;5;241m.\u001b[39mmanifest_filepath \u001b[38;5;241m=\u001b[39m val_manifest_path\n\u001b[1;32m      6\u001b[0m config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest_ds\u001b[38;5;241m.\u001b[39mmanifest_filepath \u001b[38;5;241m=\u001b[39m test_manifest_path\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_manifest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_manifest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_manifest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 35\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_manifest_path, val_manifest_path, test_manifest_path, processor, model, config, chunk_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m get_dataloader(test_manifest_path, processor, config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest_ds\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest_ds\u001b[38;5;241m.\u001b[39mnum_workers, chunk_size)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhisper_asr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Print the test results\u001b[39;00m\n\u001b[1;32m     38\u001b[0m whisper_asr_model\u001b[38;5;241m.\u001b[39mtest_epoch_end(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.test() got an unexpected keyword argument 'test_dataloaders'"
     ]
    }
   ],
   "source": [
    "### Update config\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "config.model.train_ds.manifest_filepath = train_manifest_path\n",
    "config.model.validation_ds.manifest_filepath = val_manifest_path\n",
    "config.model.test_ds.manifest_filepath = test_manifest_path\n",
    "\n",
    "train_and_evaluate(train_manifest_path, val_manifest_path, test_manifest_path, processor, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a916c5-3a33-4315-87c5-0875c24d7d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
