{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a77d649-9ff0-4991-b3c8-3bcf71d00a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import jsonlines\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1919fb63-98cd-445e-a76b-14d9e1f12012",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "src_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(os.path.dirname(src_dir))\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(test_dir, 'audio')\n",
    "data_dir = os.path.join(cur_dir, 'data')\n",
    "model_path = os.path.join(src_dir, \"models\", \"whisper\")\n",
    "config_path = os.path.join(cur_dir, \"config.yaml\")\n",
    "\n",
    "# paths for converting datasets to manifest files\n",
    "train_path = os.path.join(data_dir, 'train_data')\n",
    "val_path = os.path.join(data_dir, 'val_data')\n",
    "test_path = os.path.join(data_dir, 'test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0716df24-ef56-4a7b-892e-530362fb7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define variables directly instead of parsing from command line\n",
    "# model_name = 'openai/whisper-small'\n",
    "# language = 'English'\n",
    "# sampling_rate = 16000\n",
    "# num_proc = 2\n",
    "# train_strategy = 'steps'\n",
    "# learning_rate = 1.75e-5\n",
    "# warmup = 20000\n",
    "# train_batchsize = 48\n",
    "# eval_batchsize = 32\n",
    "# num_epochs = 20\n",
    "# num_steps = 100000\n",
    "# resume_from_ckpt = None\n",
    "# output_dir = './test'\n",
    "# train_datasets = [train_path]  # Add your paths\n",
    "# eval_datasets = [val_path]  # Add your paths\n",
    "\n",
    "# # Check train strategy validity\n",
    "# if train_strategy not in ['steps', 'epoch']:\n",
    "#     raise ValueError('The train strategy should be either steps or epoch.')\n",
    "\n",
    "# print('\\n\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\\n')\n",
    "# print('ARGUMENTS OF INTEREST:')\n",
    "# args = {\n",
    "#     'model_name': model_name,\n",
    "#     'language': language,\n",
    "#     'sampling_rate': sampling_rate,\n",
    "#     'num_proc': num_proc,\n",
    "#     'train_strategy': train_strategy,\n",
    "#     'learning_rate': learning_rate,\n",
    "#     'warmup': warmup,\n",
    "#     'train_batchsize': train_batchsize,\n",
    "#     'eval_batchsize': eval_batchsize,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'num_steps': num_steps,\n",
    "#     'resume_from_ckpt': resume_from_ckpt,\n",
    "#     'output_dir': output_dir,\n",
    "#     'train_datasets': train_datasets,\n",
    "#     'eval_datasets': eval_datasets,\n",
    "# }\n",
    "# print(args)\n",
    "# print('\\n\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\\n')\n",
    "\n",
    "# gradient_checkpointing = True\n",
    "# freeze_feature_encoder = False\n",
    "# freeze_encoder = False\n",
    "\n",
    "# do_normalize_eval = True\n",
    "# do_lower_case = False\n",
    "# do_remove_punctuation = False\n",
    "# normalizer = BasicTextNormalizer()\n",
    "\n",
    "# #############################       MODEL LOADING       #####################################\n",
    "\n",
    "# feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "# tokenizer = WhisperTokenizer.from_pretrained(model_name, language=language, task=\"transcribe\")\n",
    "# processor = WhisperProcessor.from_pretrained(model_name, language=language, task=\"transcribe\")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# if model.config.decoder_start_token_id is None:\n",
    "#     raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "# if freeze_feature_encoder:\n",
    "#     model.freeze_feature_encoder()\n",
    "\n",
    "# if freeze_encoder:\n",
    "#     model.freeze_encoder()\n",
    "#     model.model.encoder.gradient_checkpointing = False\n",
    "\n",
    "# model.config.forced_decoder_ids = None\n",
    "# model.config.suppress_tokens = []\n",
    "\n",
    "# if gradient_checkpointing:\n",
    "#     model.config.use_cache = False\n",
    "\n",
    "# ############################        DATASET LOADING AND PREP        ##########################\n",
    "\n",
    "# def load_custom_dataset(split):\n",
    "#     ds = []\n",
    "#     if split == 'train':\n",
    "#         for dset in train_datasets:\n",
    "#             ds.append(load_from_disk(dset))\n",
    "#     elif split == 'eval':\n",
    "#         for dset in eval_datasets:\n",
    "#             ds.append(load_from_disk(dset))\n",
    "\n",
    "#     ds_to_return = concatenate_datasets(ds)\n",
    "#     ds_to_return = ds_to_return.shuffle(seed=22)\n",
    "#     return ds_to_return\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     # load and (possibly) resample audio data to 16kHz\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # compute log-Mel input features from input audio array \n",
    "#     batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "#     # compute input length of audio sample in seconds\n",
    "#     batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "#     # optional pre-processing steps\n",
    "#     transcription = batch[\"sentence\"]\n",
    "#     if do_lower_case:\n",
    "#         transcription = transcription.lower()\n",
    "#     if do_remove_punctuation:\n",
    "#         transcription = normalizer(transcription).strip()\n",
    "    \n",
    "#     # encode target text to label ids\n",
    "#     batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
    "#     return batch\n",
    "\n",
    "# max_label_length = model.config.max_length\n",
    "# min_input_length = 0.0\n",
    "# max_input_length = 30.0\n",
    "\n",
    "# def is_in_length_range(length, labels):\n",
    "#     return min_input_length < length < max_input_length and 0 < len(labels) < max_label_length\n",
    "\n",
    "# print('DATASET PREPARATION IN PROGRESS...')\n",
    "# raw_dataset = DatasetDict()\n",
    "# raw_dataset[\"train\"] = load_custom_dataset('train')\n",
    "# raw_dataset[\"eval\"] = load_custom_dataset('eval')\n",
    "\n",
    "# raw_dataset = raw_dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "# raw_dataset = raw_dataset.map(prepare_dataset, num_proc=num_proc)\n",
    "\n",
    "# raw_dataset = raw_dataset.filter(\n",
    "#     is_in_length_range,\n",
    "#     input_columns=[\"input_length\", \"labels\"],\n",
    "#     num_proc=num_proc,\n",
    "# )\n",
    "\n",
    "# ###############################     DATA COLLATOR AND METRIC DEFINITION     ########################\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "#     processor: Any\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "#         # first treat the audio inputs by simply returning torch tensors\n",
    "#         input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "#         batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "#         # get the tokenized label sequences\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "#         # pad the labels to max length\n",
    "#         labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         # if bos token is appended in previous tokenization step,\n",
    "#         # cut bos token here as it's append later anyways\n",
    "#         if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "#             labels = labels[:, 1:]\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch\n",
    "\n",
    "# data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "# print('DATASET PREPARATION COMPLETED')\n",
    "\n",
    "# metric = evaluate.load(\"wer\")\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_ids = pred.predictions\n",
    "#     label_ids = pred.label_ids\n",
    "\n",
    "#     # replace -100 with the pad_token_id\n",
    "#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "#     if do_normalize_eval:\n",
    "#         pred_str = [normalizer(pred) for pred in pred_str]\n",
    "#         label_str = [normalizer(label) for label in label_str]\n",
    "\n",
    "#     wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "#     return {\"wer\": wer}\n",
    "\n",
    "# ###############################     TRAINING ARGS AND TRAINING      ############################\n",
    "\n",
    "# if train_strategy == 'epoch':\n",
    "#     training_args = Seq2SeqTrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         per_device_train_batch_size=train_batchsize,\n",
    "#         gradient_accumulation_steps=1,\n",
    "#         learning_rate=learning_rate,\n",
    "#         warmup_steps=warmup,\n",
    "#         gradient_checkpointing=gradient_checkpointing,\n",
    "#         fp16=True,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         num_train_epochs=num_epochs,\n",
    "#         save_total_limit=10,\n",
    "#         per_device_eval_batch_size=eval_batchsize,\n",
    "#         predict_with_generate=True,\n",
    "#         generation_max_length=225,\n",
    "#         logging_steps=500,\n",
    "#         report_to=[\"tensorboard\"],\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"wer\",\n",
    "#         greater_is_better=False,\n",
    "#         optim=\"adamw_bnb_8bit\",\n",
    "#         resume_from_checkpoint=resume_from_ckpt,\n",
    "#     )\n",
    "\n",
    "# elif train_strategy == 'steps':\n",
    "#     training_args = Seq2SeqTrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         per_device_train_batch_size=train_batchsize,\n",
    "#         gradient_accumulation_steps=1,\n",
    "#         learning_rate=learning_rate,\n",
    "#         warmup_steps=warmup,\n",
    "#         gradient_checkpointing=gradient_checkpointing,\n",
    "#         fp16=True,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         eval_steps=1000,\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=1000,\n",
    "#         max_steps=num_steps,\n",
    "#         save_total_limit=10,\n",
    "#         per_device_eval_batch_size=eval_batchsize,\n",
    "#         predict_with_generate=True,\n",
    "#         generation_max_length=225,\n",
    "#         logging_steps=500,\n",
    "#         report_to=[\"tensorboard\"],\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"wer\",\n",
    "#         greater_is_better=False,\n",
    "#         optim=\"adamw_bnb_8bit\",\n",
    "#         resume_from_checkpoint=resume_from_ckpt,\n",
    "#     )\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     args=training_args,\n",
    "#     model=model,\n",
    "#     train_dataset=raw_dataset[\"train\"],\n",
    "#     eval_dataset=raw_dataset[\"eval\"],\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "# )\n",
    "\n",
    "# processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# print('TRAINING IN PROGRESS...')\n",
    "# trainer.train()\n",
    "# print('DONE TRAINING')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e1d25e-6d8b-47d5-b7a3-ba6a4af223f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'openai/whisper-small'\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "935d9211-364d-4494-a05b-bf6944efad66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,  # Set batch size to 1 to train on individual examples\n",
    "    gradient_accumulation_steps=1,  # Set gradient accumulation steps to 1\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    max_steps=100000,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751129ca-cd62-49c8-aaae-3df78868b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your batch files\n",
    "batch_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith(\".jsonl\")]\n",
    "output_dir = './test'\n",
    "\n",
    "test_file = batch_files.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3455cf66-7296-4baf-aa49-5fa6f7a44890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Extract batch data\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         audio_batch, transcript_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch_data)\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# Convert batch data to tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     15\u001b[0m     batch_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_files[idx]\n\u001b[0;32m---> 16\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_data\n",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mCustomDataset.load_batch\u001b[0;34m(self, batch_file)\u001b[0m\n\u001b[1;32m     20\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m jsonlines\u001b[38;5;241m.\u001b[39mopen(batch_file) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# Process each object in the JSONL file as needed\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# For example, extract audio and transcript\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         audio \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m         transcript \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/jsonlines/jsonlines.py:434\u001b[0m, in \u001b[0;36mReader.iter\u001b[0;34m(self, type, allow_none, skip_empty, skip_invalid)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_none\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_empty\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m InvalidLineError:\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_invalid:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/jsonlines/jsonlines.py:326\u001b[0m, in \u001b[0;36mReader.read\u001b[0;34m(self, type, allow_none, skip_empty)\u001b[0m\n\u001b[1;32m    323\u001b[0m     line \u001b[38;5;241m=\u001b[39m line[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     value: JSONValue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m orig_exc:\n\u001b[1;32m    328\u001b[0m     exc \u001b[38;5;241m=\u001b[39m InvalidLineError(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline contains invalid json: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_exc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, line, lineno\n\u001b[1;32m    330\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define your custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, batch_files):\n",
    "        self.batch_files = batch_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_file = self.batch_files[idx]\n",
    "        return batch_file\n",
    "\n",
    "# Define hyperparameters and settings\n",
    "batch_size = 1  # Process one batch at a time\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define paths to your batch files\n",
    "batch_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith(\".jsonl\")]\n",
    "\n",
    "# Create your custom dataset and data loader\n",
    "train_dataset = CustomDataset(batch_files)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_file in train_loader:\n",
    "        # Load data from JSONL file\n",
    "        with jsonlines.open(batch_file) as reader:\n",
    "            for obj in reader:\n",
    "                # Process each object in the JSONL file as needed\n",
    "                audio = obj['audio']\n",
    "                transcript = obj['transcript']\n",
    "\n",
    "                # Convert data to tensors\n",
    "                audio_tensor = torch.tensor(audio)\n",
    "                transcript_tensor = torch.tensor(transcript)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(audio_tensor)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, transcript_tensor)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9cd5bb-6ffb-4faf-bfe6-035dc1349f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'transcript', 'input_features', 'input_length', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "def load_file(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return next(iter(reader))\n",
    "\n",
    "data = load_file('./data/batch_17.jsonl')\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce29a0-9c64-46f6-9de8-30fe026181b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(load_jsonl(test_file))\n",
    "test_results = trainer.evaluate(test_data)\n",
    "print(\"Test Results:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
