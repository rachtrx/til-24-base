{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42bd923e-4a53-4df4-a12f-8f42f9b5dd9f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Task: `Automatic Speech Recognition`\n",
    "\n",
    "Given an audio file of a turret command instruction, return a transcription of the instruction.\n",
    "\n",
    "Note that noise simulating the corruption of radio transmissions will be present in the audio datasets provided to **both Novice and Advanced teams.**\n",
    "\n",
    "- Audio files are provided in .WAV format with a sample rate of 16 kHz. Images are provided as 1520x870 JPG files.\n",
    "- In the **audio datasets** provided to both the Novice and Advanced Guardians, noise will be present. Guardians who wish to fine-tune their models on additional data are free to use the (clean, unaugmented) National Speech Corpus data present in the `til-ai-24-data` bucket on Google Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f50e01-5d4a-4706-be0e-a5b19b676b15",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd8288-74af-4ad7-bea5-7ceb132061ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers librosa jiwer torchaudio jsonlines datasets accelerate audiomentations # Audio Augmentation\n",
    "!pip install -q Cython\n",
    "!pip install -q nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa46e9d4-ad3a-4ef9-87c9-618291616d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import audiomentations\n",
    "import jsonlines\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import jiwer\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29c69a3-efc2-4253-aba1-ffc52be93fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/audio'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "asr_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(asr_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "data_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(data_dir, 'audio')\n",
    "\n",
    "audio_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fa074e-5c54-475c-9a0d-6cceaf1b6650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current logging level: WARNING\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "# Get the current logging level\n",
    "current_log_level = root_logger.getEffectiveLevel()\n",
    "\n",
    "# Print the current logging level\n",
    "print(f\"Current logging level: {logging.getLevelName(current_log_level)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcc418-9a4f-49ac-90f6-707c01b65060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Load your pre-trained ASR model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\") # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c330ba9-a0b5-465b-a702-e237cef2c8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "def calculate_wer(actual_sentences, predicted_sentences):\n",
    "    \"\"\" Returns: float: The average WER.\"\"\"\n",
    "    wer_values = [wer(actual, predicted) for actual, predicted in zip(actual_sentences, predicted_sentences)]\n",
    "    average_wer = sum(wer_values) / len(wer_values)\n",
    "    \n",
    "    for i, (actual, predicted, wer_value) in enumerate(zip(actual_sentences, predicted_sentences, wer_values)):\n",
    "        print(f\"Sentence {i+1} WER: {wer_value:.2f}\")\n",
    "    \n",
    "    print(f\"Average WER: {average_wer:.2f}\")\n",
    "    return average_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b2c6a-29f3-4aab-9fb4-d90cb8d011c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {'key': [], 'audio': [], 'transcript': []}\n",
    "data_path = os.path.join(data_dir, \"asr.jsonl\")\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    for obj in reader:\n",
    "        if len(data['key']) >= 10:\n",
    "            break\n",
    "        for key, value in obj.items():\n",
    "            data[key].append(value)\n",
    "\n",
    "actual_sentences = []\n",
    "predicted_sentences = []\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "for file_name, transcript in zip(data['audio'], data['transcript']):\n",
    "    actual_sentences.append(transcript)\n",
    "    \n",
    "    file_path = os.path.join(audio_dir, file_name)\n",
    "    audio_input, sample_rate = librosa.load(file_path, sr=16000)  # Ensure sample rate is 16kHz for Wav2Vec2\n",
    "    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values.to('cuda')  # Move input values to GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits  # Forward pass\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_sentences.append(processor.batch_decode(predicted_ids)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd15259-a4b2-4327-b35e-e7226540d638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate_wer(actual_sentences, predicted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f76ab-af41-4c42-8f18-9b0cc936d54f",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3d931-9c53-4934-bc9d-3b47b5d451a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "batch_size = 64\n",
    "total_size = None # set to None if use all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8053b44c-2713-43d2-901d-4256a257caa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    audio_paths = examples['audio']\n",
    "    transcripts = examples['transcript']\n",
    "    \n",
    "    # Load and process the audio files\n",
    "    speech_arrays = [torchaudio.load(os.path.join(audio_dir, path))[0].squeeze(0) for path in audio_paths]\n",
    "    sampling_rates = [torchaudio.load(os.path.join(audio_dir, path))[1] for path in audio_paths]\n",
    "    \n",
    "    # the following line calls processor.feature_extractor\n",
    "    processed_inputs = [processor(speech, sampling_rate=rate, return_tensors=\"pt\", padding=True) for speech, rate in zip(speech_arrays, sampling_rates)]\n",
    "    \n",
    "    input_values = [processed.input_values.squeeze(0) for processed in processed_inputs]\n",
    "    \n",
    "    # Create attention masks based on the input values\n",
    "    attention_masks = [torch.ones_like(values) for values in input_values]\n",
    "    for mask, values in zip(attention_masks, input_values):\n",
    "        mask[values == processor.tokenizer.pad_token_id] = 0  # Set padding tokens to 0\n",
    "    \n",
    "    # Process and pad the labels\n",
    "    processed_labels = []\n",
    "    for transcript in transcripts:\n",
    "        transcript = re.sub(chars_to_ignore_regex, '', transcript).upper()\n",
    "        with processor.as_target_processor():\n",
    "            label = processor(transcript, return_tensors=\"pt\", padding=True)\n",
    "        processed_labels.append(label.input_ids.squeeze(0))\n",
    "    \n",
    "    max_input_length = max([values.size(0) for values in input_values])\n",
    "    max_label_length = max([label.size(0) for label in processed_labels])\n",
    "    \n",
    "    padded_input_values = [torch.nn.functional.pad(values, (0, max_input_length - values.size(0)), value=processor.tokenizer.pad_token_id) for values in input_values]\n",
    "    padded_attention_masks = [torch.nn.functional.pad(mask, (0, max_input_length - mask.size(0)), value=0) for mask in attention_masks]\n",
    "    \n",
    "    padded_labels = [torch.nn.functional.pad(label, (0, max_label_length - label.size(0)), value=-100) for label in processed_labels]\n",
    "    \n",
    "    # DEBUG Check shapes\n",
    "    # print(\"Input values shapes:\", [values.shape for values in padded_input_values])\n",
    "    # print(\"Attention masks shapes:\", [mask.shape for mask in padded_attention_masks])\n",
    "    # print(\"Labels shapes:\", [label.shape for label in padded_labels])\n",
    "    \n",
    "    examples['input_values'] = padded_input_values\n",
    "    examples['attention_mask'] = padded_attention_masks\n",
    "    examples['labels'] = padded_labels\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c1f1773-5f98-4a80-b30f-29d7952d8af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889cf2d634d1449d9a4f264c382ba6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a550d05115054c168a35b38ae4a30b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6d9f4e419f4001a0ded811bbce7db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'key': [], 'audio': [], 'transcript': []}\n",
    "data_path = os.path.join(data_dir, \"asr.jsonl\")\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    for obj in reader:\n",
    "        if total_size and len(data['key']) > total_size:\n",
    "            break\n",
    "        for key, value in obj.items():\n",
    "            data[key].append(value)\n",
    "\n",
    "# Convert to a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, train_size + val_size + test_size))\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True, batch_size=batch_size, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True, batch_size=batch_size, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True, batch_size=batch_size, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5de17-6593-45af-bbd0-d91c7aba08cf",
   "metadata": {},
   "source": [
    "## Save the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e57cf48-b180-4648-978c-e651a3c529f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c82039dcb143c1bc390314b571a145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19394d902983407aafc476e1620ca5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e175cc96a42437bbef120057de5062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_dataset.save_to_disk('./data/train_dataset')\n",
    "# val_dataset.save_to_disk('./data/val_dataset')\n",
    "# test_dataset.save_to_disk('./data/test_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad690bc-5d63-4430-a5b5-72770c762806",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3be7c3f-3a6c-48ac-8087-03f253292d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk('./data/train_dataset')\n",
    "val_dataset = load_from_disk('./data/val_dataset')\n",
    "test_dataset = load_from_disk('./data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442b0b21-c0db-4d9c-a708-be6f31b8fed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        input_values = torch.tensor(item['input_values'], dtype=torch.float32).clone().detach()\n",
    "        attention_mask = torch.tensor(item['attention_mask'], dtype=torch.int64).clone().detach()\n",
    "        labels = torch.tensor(item['labels'], dtype=torch.int64).clone().detach()\n",
    "        return {\n",
    "            'input_values': input_values,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8478d4-5b3f-43a5-a952-9c4400f5bead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_dataset)\n",
    "val_dataset = CustomDataset(val_dataset)\n",
    "test_dataset = CustomDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed90dbb-6b6f-48f0-bb72-817cf6989e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # DEBUG: Check the first few samples from the preprocessed training dataset\n",
    "# for i in range(3):\n",
    "#     sample = train_dataset[i]\n",
    "#     print(f\"Sample {i+1}:\")\n",
    "#     print(f\"  input_values: {sample['input_values'][:10]}... (length: {len(sample['input_values'])})\")  # Print first 10 values\n",
    "#     print(f\"  attention_mask: {sample['attention_mask'][:10]}... (length: {len(sample['attention_mask'])})\")\n",
    "#     print(f\"  labels: {sample['labels'][:10]}... (length: {len(sample['labels'])})\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fe73e-bd1d-45a2-b2a3-f14bf633c606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # TODO check if need true\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9adc3-8b85-42b2-8b34-15556f3ed0e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # DEBUG last 3 batches of train_dataloader\n",
    "# start_idx = len(train_dataloader) - 3\n",
    "# print(start_idx)\n",
    "\n",
    "# for batch_idx, batch in enumerate(train_dataloader):\n",
    "    \n",
    "#     if batch_idx < start_idx:  # Only print shapes for the first 3 batches to avoid excessive output\n",
    "#         continue\n",
    "    \n",
    "#     print(f\"Batch {batch_idx}:\")\n",
    "#     input_values_shape = batch['input_values'].shape\n",
    "#     attention_mask_shape = batch['attention_mask'].shape\n",
    "#     labels_shape = batch['labels'].shape\n",
    "\n",
    "#     print(\"Batch input values shape:\", input_values_shape)\n",
    "#     print(\"Batch attention mask shape:\", attention_mask_shape)\n",
    "#     print(\"Batch labels shape:\", labels_shape)\n",
    "\n",
    "#     # Print the shapes of each item within the batch\n",
    "#     for item_idx in range(input_values_shape[0]):\n",
    "#         print(f\"  Item {item_idx} input values shape:\", batch['input_values'][item_idx].shape)\n",
    "#         print(f\"  Item {item_idx} attention mask shape:\", batch['attention_mask'][item_idx].shape)\n",
    "#         print(f\"  Item {item_idx} labels shape:\", batch['labels'][item_idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db92ff-b95f-42de-9f11-e94cbfdad50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_values = [feature['input_values'] for feature in features]\n",
    "        attention_mask = [feature['attention_mask'] for feature in features]\n",
    "        labels = [feature['labels'] for feature in features]\n",
    "\n",
    "        # Determine the max length for padding\n",
    "        max_input_length = max([len(input_value) for input_value in input_values])\n",
    "        max_label_length = max([len(label) for label in labels])\n",
    "\n",
    "        # Pad input values and attention masks\n",
    "        padded_input_values = [torch.nn.functional.pad(input_value, (0, max_input_length - len(input_value)), value=self.processor.tokenizer.pad_token_id) for input_value in input_values]\n",
    "        padded_attention_mask = [torch.nn.functional.pad(mask, (0, max_input_length - len(mask)), value=0) for mask in attention_mask]\n",
    "\n",
    "        # Pad labels\n",
    "        padded_labels = [torch.nn.functional.pad(label, (0, max_label_length - len(label)), value=-100) for label in labels]\n",
    "\n",
    "        # Stack the tensors\n",
    "        batch = {\n",
    "            'input_values': torch.stack(padded_input_values),\n",
    "            'attention_mask': torch.stack(padded_attention_mask),\n",
    "            'labels': torch.stack(padded_labels)\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b3799-0720-4f4a-84fa-b98ab2fecce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.005,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initially freeze all layers except the classifier layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "data_collator = CustomDataCollator(processor)\n",
    "    \n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # Use the validation dataset for evaluation\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b2122-021c-4e43-8d9e-354b65c50bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_sentences = []\n",
    "actual_sentences = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_values = batch['input_values'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_values=input_values, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the predicted token IDs\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Decode the predicted IDs to text\n",
    "        predicted_transcripts = [processor.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "\n",
    "        # Ensure labels are on the CPU and convert them to NumPy arrays if necessary\n",
    "        label_ids = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "        \n",
    "        # debug\n",
    "        # print(f\"Raw label IDs: {label_ids}\")\n",
    "        # print(f\"Label IDs length before processing: {len(label_ids)}\")\n",
    "        # print(f\"First label length before processing: {len(label_ids[0])}\")\n",
    "\n",
    "        # Replace the padding value (-100) with the pad_token_id of the tokenizer\n",
    "        pad_token_id = processor.tokenizer.pad_token_id\n",
    "        label_ids = [[id if id != -100 else pad_token_id for id in sent] for sent in label_ids]\n",
    "\n",
    "        # Decode the actual labels to text\n",
    "        actual_transcripts = [processor.decode(ids, skip_special_tokens=True) for ids in label_ids]\n",
    "\n",
    "        # Extend the lists with the current batch results\n",
    "        predicted_sentences.extend(predicted_transcripts)\n",
    "        actual_sentences.extend(actual_transcripts)\n",
    "\n",
    "# Print results\n",
    "results = [{\"actual\": actual, \"predicted\": predicted} for actual, predicted in zip(actual_sentences, predicted_sentences)]\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac581792-5cb6-49ef-afb7-86027223c0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "wer_values = [wer(actual, predicted) for actual, predicted in zip(actual_sentences, predicted_sentences)]\n",
    "\n",
    "# Calculate average WER\n",
    "average_wer = sum(wer_values) / len(wer_values)\n",
    "\n",
    "# Print WER for each sentence and the average WER\n",
    "for i, (actual, predicted, wer_value) in enumerate(zip(actual_sentences, predicted_sentences, wer_values)):\n",
    "    print(f\"Sentence {i+1} WER: {wer_value:.2f}\")\n",
    "\n",
    "print(f\"Average WER: {average_wer:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68f7c7-017f-403c-ab87-0842070edd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_id = 3\n",
    "# token = processor.tokenizer.convert_ids_to_tokens(token_id)\n",
    "# print(f\"Token ID {token_id} corresponds to token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795c3ad-83ed-4766-99b5-eaf4b92df3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"Tokenizer vocabulary size: {len(processor.tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c09ce8a-5338-4b11-8f04-c8f0b98de58d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchaudio.transforms import AddNoise, SpeedPerturb, TimeStretch\n",
    "\n",
    "# def augment_audio(audio):\n",
    "#     # Add noise\n",
    "#     audio = AddNoise()(audio)\n",
    "#     # Speed perturbation\n",
    "#     audio = SpeedPerturb()(audio)\n",
    "#     # Time stretch\n",
    "#     audio = TimeStretch()(audio)\n",
    "#     return audio\n",
    "\n",
    "# # Apply augmentation to your dataset\n",
    "# train_dataset = [augment_audio(audio) for audio in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d3bf2-591f-463f-aa0a-95743e216c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
