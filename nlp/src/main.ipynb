{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662e471b-1fc2-4094-88f6-6860724ff715",
   "metadata": {},
   "source": [
    "# Task: `Natural Language Processing`\n",
    "\n",
    "Given an audio transcription of a turret command instruction, return a JSON object corresponding to the specified targetâ€™s description, heading, and the tool to be deployed against it.\n",
    "\n",
    "**For Advanced teams**, the transcript will be a turret instruction in natural language.\n",
    "\n",
    "**For Novice teams**, the transcript will follow a relatively structured format with the turret operator explicitly reading out the turret, heading, and tool, though the order may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27cdc2-3bc7-4b37-acd8-0c64b18d9b0e",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f850c8-c64c-4000-894d-f964e48d90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q seqeval spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e115531c-23a6-4b21-bef7-a07cfbe99fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5d7f60-31df-4de4-8e85-f5149928c6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/til-24-base'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "nlp_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(nlp_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "test_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(test_dir, 'audio')\n",
    "\n",
    "til_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3fcd0-51b7-4574-ab79-c8b2f75cd269",
   "metadata": {},
   "source": [
    "## Fine Tuning bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3eca84-34b9-4cf0-922a-ab9ba9c55912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-TARGET\", \"I-TARGET\", \"B-HEADING\", \"I-HEADING\", \"B-TOOL\", \"I-TOOL\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c7c8d59-5451-4ff4-a459-3548c4941841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest token sequence: ['heading', 'is', '2', '6', '0', 'target', 'is', 'black', 'white', 'and', 'yellow', 'commercial', 'aircraft', 'tool', 'to', 'deploy', 'is', 'surface-to-air', 'missiles']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_transcript(transcript):\n",
    "    # Convert to lowercase\n",
    "    transcript = transcript.lower()\n",
    "    \n",
    "    # Remove punctuation except hyphens\n",
    "    cleaned_transcript = re.sub(r'[^\\w\\s-]', '', transcript)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    cleaned_transcript = re.sub(r'\\s+', ' ', cleaned_transcript).strip()\n",
    "    \n",
    "    return cleaned_transcript\n",
    "\n",
    "def word_to_num(word):\n",
    "    \"\"\"Convert number words to digits if possible, and handle variations.\"\"\"\n",
    "    word = word.replace(',', '')  # Remove commas\n",
    "    num_dict = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9'\n",
    "    }\n",
    "    \n",
    "    # Regular expression patterns for numbers\n",
    "    pattern_actual = r'^(zero|one|two|three|four|five|six|seven|eight|nine)$'\n",
    "    pattern_variations = rf'^{pattern_actual}(er|s)?$'\n",
    "    \n",
    "    match = re.match(pattern_variations, word)\n",
    "    \n",
    "    if match:\n",
    "        actual = match.group(1)\n",
    "        return num_dict[actual]\n",
    "    \n",
    "    return word\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    label_map = {\n",
    "        \"target\": \"TARGET\",\n",
    "        \"heading\": \"HEADING\",\n",
    "        \"tool\": \"TOOL\"\n",
    "    }\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            transcript = clean_transcript(data[\"transcript\"]).split()\n",
    "            transcript = [word_to_num(word) for word in transcript]\n",
    "            token_list = []\n",
    "            label_list = [\"O\"] * len(transcript)\n",
    "\n",
    "            for key in [\"target\", \"heading\", \"tool\"]:\n",
    "                if key in data:\n",
    "                    entity = clean_transcript(data[key]).split()\n",
    "                    if key == \"heading\":\n",
    "                        entity = list(data[key])\n",
    "                    entity_len = len(entity)\n",
    "                    for i in range(len(transcript)):\n",
    "                        if transcript[i:i + entity_len] == entity:\n",
    "                            # if key == \"heading\" and idx < 5: # DEBUG\n",
    "                            #     print(entity)\n",
    "                            label_list[i] = f\"B-{label_map[key].upper()}\"\n",
    "                            for j in range(1, entity_len):\n",
    "                                label_list[i + j] = f\"I-{label_map[key].upper()}\"\n",
    "                            break\n",
    "\n",
    "            tokens.append(transcript)\n",
    "            labels.append(label_list)\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "tokens, labels = read_jsonl(os.path.join(test_dir, \"nlp.jsonl\"))\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_tokens, temp_tokens, train_labels, temp_labels = train_test_split(tokens, labels, test_size=0.4, random_state=42)\n",
    "val_tokens, test_tokens, val_labels, test_labels = train_test_split(temp_tokens, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_tokens, \"ner_tags\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"tokens\": val_tokens, \"ner_tags\": val_labels})\n",
    "test_dataset = Dataset.from_dict({\"tokens\": test_tokens, \"ner_tags\": test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102e5ddf-29ab-494b-b1a6-254af4938dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heading', 'is', '1', '6', '5', 'target', 'is', 'grey', 'and', 'purple', 'drone', 'tool', 'to', 'deploy', 'is', 'anti-air', 'artillery']\n",
      "['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['tokens'])\n",
    "print(train_dataset[0]['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294d2add-f926-498e-823d-4bf43d0018aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dbcaf73be441d290fd5b847aab127d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19076d4a7b5945509f8190fab810842e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28aa067469e42ccb092273ebf25ea41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=\"max_length\")\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special token like [CLS], [SEP], etc.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Set padding tokens' label to -100\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Ensure all label sequences are the same length\n",
    "    max_len = len(tokenized_inputs[\"input_ids\"][0])\n",
    "    for label_ids in labels:\n",
    "        if len(label_ids) < max_len:\n",
    "            label_ids.extend([-100] * (max_len - len(label_ids)))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"word_ids\"] = [tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))]\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff39e497-6e69-457e-b834-f692141f39fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first_element = tokenized_train_dataset[0]\n",
    "# print(first_element)\n",
    "\n",
    "# def print_word_ids(tokenized_inputs, idx):\n",
    "#     word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "#     tokens = tokenized_inputs.tokens(batch_index=idx)\n",
    "#     print(f\"Tokens: {tokens}\")\n",
    "#     print(f\"Word IDs: {word_ids}\")\n",
    "\n",
    "# for i in range(len(tokenized_test_dataset)):\n",
    "#     tokenized_example = tokenizer(tokenized_test_dataset[i]['tokens'], truncation=True, is_split_into_words=True, padding=\"max_length\")\n",
    "#     print(f\"\\nExample {i}:\")\n",
    "#     print_word_ids(tokenized_example, 0)\n",
    "\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bdb0d-64ae-4a35-9173-2e424a8edcc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "837811b5-9caf-4cd6-bf6d-b58eba2c20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list))\n",
    "model.gradient_checkpointing_enable() # using gradient checkpointing to reduce memory usage by trading off compute time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598c54b-cdd8-44a5-a7fb-55e4d06864b8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81f3928-ef11-4f47-900d-afbe5a796a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [396/396 13:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.000586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./models/nlp_4/tokenizer_config.json',\n",
       " './models/nlp_4/special_tokens_map.json',\n",
       " './models/nlp_4/vocab.txt',\n",
       " './models/nlp_4/added_tokens.json',\n",
       " './models/nlp_4/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",  # Enable logging\n",
    "    logging_steps=50,  # Log every 50 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./models/nlp_4\")\n",
    "tokenizer.save_pretrained(\"./models/nlp_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1957e97-a8a1-4d3a-a9d7-2d117b14210d",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071e243b-8bfa-4852-8433-b57d7ac653df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./models/nlp_4\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/nlp_4\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "\n",
    "# Define the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45cd89cd-e7ad-458e-8a4c-e7574f51dd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "predictions, label_ids, _ = trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a99c4e-cbcc-478d-a648-8a139cf70376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE SPACY IF MORE THAN 2 PREDICTIONS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_mean_embedding_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return np.mean([token.vector for token in doc if token.has_vector], axis=0)\n",
    "\n",
    "# Representative vectors for descriptive and weapon concepts\n",
    "descriptive_vector = get_mean_embedding_spacy(\"colorful vibrant bright vivid\") # TODO since data is mostly color describing targets\n",
    "weapon_vector = get_mean_embedding_spacy(\"gun rifle knife bomb missile grenade weapon\")\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "def select_best_entity(entities, representative_vector_fn, concept_vector):\n",
    "    best_entity = None\n",
    "    best_similarity = -1\n",
    "    for entity in entities:\n",
    "        entity_vector = representative_vector_fn(entity)\n",
    "        similarity = cosine_sim(entity_vector, concept_vector)\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_entity = entity\n",
    "    return best_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7649082-a487-4ab0-bde6-3add224910c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids, tokenized_inputs):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Initialize lists to hold labels, entities, and tokens\n",
    "    label_list = [[] for _ in range(len(label_ids))]\n",
    "    pred_list = [[] for _ in range(len(label_ids))]\n",
    "    entity_list = [[] for _ in range(len(label_ids))]\n",
    "    token_list = [[] for _ in range(len(label_ids))]\n",
    "\n",
    "    for i in range(len(label_ids)):\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        word_ids = tokenized_inputs[\"word_ids\"][i]  # Get word IDs for the current example\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][i])  # Get tokens for the current example\n",
    "\n",
    "        entity_types = {}\n",
    "\n",
    "        for j, word_idx in enumerate(word_ids):\n",
    "            # Skip special tokens and padding\n",
    "            if word_idx is None or label_ids[i][j] == -100:\n",
    "                continue\n",
    "\n",
    "            true_label = id2label[label_ids[i][j]]\n",
    "            pred_label = id2label[preds[i][j]]\n",
    "\n",
    "            label_list[i].append(true_label)\n",
    "            pred_list[i].append(pred_label)\n",
    "            token_list[i].append(tokens[j])\n",
    "\n",
    "            # Extract entities based on predicted labels\n",
    "            if pred_label.startswith(\"B-\"):\n",
    "                if current_entity and current_label:\n",
    "                    if current_label not in entity_types:\n",
    "                        entity_types[current_label] = []\n",
    "                    entity_types[current_label].append(\" \".join(current_entity))\n",
    "                current_entity = [tokens[j]]\n",
    "                current_label = pred_label[2:]\n",
    "            elif pred_label.startswith(\"I-\") and current_label == pred_label[2:]:\n",
    "                current_entity.append(tokens[j])\n",
    "            else:\n",
    "                if current_entity and current_label:\n",
    "                    if current_label not in entity_types:\n",
    "                        entity_types[current_label] = []\n",
    "                    entity_types[current_label].append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "        if current_entity and current_label:\n",
    "            if current_label not in entity_types:\n",
    "                entity_types[current_label] = []\n",
    "            entity_types[current_label].append(\" \".join(current_entity))\n",
    "\n",
    "        for entity_type, entities in entity_types.items():\n",
    "            if entity_type == \"TARGET\":\n",
    "                best_entity = sorted(entities, key=lambda x: cosine_sim(get_mean_embedding_spacy(x), descriptive_vector), reverse=True)[0]\n",
    "            elif entity_type == \"TOOL\":\n",
    "                best_entity = sorted(entities, key=lambda x: cosine_sim(get_mean_embedding_spacy(x), weapon_vector), reverse=True)[0]\n",
    "            else:\n",
    "                best_entity = entities[0]\n",
    "            entity_list[i].append((entity_type, best_entity))\n",
    "\n",
    "    return token_list, pred_list, label_list, entity_list\n",
    "\n",
    "tokens, pred_labels, true_labels, entities = align_predictions(predictions, label_ids, tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00017b57-ad0c-4456-b2d5-b56e99dee3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['heading', 'is', '2', '0', '0', 'target', 'is', 'silver', 'and', 'black', 'drone', 'tool', 'to', 'deploy', 'is', 'drone', 'catcher']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '2 0 0'), ('TARGET', 'silver and black drone'), ('TOOL', 'drone catcher')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '2', '2', '0', 'target', 'is', 'white', 'cargo', 'aircraft', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '2 2 0'), ('TARGET', 'white cargo aircraft'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '2', 'nine', '0', 'target', 'is', 'orange', 'and', 'silver', 'helicopter', 'tool', 'to', 'deploy', 'is', 'anti', 'artillery']\n",
      "Predicted Labels:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('TARGET', 'orange and silver helicopter'), ('TOOL', 'anti artillery')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '0', '3', '5', 'target', 'is', 'yellow', 'red', 'and', 'green', 'light', 'aircraft', 'tool', 'to', 'deploy', 'is', 'em']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL']\n",
      "Entities:  [('HEADING', '0 3 5'), ('TARGET', 'yellow red and green light aircraft'), ('TOOL', 'em')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '4', '5', 'target', 'is', 'orange', 'missile', 'tool', 'to', 'deploy', 'is', 'anti', 'artillery']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 4 5'), ('TARGET', 'orange missile'), ('TOOL', 'anti artillery')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '1', 'nine', '5', 'target', 'is', 'brown', 'yellow', 'and', 'white', 'fighter', 'jet', 'tool', 'to', 'deploy', 'is', 'surface', 'missiles']\n",
      "Predicted Labels:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('TARGET', 'brown yellow and white fighter jet'), ('TOOL', 'surface missiles')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '3', '0', 'target', 'is', 'green', 'drone', 'tool', 'to', 'deploy', 'is', 'surface', 'missiles']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 3 0'), ('TARGET', 'green drone'), ('TOOL', 'surface missiles')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '1', '6', '5', 'target', 'is', 'red', 'missile', 'tool', 'to', 'deploy', 'is', 'machine', 'gun']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '1 6 5'), ('TARGET', 'red missile'), ('TOOL', 'machine gun')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '6', '0', 'target', 'is', 'grey', 'silver', 'and', 'purple', 'light', 'aircraft', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 6 0'), ('TARGET', 'grey silver and purple light aircraft'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '5', '5', 'target', 'is', 'yellow', 'and', 'brown', 'helicopter', 'tool', 'to', 'deploy', 'is', 'surface', 'missiles']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 5 5'), ('TARGET', 'yellow and brown helicopter'), ('TOOL', 'surface missiles')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '3', '0', 'target', 'is', 'green', 'red', 'and', 'black', 'cargo', 'aircraft', 'tool', 'to', 'deploy', 'is', 'interceptor', 'jets']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 3 0'), ('TARGET', 'green red and black cargo aircraft'), ('TOOL', 'interceptor jets')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '2', '2', '5', 'target', 'is', 'red', 'and', 'orange', 'helicopter', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '2 2 5'), ('TARGET', 'red and orange helicopter'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '1', '1', '5', 'target', 'is', 'green', 'commercial', 'aircraft', 'tool', 'to', 'deploy', 'is', 'em']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL']\n",
      "Entities:  [('HEADING', '1 1 5'), ('TARGET', 'green commercial aircraft'), ('TOOL', 'em')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '0', '0', 'target', 'is', 'grey', 'and', 'silver', 'commercial', 'aircraft', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 0 0'), ('TARGET', 'grey and silver commercial aircraft'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '2', '2', '0', 'target', 'is', 'green', 'helicopter', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '2 2 0'), ('TARGET', 'green helicopter'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '5', '5', 'target', 'is', 'brown', 'drone', 'tool', 'to', 'deploy', 'is', 'surface', 'missiles']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 5 5'), ('TARGET', 'brown drone'), ('TOOL', 'surface missiles')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '2', '4', '0', 'target', 'is', 'grey', 'commercial', 'aircraft', 'tool', 'to', 'deploy', 'is', 'em']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL']\n",
      "Entities:  [('HEADING', '2 4 0'), ('TARGET', 'grey commercial aircraft'), ('TOOL', 'em')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '3', '4', '0', 'target', 'is', 'white', 'green', 'and', 'orange', 'missile', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '3 4 0'), ('TARGET', 'white green and orange missile'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '0', '6', '0', 'target', 'is', 'orange', 'commercial', 'aircraft', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '0 6 0'), ('TARGET', 'orange commercial aircraft'), ('TOOL', 'electromagnetic pulse')]\n",
      "\n",
      "\n",
      "Tokens: ['heading', 'is', '1', '8', '5', 'target', 'is', 'red', 'brown', 'and', 'yellow', 'commercial', 'aircraft', 'tool', 'to', 'deploy', 'is', 'surface', 'missiles']\n",
      "Predicted Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "True Labels:  ['O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'I-TOOL']\n",
      "Entities:  [('HEADING', '1 8 5'), ('TARGET', 'red brown and yellow commercial aircraft'), ('TOOL', 'surface missiles')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zipped_results = list(zip(tokens, pred_labels, true_labels, entities))\n",
    "\n",
    "# Print the zipped results for inspection\n",
    "for token, pred, true, entity in zipped_results[:20]:\n",
    "    print(\"Tokens:\", token)\n",
    "    print(\"Predicted Labels: \", pred)\n",
    "    print(\"True Labels: \", true)\n",
    "    print(\"Entities: \", entity)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a41c62-1373-48ac-b641-687886eb25d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     HEADING       1.00      1.00      1.00       637\n",
      "      TARGET       1.00      1.00      1.00       700\n",
      "        TOOL       1.00      1.00      1.00       700\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      2037\n",
      "   macro avg       1.00      1.00      1.00      2037\n",
      "weighted avg       1.00      1.00      1.00      2037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0e2d19a-5f8a-437b-b570-c16c13f2f689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_random_sentence(sentence, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sentence = clean_transcript(sentence)\n",
    "    tokens = sentence.split()\n",
    "    tokens = [word_to_num(word) for word in tokens]\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(**inputs)\n",
    "    predictions = outputs.logits.cpu().numpy()\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "\n",
    "    pred_labels = [id2label[pred] if word_idx is not None else 'O' for pred, word_idx in zip(preds[0], word_ids)]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    entity_list = []\n",
    "    entity_types = {}\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, pred_labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity and current_label:\n",
    "                if current_label not in entity_types:\n",
    "                    entity_types[current_label] = []\n",
    "                entity_types[current_label].append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "            current_label = label[2:]\n",
    "        elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity and current_label:\n",
    "                if current_label not in entity_types:\n",
    "                    entity_types[current_label] = []\n",
    "                entity_types[current_label].append(\" \".join(current_entity))\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "\n",
    "    if current_entity and current_label:\n",
    "        if current_label not in entity_types:\n",
    "            entity_types[current_label] = []\n",
    "        entity_types[current_label].append(\" \".join(current_entity))\n",
    "\n",
    "    for entity_type, entities in entity_types.items():\n",
    "        if entity_type == \"TARGET\":\n",
    "            best_entity = sorted(entities, key=lambda x: cosine_sim(get_mean_embedding_spacy(x), descriptive_vector), reverse=True)[0]\n",
    "        elif entity_type == \"TOOL\":\n",
    "            best_entity = sorted(entities, key=lambda x: cosine_sim(get_mean_embedding_spacy(x), weapon_vector), reverse=True)[0]\n",
    "        else:\n",
    "            best_entity = entities[0]\n",
    "        entity_list.append((entity_type, best_entity))\n",
    "\n",
    "    return tokens, pred_labels, entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e9c7aa3-1a5a-4060-8b5c-2ec3f4e7f8ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'heading', 'is', '1', '6', '7', 'target', 'is', 'a', 'large', 'red', 'and', 'white', 'cargo', 'ship', 'with', 'black', 'stripes', 'tool', 'to', 'deploy', 'is', 'radar', 'surveillance', 'system', 'with', 'advanced', 'tracking', 'capabilities', 'and', 'high', '-', 'resolution', 'imaging', 'for', 'detailed', 'monitoring', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Predicted Labels: ['O', 'O', 'O', 'B-HEADING', 'I-HEADING', 'I-HEADING', 'O', 'O', 'O', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TOOL', 'B-TOOL', 'O', 'O', 'B-TOOL', 'B-TOOL', 'O', 'O', 'B-TOOL', 'B-TOOL', 'I-TARGET', 'I-TARGET', 'O', 'B-TOOL', 'I-TOOL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Entities: [('HEADING', '1 6 7'), ('TARGET', 'black stripes'), ('TOOL', 'radar')]\n"
     ]
    }
   ],
   "source": [
    "random_sentence = \"Heading is one six seven, target is a large red and white cargo ship with black stripes, tool to deploy is radar surveillance system with advanced tracking capabilities and high-resolution imaging for detailed monitoring.\"\n",
    "tokens, pred_labels, entity_list = predict_on_random_sentence(random_sentence, trainer.model)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Predicted Labels:\", pred_labels)\n",
    "print(\"Entities:\", entity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027469c9-62ff-4a29-b1d7-7b224c262fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "TO IMPROVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7c2b1-d766-41a0-a725-8583547fd9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
