{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662e471b-1fc2-4094-88f6-6860724ff715",
   "metadata": {},
   "source": [
    "# Task: `Natural Language Processing`\n",
    "\n",
    "Given an audio transcription of a turret command instruction, return a JSON object corresponding to the specified targetâ€™s description, heading, and the tool to be deployed against it.\n",
    "\n",
    "**For Advanced teams**, the transcript will be a turret instruction in natural language.\n",
    "\n",
    "**For Novice teams**, the transcript will follow a relatively structured format with the turret operator explicitly reading out the turret, heading, and tool, though the order may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27cdc2-3bc7-4b37-acd8-0c64b18d9b0e",
   "metadata": {},
   "source": [
    "_Insert Code Here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e115531c-23a6-4b21-bef7-a07cfbe99fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertForQuestionAnswering, AutoTokenizer, DefaultDataCollator, TrainingArguments, Trainer, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5d7f60-31df-4de4-8e85-f5149928c6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/novice/audio'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "nlp_dir = os.path.dirname(cur_dir)\n",
    "til_dir = os.path.dirname(nlp_dir)\n",
    "home_dir = os.path.dirname(til_dir)\n",
    "data_dir = os.path.join(home_dir, 'novice')\n",
    "audio_dir = os.path.join(data_dir, 'audio')\n",
    "\n",
    "audio_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7c8d59-5451-4ff4-a459-3548c4941841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0330466f-6cdc-481c-bed8-c045997359ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: when was the university of california established? [SEP] the university of california\n"
     ]
    }
   ],
   "source": [
    "context = \"The University of California was founded in 1868, located in Berkeley.\"\n",
    "question = \"When was the University of California established?\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Find the tokens with the highest `start` and `end` scores\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Convert tokens to answer string\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0, answer_start:answer_end]))\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3fcd0-51b7-4574-ab79-c8b2f75cd269",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d2add-f926-498e-823d-4bf43d0018aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        token_list = []\n",
    "        label_list = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.strip().split()\n",
    "                token_list.append(token)\n",
    "                label_list.append(label)\n",
    "            else:\n",
    "                tokens.append(token_list)\n",
    "                labels.append(label_list)\n",
    "                token_list = []\n",
    "                label_list = []\n",
    "        if token_list:\n",
    "            tokens.append(token_list)\n",
    "            labels.append(label_list)\n",
    "    return tokens, labels\n",
    "\n",
    "train_tokens, train_labels = read_data(\"train.txt\")\n",
    "val_tokens, val_labels = read_data(\"val.txt\")\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"tokens\": train_tokens, \"ner_tags\": train_labels})\n",
    "val_dataset = datasets.Dataset.from_dict({\"tokens\": val_tokens, \"ner_tags\": val_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837811b5-9caf-4cd6-bf6d-b58eba2c20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-TARGET\", \"I-TARGET\", \"B-HEADING\", \"I-HEADING\", \"B-TOOL\", \"I-TOOL\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29550bea-2e1b-46c7-8430-97b030f785c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special token like [CLS], [SEP], etc.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e85813-f60a-4f1d-ab56-c5eac8a2048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2ab37-5f8b-43ab-b4b8-717749e7d7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
